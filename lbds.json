[{"content":{"TLDR":"Olaf is a portable, landmark-based, acoustic fingerprint-ing system released as open source software. Olaf runs on embedded platforms, traditional computers and in the browser. Olaf is able to extract fingerprints from an audio stream, and either store those fingerprints in a database, or find a match between extracted fingerprints and stored fingerprints. It implements an algorithm similar to the one described in a classic ISMIR paper and has similar retrieval performance. It facilitates the many use cases acoustic fingerprinting has to offer such as duplicate detection, meta-data coupling, and synchronization.","abstract":"Olaf is a portable, landmark-based, acoustic fingerprint-ing system released as open source software. Olaf runs on embedded platforms, traditional computers and in the browser. Olaf is able to extract fingerprints from an audio stream, and either store those fingerprints in a database, or find a match between extracted fingerprints and stored fingerprints. It implements an algorithm similar to the one described in a classic ISMIR paper and has similar retrieval performance. It facilitates the many use cases acoustic fingerprinting has to offer such as duplicate detection, meta-data coupling, and synchronization.","authors":["Joren Six"],"bilibili_id":"BV1oZ4y157ER","channel_name":"lbd-8-418-six","channel_url":"https://ismir2020.slack.com/archives/C01CR4BSZLZ","day":4,"poster_type":"html","session":["8"],"title":"OLAF: Overly Lightweight Acoustic Fingerprinting","youtube_id":"wP29RaQicwE"},"forum":"418","id":"418"},{"content":{"TLDR":"Beat processing is a critical component of how humans experience music and a core topic in Music Information Retrieval (MIR). In this study we investigate relationships between computational features and human perception using real-world music spanning a variety of rhythmic categories. We observe a significant relationship between pulse clarity computed from the audio and inter-subject correlation of the neural responses. This finding suggests promising future lines of investigation integrating MIR audio features, neural correlation, and subjective behavioral reports in understanding beat processing and representation.","abstract":"Beat processing is a critical component of how humans experience music and a core topic in Music Information Retrieval (MIR). In this study we investigate relationships between computational features and human perception using real-world music spanning a variety of rhythmic categories. We observe a significant relationship between pulse clarity computed from the audio and inter-subject correlation of the neural responses. This finding suggests promising future lines of investigation integrating MIR audio features, neural correlation, and subjective behavioral reports in understanding beat processing and representation.","authors":["Jay Appaji","Jacek Dmochowski","Blair Kaneshiro"],"bilibili_id":"","channel_name":"lbd-7-419-appaji","channel_url":"https://ismir2020.slack.com/archives/C01C5PFTHPY","day":4,"poster_type":"pdf","session":["7"],"title":"Modelling Perception of Rhythmic Complexity: Computational and Neural Measures","youtube_id":""},"forum":"419","id":"419"},{"content":{"TLDR":"In order to study performance characteristics of untrained, amateur singers, we developed an online tool through which coders could evaluate real-world vocal performances of \u201cAmazing Grace\u201d from a Smule dataset. Coders from Stanford University used the online evaluation tool to deliver judgments of age and gender of the performers, as well as skill, likeability, and expressiveness of the vocal performances. Initial results show subjective evaluations of skill, likeability, and expressiveness are highly correlated, and coders rarely gave the highest possible score in any of the three metrics. This online evaluation tool can be used in future computational studies of vocal performance.","abstract":"In order to study performance characteristics of untrained, amateur singers, we developed an online tool through which coders could evaluate real-world vocal performances of \u201cAmazing Grace\u201d from a Smule dataset. Coders from Stanford University used the online evaluation tool to deliver judgments of age and gender of the performers, as well as skill, likeability, and expressiveness of the vocal performances. Initial results show subjective evaluations of skill, likeability, and expressiveness are highly correlated, and coders rarely gave the highest possible score in any of the three metrics. This online evaluation tool can be used in future computational studies of vocal performance.","authors":["Elena Georgieva","Camille Noufi","Vidya Rangasayee","Blair Kaneshiro","Jonathan Berger"],"bilibili_id":"","channel_name":"lbd-8-420-georgieva","channel_url":"https://ismir2020.slack.com/archives/C01CR4Q0BS5","day":4,"poster_type":"pdf","session":["8"],"title":"An Evaluation Tool for Subjective Evaluation of Amateur Vocal Performances of \u201cAmazing Grace\u201d","youtube_id":""},"forum":"420","id":"420"},{"content":{"TLDR":"In this late-breaking abstract we propose a modified approach for beat tracking evaluation which poses the problem in terms of the effort required to transform a sequence of beat detections such that they maximise the well-known F-measure calculation when compared to a sequence of ground truth annotations. Central to our approach is the inclusion of a shifting operation conducted over an additional, larger, tolerance window, which can substitute the combination of insertions and deletions. We describe a straightforward calculation of annotation efficiency and combine this with an informative visualisation which can be of use for the qualitative evaluation of beat tracking systems. We make our implementation and visualisation code freely available in a GitHub repository.","abstract":"In this late-breaking abstract we propose a modified approach for beat tracking evaluation which poses the problem in terms of the effort required to transform a sequence of beat detections such that they maximise the well-known F-measure calculation when compared to a sequence of ground truth annotations. Central to our approach is the inclusion of a shifting operation conducted over an additional, larger, tolerance window, which can substitute the combination of insertions and deletions. We describe a straightforward calculation of annotation efficiency and combine this with an informative visualisation which can be of use for the qualitative evaluation of beat tracking systems. We make our implementation and visualisation code freely available in a GitHub repository.","authors":["Ant\u00f3nio Pinto","Ines Domingues","Matthew Davies"],"bilibili_id":"","channel_name":"lbd-8-421-pinto","channel_url":"https://ismir2020.slack.com/archives/C01C9CZMC3F","day":4,"poster_type":"pdf","session":["8"],"title":"Shift If You Can: Counting and Visualising Correction Operations for Beat Tracking Evaluation","youtube_id":""},"forum":"421","id":"421"},{"content":{"TLDR":"We introduce rolypoly~, the first drum machine for live performance that adapts its microtiming in relation to a human musician. We leverage state-of-the-art work in expressive performance modelling with recurrent nets, towards real-time application on the micro scale. Our models are pretrained on the Groove MIDI Dataset from Magenta, and then fine-tuned iteratively over several duet performances of a new piece. We propose a method for defining training targets based on previous performances, rather than a prior ground truth. The agent is able to adapt to human timing nuances, and can achieve effects such as morphing a rhythm from straight to swing.","abstract":"We introduce rolypoly~, the first drum machine for live performance that adapts its microtiming in relation to a human musician. We leverage state-of-the-art work in expressive performance modelling with recurrent nets, towards real-time application on the micro scale. Our models are pretrained on the Groove MIDI Dataset from Magenta, and then fine-tuned iteratively over several duet performances of a new piece. We propose a method for defining training targets based on previous performances, rather than a prior ground truth. The agent is able to adapt to human timing nuances, and can achieve effects such as morphing a rhythm from straight to swing.","authors":["Grigore Burloiu"],"bilibili_id":"BV1XZ4y157Dt","channel_name":"lbd-8-422-burloiu","channel_url":"https://ismir2020.slack.com/archives/C01CCFLEDS6","day":4,"poster_type":"pdf","session":["8"],"title":"Adaptive Drum Machine Microtiming with Transfer Learning and RNNs","youtube_id":"zUICIHncfXE"},"forum":"422","id":"422"},{"content":{"TLDR":"We propose the Multi-Track Music Machine (MMM), a generative system based on the Transformer architecture that is capable of generating multi-track music. In contrast to previous work, which represents musical material as a single time-ordered sequence, where the musical events corresponding to different tracks are interleaved, we create a time-ordered sequence of musical events for each track and concatenate several tracks into a single sequence. This takes advantage of the Transformer's attention-mechanism, which can adeptly handle long-term dependencies. We explore how various representations can offer the user a high degree of control at generation time, providing an interactive demo that accommodates track-level and bar-level inpainting, and offers control over track instrumentation and note density.","abstract":"We propose the Multi-Track Music Machine (MMM), a generative system based on the Transformer architecture that is capable of generating multi-track music. In contrast to previous work, which represents musical material as a single time-ordered sequence, where the musical events corresponding to different tracks are interleaved, we create a time-ordered sequence of musical events for each track and concatenate several tracks into a single sequence. This takes advantage of the Transformer's attention-mechanism, which can adeptly handle long-term dependencies. We explore how various representations can offer the user a high degree of control at generation time, providing an interactive demo that accommodates track-level and bar-level inpainting, and offers control over track instrumentation and note density.","authors":["Jeffrey Ens","Philippe Pasquier"],"bilibili_id":"BV1M54y127yZ","channel_name":"lbd-7-423-ens","channel_url":"https://ismir2020.slack.com/archives/C01CCFLEKLJ","day":4,"poster_type":"pdf","session":["7"],"title":"Flexible Generation with the Multi-track Music Machine","youtube_id":"_tDGv6zBrVg"},"forum":"423","id":"423"},{"content":{"TLDR":"Choirs aim at blending the voices of different choral parts together in order to create a cohesive whole. Reaching this goal requires significant time and effort spent in rehearsals. Since joint rehearsal time is limited, amateur choir singers often practice their parts individually (e.g., at home) before the rehearsal. Over the last years, applications have become popular that offer sing-along and score following functionalities for individual rehearsals. In this work, we present a web-based interface with intonation feedback mechanism for choir rehearsal preparation. The interface combines several open-source tools that have been developed by the MIR community.","abstract":"Choirs aim at blending the voices of different choral parts together in order to create a cohesive whole. Reaching this goal requires significant time and effort spent in rehearsals. Since joint rehearsal time is limited, amateur choir singers often practice their parts individually (e.g., at home) before the rehearsal. Over the last years, applications have become popular that offer sing-along and score following functionalities for individual rehearsals. In this work, we present a web-based interface with intonation feedback mechanism for choir rehearsal preparation. The interface combines several open-source tools that have been developed by the MIR community.","authors":["Sebastian Rosenzweig","Lukas Dietz","Johannes Graulich","Meinard M\u00fcller"],"bilibili_id":"","channel_name":"lbd-8-424-rosenzweig","channel_url":"https://ismir2020.slack.com/archives/C01C9CZMSDT","day":4,"poster_type":"pdf","session":["8"],"title":"TuneIn: a Web-based Interface for Practicing Choral Parts","youtube_id":""},"forum":"424","id":"424"},{"content":{"TLDR":"We explore the task of predicting artist origin (hometown, current city of residence) based on their past live music events, using a heuristic approach; specifically, by calculating the proportion of the artist\u2019s events that are located near the city where the artist performs most frequently.","abstract":"We explore the task of predicting artist origin (hometown, current city of residence) based on their past live music events, using a heuristic approach; specifically, by calculating the proportion of the artist\u2019s events that are located near the city where the artist performs most frequently.","authors":["Michael G Zhou","Douglas Turnbull","Thorsten Joachims"],"bilibili_id":"BV1it4y1v7up","channel_name":"lbd-8-425-zhou","channel_url":"https://ismir2020.slack.com/archives/C01CJLC3WTW","day":4,"poster_type":"pdf","session":["8"],"title":"Can We Determine Artist Origin from Past Live Events?","youtube_id":"OpqFuyO_ews"},"forum":"425","id":"425"},{"content":{"TLDR":"The present study introduces a mathematical model for the symmetric phrasing scheme of German musicologist Hugo Riemann (1849\u20131919). The model is used to create an artificially expressive timing pattern in Max Reger\u2019s (1873\u00ac\u20131916) organ Prelude op. 135a/1 and is evaluated analytically against professional human interpretation.","abstract":"The present study introduces a mathematical model for the symmetric phrasing scheme of German musicologist Hugo Riemann (1849\u20131919). The model is used to create an artificially expressive timing pattern in Max Reger\u2019s (1873\u00ac\u20131916) organ Prelude op. 135a/1 and is evaluated analytically against professional human interpretation.","authors":["Yulia Draginda","Ichiro Fujinaga"],"bilibili_id":"BV1ot4y1v7Kd","channel_name":"lbd-7-426-draginda","channel_url":"https://ismir2020.slack.com/archives/C01CCMPQCE7","day":4,"poster_type":"pdf","session":["7"],"title":"First Steps Towards Modelling Expressive Timing in German Late Romantic Organ Music","youtube_id":"08WBIMDB8cY"},"forum":"426","id":"426"},{"content":{"TLDR":"The topic of Music Emotion Recognition (MER) evolved as music is a fascinating expression of emotions, yet it faces challenges given its subjectivity. Because each language has its particularities in terms of sound and intonation, and implicitly associations made upon them, we hypothesize perceived emotions might vary in different cultures. To address this issue, we test a novel approach towards emotion detection and propose a language sensitive end-to-end model that learns to tag emotions from music with lyrics in English, Mandarin and Turkish.","abstract":"The topic of Music Emotion Recognition (MER) evolved as music is a fascinating expression of emotions, yet it faces challenges given its subjectivity. Because each language has its particularities in terms of sound and intonation, and implicitly associations made upon them, we hypothesize perceived emotions might vary in different cultures. To address this issue, we test a novel approach towards emotion detection and propose a language sensitive end-to-end model that learns to tag emotions from music with lyrics in English, Mandarin and Turkish.","authors":["Ana Gabriela Pandrea","Juan S. G\u00f3mez-Ca\u00f1\u00f3n","Perfecto Herrera"],"bilibili_id":"","channel_name":"lbd-8-427-pandrea","channel_url":"https://ismir2020.slack.com/archives/C01CR4Q1G0Z","day":4,"poster_type":"pdf","session":["8"],"title":"Cross-dataset Music Emotion Recognition: an End-to-end Approach","youtube_id":""},"forum":"427","id":"427"},{"content":{"TLDR":"In this paper, we introduce a simple method that can separate arbitrary musical instruments from an audio mixture. Given an unaligned MIDI transcription for a target instrument from an input mixture, we synthesize new mixtures from the midi transcription that sound similar to the mixture to be separated. This lets us create a labeled training set to train a network on the specific bespoke task. When this model applied to the original mixture, we demonstrate that this method can: 1) successfully separate out the desired instrument with access to only unaligned MIDI, 2) separate arbitrary instruments, and 3) get results in a fraction of the time of existing methods. We encourage readers to listen to the demos posted here: https://git.io/JUu5q.","abstract":"In this paper, we introduce a simple method that can separate arbitrary musical instruments from an audio mixture. Given an unaligned MIDI transcription for a target instrument from an input mixture, we synthesize new mixtures from the midi transcription that sound similar to the mixture to be separated. This lets us create a labeled training set to train a network on the specific bespoke task. When this model applied to the original mixture, we demonstrate that this method can: 1) successfully separate out the desired instrument with access to only unaligned MIDI, 2) separate arbitrary instruments, and 3) get results in a fraction of the time of existing methods. We encourage readers to listen to the demos posted here: https://git.io/JUu5q.","authors":["Ethan Manilow","Bryan Pardo"],"bilibili_id":"","channel_name":"lbd-7-428-manilow","channel_url":"https://ismir2020.slack.com/archives/C01D2AZCJSC","day":4,"poster_type":"pdf","session":["7"],"title":"Bespoke Neural Networks for Score-informed Source Separation","youtube_id":""},"forum":"428","id":"428"},{"content":{"TLDR":"Singing Voice Detection still have place in Music Information Retrieval research, particularly with the new possibilities generated by feature learning for music content recognition. VGGish embeddings are general purpose audio features that can be used as audio descriptors for multiple tasks. We make a performance comparison of singing voice detectors using vocal VGGish embeddings and vocal perceptually-motivated features. For that end, we train Random Forest models using perceptually inspired features (MFCC, Fluctogram, Vocal Variance) and VGGish embeddings. Our results show that VGGish embeddings have classification performance metrics at least comparable to perceptually-motivated features.","abstract":"Singing Voice Detection still have place in Music Information Retrieval research, particularly with the new possibilities generated by feature learning for music content recognition. VGGish embeddings are general purpose audio features that can be used as audio descriptors for multiple tasks. We make a performance comparison of singing voice detectors using vocal VGGish embeddings and vocal perceptually-motivated features. For that end, we train Random Forest models using perceptually inspired features (MFCC, Fluctogram, Vocal Variance) and VGGish embeddings. Our results show that VGGish embeddings have classification performance metrics at least comparable to perceptually-motivated features.","authors":["Shayenne Moura"],"bilibili_id":"","channel_name":"lbd-7-429-moura","channel_url":"https://ismir2020.slack.com/archives/C01C9CZNKJ9","day":4,"poster_type":"pdf","session":["7"],"title":"Comparison of VGGish Embeddings and Perceptually-motivated Features for Singing Voice Detection","youtube_id":""},"forum":"429","id":"429"},{"content":{"TLDR":"In this demo, we present Mixonset, a consumer iOS app for automatic DJ mixing and music discovery. Mixonset lets users create DJ-style music mixes from their playlists by organizing them, adding recommendations, and creat-ing transitions between each song. It also allows users to mix the highlight part of each song, filter playlists based on various music features, as well as controlling the amount of recommended songs added to the playlist.","abstract":"In this demo, we present Mixonset, a consumer iOS app for automatic DJ mixing and music discovery. Mixonset lets users create DJ-style music mixes from their playlists by organizing them, adding recommendations, and creat-ing transitions between each song. It also allows users to mix the highlight part of each song, filter playlists based on various music features, as well as controlling the amount of recommended songs added to the playlist.","authors":["Zeyu Li"],"bilibili_id":"","channel_name":"lbd-7-430-li","channel_url":"https://ismir2020.slack.com/archives/C01D2AZCRJL","day":4,"poster_type":"pdf","session":["7"],"title":"Mixonset App: Consumer Application for Automatic DJ Mixing and Music Discovery","youtube_id":""},"forum":"430","id":"430"},{"content":{"TLDR":"In this study, we train deep neural networks to classify composer on a symbolic domain. The model takes a two-channel two-dimensional input, i.e., onset and note activations of time-pitch representation, which is converted from MIDI recordings and performs a single-label classification. On the experiments conducted on MAESTRO dataset, we report an F1 value of 0.8333 for the classification of 13~classical composers.","abstract":"In this study, we train deep neural networks to classify composer on a symbolic domain. The model takes a two-channel two-dimensional input, i.e., onset and note activations of time-pitch representation, which is converted from MIDI recordings and performs a single-label classification. On the experiments conducted on MAESTRO dataset, we report an F1 value of 0.8333 for the classification of 13~classical composers.","authors":["Hye Yoon Lee","Sunghyeon Kim","SunJong Park","Keunwoo Choi","Jinho Lee"],"bilibili_id":"","channel_name":"lbd-7-431-lee","channel_url":"https://ismir2020.slack.com/archives/C01CR4Q29J5","day":4,"poster_type":"pdf","session":["7"],"title":"Deep Composer Classification Using Symbolic Representation","youtube_id":""},"forum":"431","id":"431"},{"content":{"TLDR":"In this paper, we introduce a technique for generating large collections of artificial training examples, which can be used to train chord labeling, key detection, and roman numeral analysis models. The technique consists of using roman numeral analysis annotations of existing datasets to generate harmonic reductions of the chords implied by the original annotations. The artificially generated examples ignore the original notes of the annotated example (i.e., the specific \"voicings\" of the chords), replacing them with voicings suggested by a rule-based voice leading algorithm. A relatively large number of artificial examples can be generated from a single annotated progression using this technique. For example, 10 different voicings in 12 different keys would result in 120 artificial examples generated out of one annotated chord progression. The voicings suggested for different keys do not necessarily overlap, given that the range of the voices and other variables are taken into account by the rule-based algorithm. This results in data augmentation with potentially unique voicings in each key, contrary to what would be obtained by simply transposing the artificial examples to a different key. We show the process of applying this technique to a dataset of annotated Bach chorales from the KernScores website. Similar datasets with roman numeral analysis annotations could be used with this approach to generate a large number of artificial training examples for training machine learning models.","abstract":"In this paper, we introduce a technique for generating large collections of artificial training examples, which can be used to train chord labeling, key detection, and roman numeral analysis models. The technique consists of using roman numeral analysis annotations of existing datasets to generate harmonic reductions of the chords implied by the original annotations. The artificially generated examples ignore the original notes of the annotated example (i.e., the specific \"voicings\" of the chords), replacing them with voicings suggested by a rule-based voice leading algorithm. A relatively large number of artificial examples can be generated from a single annotated progression using this technique. For example, 10 different voicings in 12 different keys would result in 120 artificial examples generated out of one annotated chord progression. The voicings suggested for different keys do not necessarily overlap, given that the range of the voices and other variables are taken into account by the rule-based algorithm. This results in data augmentation with potentially unique voicings in each key, contrary to what would be obtained by simply transposing the artificial examples to a different key. We show the process of applying this technique to a dataset of annotated Bach chorales from the KernScores website. Similar datasets with roman numeral analysis annotations could be used with this approach to generate a large number of artificial training examples for training machine learning models.","authors":["N\u00e9stor N\u00e1poles L\u00f3pez","Ichiro Fujinaga"],"bilibili_id":"","channel_name":"lbd-7-432-n\u00e1poles-l\u00f3pez","channel_url":"https://ismir2020.slack.com/archives/C01CCMPREUB","day":4,"poster_type":"pdf","session":["7"],"title":"Harmonic Reductions As a Strategy for Creative Data Augmentation","youtube_id":""},"forum":"432","id":"432"},{"content":{"TLDR":"The global music market moves billions of dollars every year, most of which comes from streaming platforms. In this paper, I present a model for predicting whether or not a song will appear in Spotify's Top 50 ranking. To make this prediction, I trained different classifiers with information of audio features from songs that appeared in this ranking between November 2018 and January 2019. When tested with data from June and July 2019, an SVM classifier with RBF kernel obtained accuracy and AUC above 80%.","abstract":"The global music market moves billions of dollars every year, most of which comes from streaming platforms. In this paper, I present a model for predicting whether or not a song will appear in Spotify's Top 50 ranking. To make this prediction, I trained different classifiers with information of audio features from songs that appeared in this ranking between November 2018 and January 2019. When tested with data from June and July 2019, an SVM classifier with RBF kernel obtained accuracy and AUC above 80%.","authors":["Carlos V Soares Araujo"],"bilibili_id":"","channel_name":"lbd-7-433-araujo","channel_url":"https://ismir2020.slack.com/archives/C01CCFLGC66","day":4,"poster_type":"pdf","session":["7"],"title":"A Model for Predicting Music Popularity on Spotify","youtube_id":""},"forum":"433","id":"433"},{"content":{"TLDR":"Time-scale modification (TSM) is a digital audio effect that adjusts the length of an audio signal while preserving its pitch. The TSM audio effect is widely used in not only sound production but also music and audio research such as for data augmentation. In this paper, we present PyTSMod, an open-source Python library that implements several different classical TSM algorithms. We expect that PyTSMod can help MIR and audio researchers easily use the TSM algorithms in the Python-based environment.","abstract":"Time-scale modification (TSM) is a digital audio effect that adjusts the length of an audio signal while preserving its pitch. The TSM audio effect is widely used in not only sound production but also music and audio research such as for data augmentation. In this paper, we present PyTSMod, an open-source Python library that implements several different classical TSM algorithms. We expect that PyTSMod can help MIR and audio researchers easily use the TSM algorithms in the Python-based environment.","authors":["Sangeon Yong","Soonbeom Choi","Juhan Nam"],"bilibili_id":"BV1fk4y1C7Yu","channel_name":"lbd-7-434-yong","channel_url":"https://ismir2020.slack.com/archives/C01D2AZDLKA","day":4,"poster_type":"html","session":["7"],"title":"PyTSMod: a Python Implementation of Time-scale Modification Algorithms","youtube_id":"AhBQ8boYv_I"},"forum":"434","id":"434"},{"content":{"TLDR":"We introduce the Children's Song Dataset (CSD) which contains vocal recordings for 100 children's songs sung in Korean or English and are temporally aligned with the MIDI transcriptions and lyrics annotations. We expect that the dataset can be useful for various singing voice analysis and synthesis tasks.","abstract":"We introduce the Children's Song Dataset (CSD) which contains vocal recordings for 100 children's songs sung in Korean or English and are temporally aligned with the MIDI transcriptions and lyrics annotations. We expect that the dataset can be useful for various singing voice analysis and synthesis tasks.","authors":["Soonbeom Choi"],"bilibili_id":"","channel_name":"lbd-7-435-choi","channel_url":"https://ismir2020.slack.com/archives/C01CR4Q2RMX","day":4,"poster_type":"html","session":["7"],"title":"Children\u2019s Song Dataset for Singing Voice Research","youtube_id":""},"forum":"435","id":"435"},{"content":{"TLDR":"We benchmark here several convolution kernels, particulary trying custom dilated convolutions, and show how convolutions following pitch spaces such as the Tonnetz may help the learning of musical tasks.","abstract":"We benchmark here several convolution kernels, particulary trying custom dilated convolutions, and show how convolutions following pitch spaces such as the Tonnetz may help the learning of musical tasks.","authors":["Rony Abecidan","Mathieu Giraud","Gianluca Micchi"],"bilibili_id":"","channel_name":"lbd-8-436-giraud","channel_url":"https://ismir2020.slack.com/archives/C01D2AZDV40","day":4,"poster_type":"pdf","session":["8"],"title":"Towards Custom Dilated Convolutions on Pitch Spaces","youtube_id":""},"forum":"436","id":"436"},{"content":{"TLDR":"Hierarchical and reductive analyses are central methods for music modeling and anaysis. However, to this day, no dedicated software exists to support analysts with this task. Here, we present a prototype of a generic tool for hierarchical analysis of scores in the Music Encoding Initiative (MEI) format. We use the rendering engine Verovio to render an MEI XML structure to a Scalable Vector Graphics (SVG) which is presented to the user. By selecting and manipulating the SVG elements using the tool, we add hierarchical analysis metadata to the MEI following the scheme proposed by Rizo and Marsden. We hope this tool will support professionals in performing hierarchical music annotations as well as find use in educational contexts.","abstract":"Hierarchical and reductive analyses are central methods for music modeling and anaysis. However, to this day, no dedicated software exists to support analysts with this task. Here, we present a prototype of a generic tool for hierarchical analysis of scores in the Music Encoding Initiative (MEI) format. We use the rendering engine Verovio to render an MEI XML structure to a Scalable Vector Graphics (SVG) which is presented to the user. By selecting and manipulating the SVG elements using the tool, we add hierarchical analysis metadata to the MEI following the scheme proposed by Rizo and Marsden. We hope this tool will support professionals in performing hierarchical music annotations as well as find use in educational contexts.","authors":["Petter Ericson","Martin Rohrmeier"],"bilibili_id":"","channel_name":"lbd-8-437-ericson","channel_url":"https://ismir2020.slack.com/archives/C01C5PHNSNS","day":4,"poster_type":"pdf","session":["8"],"title":"Hierarchical Annotation of MEI-encoded Sheet Music","youtube_id":""},"forum":"437","id":"437"},{"content":{"TLDR":"To date, most research in automatic musical structure analysis and segmentation from audio has been con-ducted using established audio features with boundary detection and clustering methods. In other Music IR tasks, we have seen significant advances in performance using deep learning, specifically convolutional neural networks -- networks pioneered in image understanding problems. Several experiments have examined the \u201cimages\u201d of musical self-similarity, computed from acoustic features, which can provide a compelling visual representation of overall musical structure, but remain difficult to automatically interpret. In a recent trial, we directly trained and applied a robust visual object detection network on layered self-similarity matrices to identify musical segments (intro, verse, chorus) of a song, and highlight them using bounding boxes as done in visual object detection. We performed an initial assessment of this task using an \u201coff-the-shelf\u201d implementation, Facebook AI Research\u2019s Detectron2, of the Faster R-CNN recognition method on SSMs generated using audio features as separate \u201ccolor channels\u201d. Preliminary results of the system and general knowledge of this task provide some indications that visual object detection methods examining the entire SSM may allow us to characterize musical sections and reveal indicative features identifying different segments.","abstract":"To date, most research in automatic musical structure analysis and segmentation from audio has been con-ducted using established audio features with boundary detection and clustering methods. In other Music IR tasks, we have seen significant advances in performance using deep learning, specifically convolutional neural networks -- networks pioneered in image understanding problems. Several experiments have examined the \u201cimages\u201d of musical self-similarity, computed from acoustic features, which can provide a compelling visual representation of overall musical structure, but remain difficult to automatically interpret. In a recent trial, we directly trained and applied a robust visual object detection network on layered self-similarity matrices to identify musical segments (intro, verse, chorus) of a song, and highlight them using bounding boxes as done in visual object detection. We performed an initial assessment of this task using an \u201coff-the-shelf\u201d implementation, Facebook AI Research\u2019s Detectron2, of the Faster R-CNN recognition method on SSMs generated using audio features as separate \u201ccolor channels\u201d. Preliminary results of the system and general knowledge of this task provide some indications that visual object detection methods examining the entire SSM may allow us to characterize musical sections and reveal indicative features identifying different segments.","authors":["Christopher N Uzokwe","Youngmoo Kim"],"bilibili_id":"","channel_name":"lbd-7-438-uzokwe","channel_url":"https://ismir2020.slack.com/archives/C01BXP8HUS3","day":4,"poster_type":"pdf","session":["7"],"title":"Musical Structure Analysis Using Image Segmentation Networks","youtube_id":""},"forum":"438","id":"438"},{"content":{"TLDR":"We present the integration of various CNN TensorFlow models developed for different MIR tasks into Essentia. This is a continuation of our previous work, extending the list of supported models and adding new algorithms to facilitate usability. Essentia provides input feature extraction and inference with TensorFlow models in a single C++ pipeline with Python bindings, the overhead of Python bindings and facilitating the deployment of C++ and Python MIR applications. We assess the new models' capabilities to serve as embedding extractors in many downstream classification tasks. All presented models are publicly available on the Essentia website.","abstract":"We present the integration of various CNN TensorFlow models developed for different MIR tasks into Essentia. This is a continuation of our previous work, extending the list of supported models and adding new algorithms to facilitate usability. Essentia provides input feature extraction and inference with TensorFlow models in a single C++ pipeline with Python bindings, the overhead of Python bindings and facilitating the deployment of C++ and Python MIR applications. We assess the new models' capabilities to serve as embedding extractors in many downstream classification tasks. All presented models are publicly available on the Essentia website.","authors":["Pablo Alonso-Jim\u00e9nez","Dmitry Bogdanov","Xavier Serra"],"bilibili_id":"BV1Wi4y1j7VW/","channel_name":"lbd-8-439-alonso-jim\u00e9nez","channel_url":"https://ismir2020.slack.com/archives/C01BXP8HZ6K","day":4,"poster_type":"pdf","session":["8"],"title":"Deep Embeddings with Essentia Models","youtube_id":"hzaCpiudF3c"},"forum":"439","id":"439"},{"content":{"TLDR":"We introduce a deep neural network design for the unsupervised pitch estimation of acoustic guitar chords. The proposed system takes in a short audio clip containing a guitar chord or note and produces estimates for the pitches present and their amplitudes. It trains without requiring labeled data. In an analysis part of the network, a convolutional neural network produces pitch estimates from an input spectrogram. These pitch estimates are fed into a synthesis part that attempts to reconstruct the original input. The analyzer trains while the synthesizer remains fixed, and a reconstruction loss is minimized. As the network improves its reconstructions, it learns to produce accurate pitch estimates. We discuss two variants for the synthesis part: component note synthesis and Karplus-Strong synthesis. We hope that insights from this work can be integrated into a full network for unsupervised acoustic guitar transcription.","abstract":"We introduce a deep neural network design for the unsupervised pitch estimation of acoustic guitar chords. The proposed system takes in a short audio clip containing a guitar chord or note and produces estimates for the pitches present and their amplitudes. It trains without requiring labeled data. In an analysis part of the network, a convolutional neural network produces pitch estimates from an input spectrogram. These pitch estimates are fed into a synthesis part that attempts to reconstruct the original input. The analyzer trains while the synthesizer remains fixed, and a reconstruction loss is minimized. As the network improves its reconstructions, it learns to produce accurate pitch estimates. We discuss two variants for the synthesis part: component note synthesis and Karplus-Strong synthesis. We hope that insights from this work can be integrated into a full network for unsupervised acoustic guitar transcription.","authors":["Andrew F Wiggins","Youngmoo Kim"],"bilibili_id":"","channel_name":"lbd-7-440-wiggins","channel_url":"https://ismir2020.slack.com/archives/C01BXP8J3NK","day":4,"poster_type":"pdf","session":["7"],"title":"Towards Unsupervised Acoustic Guitar Transcription","youtube_id":""},"forum":"440","id":"440"},{"content":{"TLDR":"We present a visualization tool for Convolutional Neural Networks focused on the task of instrument recognition. This tool allows you to visualize the network response layer by layer to a specific input sample as an array of animated activation plots corresponding to nodes, or filters, in the network. The recognition of instruments from audio, particularly in ensemble mixtures, remains a challenging and important problem fundamental to the field of music information retrieval. Early solutions to this problem focused heavily on designing task specific input features. These features were very well defined, however, their performance does not come close to state-of-the-art deep learning approaches such as convolutional neural networks, multi-task approaches, and transfer learning. However, the reported results of these black-box networks generally focus on overall performance across a dataset and ignore underlying instrument class performance disparities, which may overlook deeper issues with these approaches. Recently these types of deep learning approaches have become de facto standards for solving a wide variety of problems in the field of MIR. Still the underlying feature representations learned by these networks are not well understood in deep learning problems at large and even less in audio and spectrogram input specific cases. Our goal is to apply deep network and CNN analysis tools to the problem of predominant instrument recognition and create an analysis tool widely applicable and useful for MIR specific deep learning models.","abstract":"We present a visualization tool for Convolutional Neural Networks focused on the task of instrument recognition. This tool allows you to visualize the network response layer by layer to a specific input sample as an array of animated activation plots corresponding to nodes, or filters, in the network. The recognition of instruments from audio, particularly in ensemble mixtures, remains a challenging and important problem fundamental to the field of music information retrieval. Early solutions to this problem focused heavily on designing task specific input features. These features were very well defined, however, their performance does not come close to state-of-the-art deep learning approaches such as convolutional neural networks, multi-task approaches, and transfer learning. However, the reported results of these black-box networks generally focus on overall performance across a dataset and ignore underlying instrument class performance disparities, which may overlook deeper issues with these approaches. Recently these types of deep learning approaches have become de facto standards for solving a wide variety of problems in the field of MIR. Still the underlying feature representations learned by these networks are not well understood in deep learning problems at large and even less in audio and spectrogram input specific cases. Our goal is to apply deep network and CNN analysis tools to the problem of predominant instrument recognition and create an analysis tool widely applicable and useful for MIR specific deep learning models.","authors":["Charis Cochran","Youngmoo Kim"],"bilibili_id":"BV1zt4y1v7Sb","channel_name":"lbd-7-441-cochran","channel_url":"https://ismir2020.slack.com/archives/C01CCMPSK4K","day":4,"poster_type":"pdf","session":["7"],"title":"Visualization of Deep Networks for Musical Instrument Recognition","youtube_id":"DyLMOSfd8OI"},"forum":"441","id":"441"},{"content":{"TLDR":"We introduce a new algorithm to quantify the \"helicality\" between frequency sub-bands in an audio dataset using an isomap-based measure.","abstract":"We introduce a new algorithm to quantify the \"helicality\" between frequency sub-bands in an audio dataset using an isomap-based measure.","authors":["Sripathi Sridhar","Vincent Lostanlen"],"bilibili_id":"BV1h54y1R7rX","channel_name":"lbd-7-442-sridhar","channel_url":"https://ismir2020.slack.com/archives/C01BXP8JB8F","day":4,"poster_type":"pdf","session":["7"],"title":"Helicality: an Isomap-based Measure of Octave Equivalence in Audio Data","youtube_id":"76ukCoKg4_k"},"forum":"442","id":"442"},{"content":{"TLDR":"Identification and availability of suitable data sources is a well-known difficulty in music information retrieval research. For studies requiring annotated data, this can be compounded by inconsistent presentation formats, differences in methodologies, and annotation errors. By building a framework to apply automated data cleansing and standardization techniques to a collection of MIREX evaluation output data, I was able to extract a large, labelled chord data set for use in a harmonic modelling study.","abstract":"Identification and availability of suitable data sources is a well-known difficulty in music information retrieval research. For studies requiring annotated data, this can be compounded by inconsistent presentation formats, differences in methodologies, and annotation errors. By building a framework to apply automated data cleansing and standardization techniques to a collection of MIREX evaluation output data, I was able to extract a large, labelled chord data set for use in a harmonic modelling study.","authors":["Jeffrey K Miller","Johan Pauwels","Mark Sandler"],"bilibili_id":"","channel_name":"lbd-8-443-miller","channel_url":"https://ismir2020.slack.com/archives/C01CCMPSRTM","day":4,"poster_type":"pdf","session":["8"],"title":"A Data-cleansing Framework for Aggregating Annotated Datasets from MIREX Automated Chord Estimation Archives","youtube_id":""},"forum":"443","id":"443"},{"content":{"TLDR":"Research in automatic music transcription (AMT) showed significant improvement thanks to advances in deep learning. However, most of the research was designed for an offline scenario, where the input audio recording is provided from beginning to end. In this paper, we present an online piano AMT system with visualization using a web browser and MIDI export which works on CPU in real-time. We employed a model with auto-regressive LSTM and multi-note-state, which is adapted for an online scenario without losing its accuracy.","abstract":"Research in automatic music transcription (AMT) showed significant improvement thanks to advances in deep learning. However, most of the research was designed for an offline scenario, where the input audio recording is provided from beginning to end. In this paper, we present an online piano AMT system with visualization using a web browser and MIDI export which works on CPU in real-time. We employed a model with auto-regressive LSTM and multi-note-state, which is adapted for an online scenario without losing its accuracy.","authors":["Dasaem Jeong"],"bilibili_id":"BV1q541157Un","channel_name":"lbd-7-444-jeong","channel_url":"https://ismir2020.slack.com/archives/C01CR4Q3V7B","day":4,"poster_type":"pdf","session":["7"],"title":"Real-time Automatic Piano Music Transcription System","youtube_id":"ofaafYKJ5lA"},"forum":"444","id":"444"}]
