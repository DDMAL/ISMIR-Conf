[{"UID":"5","abstract":"This paper describes a statistical music structure analysis method that splits an audio signal of popular music into musically meaningful sections at the beat level and classifies them into predefined categories such as intro, verse, and chorus, where beat times are assumed to be estimated in advance. A basic approach to this task is to train a recurrent neural network (e.g., long short-term memory (LSTM) network) that directly predicts section labels from acoustic features. This approach, however, suffers from frequent musically unnatural label switching because the homogeneity, repetitiveness, and duration regularity of musical sections are hard to represent explicitly in the network architecture. To solve this problem, we formulate a unified hidden semi-Markov model (HSMM) that represents the generative process of homogeneous mel-frequency cepstrum coefficients, repetitive chroma features, and mel spectra from section labels, where the emission probabilities of mel spectra are computed from the posterior probabilities of section labels predicted by an LSTM. Given these acoustic features, the most likely label sequence can be estimated with Viterbi decoding. The experimental results show that the proposed LSTM-HSMM hybrid model outperformed a conventional HSMM.","affiliations":" Kyoto University|Kyoto University|Kyoto University","authors":"Go Shibata|Ryo Nishikimi|Kazuyoshi Yoshii","keywords":"Musical features and properties|Structure, segmentation, and form","primary_author":"Go Shibata","primary_email":"gshibata@sap.ist.i.kyoto-u.ac.jp","session":"1A","title":"Music Structure Analysis Based on an LSTM-HSMM Hybrid Model","type":"paper"},{"UID":"6","abstract":"This paper describes a clustering-based music transcription method that estimates the piano rolls of arbitrary musical instrument parts from multi-instrument polyphonic music signals. If target musical pieces are always played by particular kinds of musical instruments, a way to obtain piano rolls is to compute the pitchgram (pitch saliency spectrogram) of each musical instrument by using a deep neural network (DNN). However, this approach has a critical limitation that it has no way to deal with musical pieces including undefined musical instruments. To overcome this limitation, we estimate a condensed pitchgram with an existing instrument-independent neural multi-pitch estimator and then separate the pitchgram into a specified number of musical instrument parts with a deep spherical clustering technique. To improve the performance of transcription, we propose a joint spectrogram and pitchgram clustering method based on the timbral and pitch characteristics of musical instruments. The experimental results show that the proposed method can transcribe musical pieces including unknown musical instruments as well as those containing only predefined instruments, at the state-of-the-art transcription accuracy.","affiliations":"Waseda University|Waseda University|Kyoto University|Kyoto University|Waseda Research Institute for Science and Engineering","authors":"Keitaro Tanaka|Takayuki Nakatsuka|Ryo Nishikimi|Kazuyoshi Yoshii|Shigeo Morishima","keywords":"MIR tasks|Music transcription and annotation","primary_author":"Keitaro Tanaka","primary_email":"phys.keitaro1227@ruri.waseda.jp","session":"1A","title":"Multi-Instrument Music Transcription Based on Deep Spherical Clustering of Spectrograms and Pitchgrams","type":"paper"},{"UID":"12","abstract":"This paper explores sequential modelling of polyphonic music with deep neural networks. While recent breakthroughs have focussed on network architecture, we demonstrate that the representation of the sequence can make an equally significant contribution to the performance of the model as measured by validation set loss. By extracting salient features inherent to the training dataset, the model can either be conditioned on these features or trained to predict said features as extra components of the sequences being modelled. We show that training a neural network to predict a seemingly more complex sequence, with extra features included in the series being modelled, can improve overall model performance significantly. We first introduce TonicNet, a GRU-based model trained to initially predict the chord at a given time-step before then predicting the notes of each voice at that time-step, in contrast with the typical approach of predicting only the notes. We then evaluate TonicNet on the canonical JSB Chorales dataset and obtain state-of-the-art results.","affiliations":"Humtap, Inc.","authors":"Omar A Peracha","keywords":"MIR tasks|Pattern matching and detection|Applications|Music composition, performance, and production","primary_author":"Omar A Peracha","primary_email":"omar.peracha@gmail.com","session":"1A","title":"IMPROVING POLYPHONIC MUSIC MODELS WITH FEATURE-RICH ENCODING","type":"paper"},{"UID":"13","abstract":"Studies have shown that repeated exposures to novel songs cause an increase in a person\u201a\u00c4\u00f4s memory and liking. These studies are commonly verified through self-reporting emotion-based surveys. This paper proposes the \u201a\u00c4\u00faretention rate\u201a\u00c4\u00f9 as an additional parameter for evaluation. The \u201a\u00c4\u00faretention rate\u201a\u00c4\u00f9 is one at which the listener revisits the novel items. The authors hypothesize that when a person listens to novel (i.e., both unfamiliar and interesting) pieces of music, the retention rate will be proportional to the number of times the discovery engine suggests the pieces to her, as long as they remain novel. The authors have tested the hypothesis through a six-week human-subject experiment that simulates a real-world listening environment and a follow-up survey. During the experiment period, each subject received, through Discover Weekly in Spotify, suggestions for novel songs up to three times and provided evaluation. One month after the evaluation experiment, the human-subjects answered whether they had revisited the novel songs. Through the analysis of the response and survey data, the researchers conclude that the more times a listener is exposed to a song during the discovery process, the more likely she is to return to the song.","affiliations":"University of Miami|University of Miami","authors":"Brian Manolovitz|Mitsunori Ogihara","keywords":"Human-centered MIR|User-centered evaluation|Applications|Music recommendation and playlist generation|Personalization|User behavior analysis and mining, user modeling","primary_author":"Brian Manolovitz","primary_email":"bmm157@miami.edu","session":"1A","title":"Practical Evaluation of Repeated Recommendations in Personalized Music Discovery","type":"paper"},{"UID":"14","abstract":"Even though local tempo estimation promises musicological insights into expressive musical performances, it has never received as much attention in the music information retrieval (MIR) research community as either beat tracking or global tempo estimation. One reason for this may be the lack of a generally accepted definition. In this paper, we discuss how to model and measure local tempo in a musically meaningful way using a cross-version dataset of Fr\u221a\u00a9d\u221a\u00a9ric Chopin\u201a\u00c4\u00f4s Mazurkas as a use case. In particular, we explore how tempo stability can be measured and taken into account during evaluation. Comparing existing and newly trained systems, we find that CNN-based approaches can accurately measure local tempo even for expressive classical music, if trained on the target genre. Furthermore, we show that different training\u201a\u00c4\u00ectest splits have a considerable impact on accuracy for difficult segments.","affiliations":"International Audio Laboratories Erlangen|International Audio Laboratories Erlangen|International Audio Laboratories Erlangen","authors":"Hendrik Schreiber|Frank Zalkow|Meinard M\u221a\u00baller","keywords":"Musical features and properties|Rhythm, beat, tempo|Applications|Digital libraries and archives|Domain knowledge|Computational music theory and musicology|Machine learning/Artificial intelligence for music|Evaluation, datasets, and reproducibility|Evaluation metrics","primary_author":"Hendrik Schreiber","primary_email":"hendrik.schreiber@audiolabs-erlangen.de","session":"1A","title":"Modeling and Estimating Local Tempo: A Case Study on Chopin\u201a\u00c4\u00f4s Mazurkas","type":"paper"},{"UID":"19","abstract":"From the 19th century on, several composers of Western opera made use of leitmotifs (short musical ideas referring to semantic entities such as characters, places, items, or feelings) for guiding the audience through the plot and illustrating the events on stage. A prime example of this compositional technique is Richard Wagner\u201a\u00c4\u00f4s four-opera cycle Der Ring des Nibelungen. Across its different occurrences in the score, a leitmotif may undergo considerable musical variations. The concrete leitmotif instances in an audio recording are subject to acoustic variability. Our paper approaches the task of classifying such leitmotif instances in audio recordings. As our main contribution, we conduct a case study on a dataset covering 16 recorded performances of the Ring with annotations of ten central leitmotifs, leading to 2403 occurrences and 38448 instances in total. We build a neural network classification model and evaluate its ability to generalize across different performances and leitmotif occurrences. Our findings demonstrate the possibilities and limitations of leitmotif classification in audio recordings and pave the way towards the fully automated detection of leitmotifs in music recordings.","affiliations":"International Audio Laboratories Erlangen|International Audio Laboratories Erlangen|International Audio Laboratories Erlangen|International Audio Laboratories Erlangen","authors":"Michael Krause|Frank Zalkow|Christof Weiss|Meinard M\u221a\u00baller","keywords":"MIR tasks|Automatic classification|Applications|Music retrieval systems|Domain knowledge|Machine learning/Artificial intelligence for music|Evaluation, datasets, and reproducibility|Novel datasets and use cases|Pattern matching and detection|Musical features and properties|Melody and motives","primary_author":"Michael Krause","primary_email":"michael.krause@audiolabs-erlangen.de","session":"1A","title":"Classifying Leitmotifs in Recordings of Operas by Richard Wagner","type":"paper"},{"UID":"21","abstract":"This paper studies composer style classification of piano sheet music images.  Previous approaches to the composer classification task have been limited by a scarcity of data.  We address this issue in two ways: (1) we recast the problem to be based on raw sheet music images rather than a symbolic music format, and (2) we propose an approach that can be trained on unlabeled data.  Our approach first converts the sheet music image into a sequence of musical ``words\" based on the bootleg feature representation, and then feeds the sequence into a text classifier.  We show that it is possible to significantly improve classifier performance by first training a language model on a set of unlabeled data, initializing the classifier with the pretrained language model weights, and then finetuning the classifier on a small amount of labeled data.  We train AWD-LSTM, GPT-2, and RoBERTa language models on all piano sheet music images in IMSLP.  We find that transformer-based architectures outperform CNN and LSTM models, and pretraining boosts classification accuracy for the GPT-2 model from 46% to 70% on a 9-way classification task.  The trained model can also be used as a feature extractor that projects piano sheet music into a feature space that characterizes compositional style.","affiliations":"Harvey Mudd College|Harvey Mudd College","authors":"Timothy Tsai|Kevin Ji","keywords":"Musical features and properties|Musical style and genre|Domain knowledge|Machine learning/Artificial intelligence for music|Representations of music|MIR fundamentals and methodology|Symbolic music processing|MIR tasks|Automatic classification","primary_author":"Timothy Tsai","primary_email":"ttsai@g.hmc.edu","session":"1A","title":"Composer Style Classification of Piano Sheet Music Images Using Language Model Pretraining","type":"paper"},{"UID":"23","abstract":"Many music information retrieval tasks involve the comparison of a symbolic score representation with an audio recording. A typical strategy is to compare score\u201a\u00c4\u00ecaudio pairs based on a common mid-level representation, such as chroma features. Several recent studies demonstrated the effectiveness of deep learning models that learn task-specific mid-level representations from temporally aligned training pairs. However, in practice, there is often a lack of strongly aligned training data, in particular for real-world scenarios. In our study, we use weakly aligned score\u201a\u00c4\u00ecaudio pairs for training, where only the beginning and end of a score excerpt is annotated in an audio recording, without aligned correspondences in between. To exploit such weakly aligned data, we employ the Connectionist Temporal Classification (CTC) loss to train a deep learning model for computing an enhanced chroma representation. We then apply this model to a cross-modal retrieval task, where we aim at finding relevant audio recordings of Western classical music, given a short monophonic musical theme in symbolic notation as a query. We present systematic experiments that show the effectiveness of the CTC-based model for this theme-based retrieval task.","affiliations":"International Audio Laboratories Erlangen|International Audio Laboratories Erlangen","authors":"Frank Zalkow|Meinard M\u221a\u00baller","keywords":"Applications|Music retrieval systems|Domain knowledge|Machine learning/Artificial intelligence for music|Representations of music|MIR fundamentals and methodology|Music signal processing|MIR tasks|Alignment, synchronization, and score following|Musical features and properties|Harmony, chords, and tonality","primary_author":"Frank Zalkow","primary_email":"frank.zalkow@audiolabs-erlangen.de","session":"1A","title":"Using Weakly Aligned Score\u201a\u00c4\u00ecAudio Pairs to Train Deep Chroma Models for Cross-Modal Music Retrieval","type":"paper"},{"UID":"24","abstract":"This paper presents a method for large-scale retrieval of piano sheet music images.  Our work differs from previous studies on sheet music retrieval in two ways.  First, we investigate the problem at a much larger scale than previous studies, using all solo piano sheet music images in the entire IMSLP dataset as a searchable database.  Second, we use cell phone images of sheet music as our input queries, which lends itself to a practical, user-facing application.  We show that a previously proposed fingerprinting method for sheet music retrieval is far too slow for a real-time application, and we diagnose its shortcomings.  We propose a novel hashing scheme called dynamic n-gram fingerprinting that significantly reduces runtime while simultaneously boosting retrieval accuracy.  In experiments on IMSLP data, our proposed method achieves a mean reciprocal rank of 0.85 and an average runtime of 0.98 seconds per query.","affiliations":"Harvey Mudd College|Harvey Mudd College","authors":"Daniel Yang|Timothy Tsai","keywords":"MIR tasks|Fingerprinting|Applications|Music retrieval systems|Domain knowledge|Representations of music|Indexing and querying","primary_author":"Timothy Tsai","primary_email":"ttsai@g.hmc.edu","session":"1A","title":"Camera-Based Piano Sheet Music Identification","type":"paper"},{"UID":"25","abstract":"While prior studies investigating the social aspects of music provide a landscape of users\u201a\u00c4\u00f4 various social behaviors around commercial music services (CMS), there remains a lack in understanding of users\u201a\u00c4\u00f4 perceptions and value judgments underlying these behaviors. Specifically, there is more to learn about what influences and behaviors individual music users perceive as meaningful in social contexts. We used the Q methodology to explore which behaviors and influences are important to CMS users and why. We extracted two factors that explain the two different viewpoints shared by groups of music users, focusing on how they perceive the meaning and value of different social music behavior and interactions. From these findings, we then revise an existing social music coding dictionary and interaction model and offer new CMS design insights.","affiliations":"University of Washington|University of Washington|University of Washington","authors":"Louis Spinelli|Josephine Lau|Jin Ha Lee","keywords":"Human-centered MIR|Human-computer interaction and interfaces","primary_author":"Louis Spinelli","primary_email":"spinelli@uw.edu","session":"1A","title":"USER PERCEPTIONS UNDERLYING SOCIAL MUSIC BEHAVIOR","type":"paper"},{"UID":"28","abstract":"Music copyright lawsuits often result in multimillion dollar damage awards or settlements, yet there are few objective guidelines for applying copyright law in in-fringement claims involving musical works. Recent re-search has attempted to develop objective methods based on automated similarity algorithms, but there remains almost no data on the role of perceived similarity in mu-sic copyright decisions despite its crucial role in copy-right law. We collected perceptual data from 20 partici-pants for 17 adjudicated copyright cases from the USA and Japan after editing the disputed sections to contain either full audio, melody only, or lyrics only. Due to the historical emphasis in legal opinions on melody as the key criterion for deciding infringement, we predicted that listening to melody-only versions would result in percep-tual judgements that more closely matched actual past legal decisions. Surprisingly, however, we found no sig-nificant differences between the three conditions, with participants matching past decisions in between 50-60% of cases in all three conditions. Automated algorithms designed to calculate melodic and audio similarity pro-duced comparable results: both algorithms were able to match past decisions with identical accuracy of 71% (12/17 cases). Analysis of cases that were difficult to classify suggests that melody, lyrics, and other factors sometimes interact in complex ways difficult to capture using quantitative metrics. We propose directions for fur-ther investigation of the role of similarity in music copy-right law using larger and more diverse samples of cases and enhanced methods, and adapting our perceptual ex-periment method to avoid relying for ground truth data only on court decisions (which may be subject to selec-tion bias). Our results contribute to important practical debates, such as whether jury members should be allowed to listen to full audio recordings during copyright cases. ","affiliations":"Keio University|Keio University|George Washing University Law School|Goldsmiths|University of Auckland|Keio University|Keio University","authors":"Yuchen Yuan|Sho Oishi|Charles Cronin|Daniel M\u221a\u00ballensiefen|Quentin Atkinson|Shinya Fujii|Patrick E. Savage","keywords":"Domain knowledge|Cognitive MIR|Applications|Business and marketing|MIR tasks|Similarity metrics|Musical features and properties|Melody and motives|Philosophical and ethical discussions|Legal and societal aspects of MIR","primary_author":"Patrick E. Savage","primary_email":"psavage@sfc.keio.ac.jp","session":"1A","title":"Perceptual vs. automated judgements of music copyright infringement","type":"paper"},{"UID":"33","abstract":"Accurate and flexible representations of music data are paramount to addressing MIR tasks, yet many of the existing approaches are difficult to interpret or rigid in nature. This work introduces two new song representations for structure-based retrieval methods: Surface Pattern Preservation (SuPP), a continuous song representation, and Matrix Pattern Preservation (MaPP), SuPP\u201a\u00c4\u00f4s discrete counterpart. These representations come equipped with several user-defined parameters so that they are adaptable for a range of MIR tasks. Experimental results show MaPP as successful in addressing the cover song task on a set of Mazurka scores, with a mean precision of 0.965 and recall of 0.776. SuPP and MaPP also show promise in other MIR applications, such as novel-segment detection and genre classification, the latter of which demonstrates their suitability as inputs for machine learning problems.","affiliations":"University of Colorado, Boulder|Carnegie Mellon University|Brown University |Smith College","authors":"Claire Savard|Erin H Bugbee|Melissa R McGuirl|Katherine M. Kinnaird","keywords":"Domain knowledge|Representations of music|Evaluation, datasets, and reproducibility|MIR tasks|Fingerprinting|Pattern matching and detection|Similarity metrics|Musical features and properties|Structure, segmentation, and form","primary_author":"Claire Savard","primary_email":"claire.savard@colorado.edu","session":"1A","title":"SuPP & MaPP: Adaptable Structure-Based Representations for MIR Tasks","type":"paper"},{"UID":"40","abstract":"This paper focuses on the computational study of figured bass, which remains an under-researched topic in MIR, likely due to a lack of machine-readable datasets. First, we introduce the Bach Chorales Figured Bass dataset (BCFB), a collection of 139 chorales composed by Johann Sebastian Bach that includes both the original music and figured bass annotations encoded in MusicXML, **kern, and MEI formats. We also present a comparative study on automatic figured bass annotation using both rule-based and machine learning approaches, which respectively achieved classification accuracies of 85.3% and 85.9% on BCFB. Finally, we discuss promising areas for MIR research involving figured bass, including automatic harmonic analysis. ","affiliations":"McGill University|McGill University|Marianopolis College|University of Utah|McGill University","authors":"Yaolong Ju|Sylvain Margot|Cory McKay|Luke Dahn|Ichiro Fujinaga","keywords":"Evaluation, datasets, and reproducibility|Novel datasets and use cases|Domain knowledge|Machine learning/Artificial intelligence for music|MIR fundamentals and methodology|Symbolic music processing|Musical features and properties|Harmony, chords, and tonality","primary_author":"Yaolong Ju","primary_email":"yaolong.ju@mail.mcgill.ca","session":"1A","title":"Automatic Figured Bass Annotation Using the New Bach Chorales Figured Bass Dataset","type":"paper"},{"UID":"41","abstract":"In this paper, we build on and extend a number of previous studies of rhythmic patterns that occur in ragtime music.  All of these studies have used the RAG-C dataset of approximately 11,000 symbolically-encoded ragtime pieces to identify salient rhythmic patterns in the corpus and  qualify how they are used.  Ragtime music is distinguished from other musical genres by frequent use of syncopation, and previous computational studies have confirmed a number of musicological hypotheses regarding the use of syncopated patterns in ragtime compositions.  In this work, we extend these studies to investigate further questions involving the use of syncopation.  Specifically, we introduce a new methodological framework for processing the RAG-C dataset and confirm that experiments from previous studies obtain similar results using the new methodology.  We investigate the use of the common ``short-long-short'' syncopated pattern in different time periods and present new results detailing its use by three well-known ragtime composers.  We describe how the use of other syncopated patterns has evolved over time and the different distributions of patterns that result from those changes.  Lastly, we present novel results identifying statistically significant patterns in the way composers varied the amount of syncopation in consecutive measures in compositions.","affiliations":"Rhodes College","authors":"Phillip B Kirlin","keywords":"MIR tasks|Pattern matching and detection|Applications|Music retrieval systems|Automatic classification|Indexing and querying|Music summarization|Similarity metrics","primary_author":"Phillip B Kirlin","primary_email":"kirlinp@rhodes.edu","session":"1A","title":"A corpus-based analysis of syncopated patterns in ragtime","type":"paper"},{"UID":"44","abstract":"While common approaches to automatic structural analysis of music typically focus on individual audio files, our approach collates audio features of large sets of related files in order to find a shared musical temporal structure. The content of each individual file and the differences between them can then be described in relation to this shared structure. We first construct a large similarity graph of temporal segments, such as beats or bars, based on self-alignments and selected pair-wise alignments between the given input files. Part of this graph is then partitioned into groups of corresponding segments using multiple sequence alignment. This partitioned graph is searched for recurring sections which can be organized hierarchically based on their co-occurrence. We apply our approach to discover shared harmonic structure in a dataset containing a large number of different live performances of a number of songs. Our evaluation shows that using the joint information from a number of files has the advantage of evening out the noisiness or inaccuracy of the underlying feature data and leads to a robust estimate of shared musical material.","affiliations":"Kyoto University|Kyoto University|Queen Mary University of London|Vrije Universitet|Queen Mary University of London","authors":"Florian Thalmann|Kazuyoshi Yoshii|Thomas Wilmering|Wiggins Geraint|Mark B. Sandler","keywords":"Musical features and properties|Structure, segmentation, and form|Domain knowledge|Computational music theory and musicology|Representations of music|MIR tasks|Pattern matching and detection|Harmony, chords, and tonality","primary_author":"Florian Thalmann","primary_email":"thalm007@umn.edu","session":"1A","title":"A Method for Analysis of Shared Structure in Large Music Collections using Techniques from Genetic Sequencing and Graph Theory","type":"paper"},{"UID":"46","abstract":"Singing Voice Separation (SVS) tries to separate singing voice from a given mixed musical signal. Recently, many U-Net-based models have been proposed for the SVS task, but there were no existing works that evaluate and compare various types of intermediate blocks that can be used in the U-Net architecture. In this paper, we introduce a variety of intermediate spectrogram transformation blocks. We implement U-nets based on these blocks and train them on complex-valued spectrograms to consider both magnitude and phase. These networks are then compared on the SDR metric. When using a particular block composed of convolutional and fully-connected layers, it achieves state-of-the-art SDR on the MUSDB singing voice separation task by a large margin of 0.9 dB. Our code and models are available online.","affiliations":"Korea University|Korea University|Korea National Open University|Seokyeong University|Korea University","authors":"Woosung Choi|Minseok  Kim|Jaehwa Chung|Daewon Lee|Soonyoung Jung","keywords":"MIR tasks|Sound source separation|Musical features and properties|Timbre, instrumentation, and voice","primary_author":"Woosung Choi","primary_email":"ws_choi@korea.ac.kr","session":"1A","title":"Investigating U-Nets with various Intermediate Blocks for Spectrogram-based Singing Voice Separation","type":"paper"},{"UID":"48","abstract":"We present the development and evaluation of a gamified rhythmic dictation application for music theory learning. The application's focus is on mobile accessibility and user experience, so it includes intuitive controls for input of rhythmic exercises, a responsive user interface, several gamification elements and a flexible exercise generator. We evaluated the rhythmic dictation application with conservatory-level music theory students through A/B testing, to assess their engagement and performance. The results show a significant impact of the application on the students' exam scores. ","affiliations":"University of Ljubljana|University of Ljubljana|Conservatory of Music and Ballet Ljubljana|University of Ljubljana","authors":"Matev\u2248\u00e6 Pesek|Lovro Suhadolnik|Peter \u2248\u2020avli|Matija Marolt","keywords":"Applications|Music training and education|Human-centered MIR|User-centered evaluation","primary_author":"Matev\u2248\u00e6 Pesek","primary_email":"matevz.pesek@fri.uni-lj.si","session":"1A","title":"The Rhythmic Dictator: Does Gamification of Rhythm Dictation Exercises Help?","type":"paper"},{"UID":"49","abstract":"This paper addresses the task of score following in sheet music given as unprocessed images. While existing work either relies on OMR software to obtain a computer-readable score representation, or crucially relies on prepared sheet image excerpts, we propose the first system that directly performs score following in full-page, completely unprocessed sheet images.  Based on incoming audio and a given image of the score, our system directly predicts the most likely position within the page that matches the audio, outperforming current state-of-the-art image-based score followers in terms of alignment precision. We also compare our method to an OMR-based approach and empirically show that it can be a viable alternative to such a system.","affiliations":"Johannes Kepler University Linz|Austrian Research Institute for Artificial Intelligence|Johannes Kepler University","authors":"Florian Henkel|Rainer Kelz|Gerhard Widmer","keywords":"MIR tasks|Alignment, synchronization, and score following|Domain knowledge|Machine learning/Artificial intelligence for music|MIR fundamentals and methodology|Multimodality","primary_author":"Florian Henkel","primary_email":"florian.henkel@jku.at","session":"1A","title":"Learning to Read and Follow Music in Complete Score Sheet Images","type":"paper"},{"UID":"57","abstract":"We propose an audio-to-audio generative model that learns to denoise old music recordings. Our model internally converts its input into a time-frequency representation by means of a short-time Fourier transform (STFT), and processes the resulting complex spectrogram using a convolutional neural network. The network is trained with both reconstruction and adversarial objectives on a synthetic noisy music dataset, which is created by mixing clean music with real noise samples extracted from quiet segments of old recordings. We evaluate our method quantitatively on held-out test examples of the synthetic dataset, and qualitatively by human rating on samples of actual historical recordings. Our results show that the proposed method is effective in removing noise, while preserving the quality and details of the original music.","affiliations":"Google|Google|Google|Google","authors":"Yunpeng Li|Marco Tagliasacchi|Beat Gfeller|Dominik Roblek","keywords":"Domain knowledge|Machine learning/Artificial intelligence for music|Applications|Digital libraries and archives|MIR fundamentals and methodology|Music signal processing|MIR tasks|Music synthesis and transformation","primary_author":"Yunpeng Li","primary_email":"yunpeng@google.com","session":"1A","title":"Learning to Denoise Historical Music","type":"paper"},{"UID":"58","abstract":"The computational analysis of music has traditionally seen a sharp divide between the \"audio approach\" relying on signal processing and the \"symbolic approach\" based on scores. Likewise, there has also been an unfortunate gap between any such computational endeavour and more traditional approaches as used in historical musicology. In this paper, we take a step towards ameliorating this situation through the application of a computational method for visualizing local key characteristics in audio recordings. We exploit these visualizations of diatonic scale content by discussing their musicological implications, being aware of methodological limitations as for the case of minor keys. As a proof of concept, we use this method for investigating differences between the traditional sonata-form model and selected Beethoven piano sonatas in the context of sonata theory from the end of the 18th century. We consider this scenario as an example for a rewarding dialogue between computer science and historical musicology.","affiliations":"International Audio Laboratories Erlangen|Institut f\u221a\u00bar Musikwissenschaft, Saarland University|Universit\u221a\u00a7t des Saarlandes|International Audio Laboratories Erlangen|Saarland University","authors":"Christof Weiss|Stephanie Klauk|Mark R H Gotham|Meinard M\u221a\u00baller|Rainer Kleinertz","keywords":"Domain knowledge|Computational music theory and musicology","primary_author":"Christof Weiss","primary_email":"christof.weiss@audiolabs-erlangen.de","session":"1A","title":"Discourse not Dualism: An Interdisciplinary Dialogue on Sonata Form in Beethoven's Early Piano Sonatas","type":"paper"},{"UID":"60","abstract":"The growing market of voice-enabled devices introduces new types of music search requests that can be more ambiguous than in typed search interfaces as voice assistants can potentially support conversational requests. However, these systems may not be able to fulfill ambiguous requests in a manner that matches the user need.  In this work, we study an example of ambiguous requests which we term as non-specific queries (NSQs), such as \"play music,\" where users ask to stream content using a single utterance that does not specify what content they want to hear.  To better understand user motivations for making NSQs, we conducted semi-structured qualitative interviews with voice users. We observed four themes that structure user perceptions of the benefits and shortcomings of making NSQs: the tradeoff between control and convenience, varying expectations for personalization, the effects of context on expectations, and learned user behaviors. We conclude with implications for how these themes can inform the interaction design of voice search systems in handling non-specific music requests in voice search systems. ","affiliations":"Spotify|UC Davis|Spotify|Spotify|Spotify","authors":"Jennifer Thom|Angela Nazarian|Ruth Brillman|Henriette Cramer|Sarah Mennicken","keywords":"Human-centered MIR|User-centered evaluation|Human-computer interaction and interfaces","primary_author":"Jennifer Thom","primary_email":"jennthom@spotify.com","session":"1A","title":"\"Play Music\": User Motivations and Expectations for Non-Specific Voice Queries","type":"paper"},{"UID":"61","abstract":"Investigating music with a focus on the similarity relations between songs, albums, and artists plays an important role when trying to understand trends in the history of music genres. In particular, representing these relations as a similarity network allows us to investigate the innovation presented by these entities in a multitude of points-of-view, including disruption. A disruptive object is one that creates a new stream of events, changing the traditional way of how a context usually works. The proper measurement of disruption remains as a task with large room for improvement, and these gaps are even more evident in the music domain, where the topic has not received much attention so far. This work builds on preliminary studies focused on the analysis of music disruption derived from metadata-based similarity networks, demonstrating that the raw audio can augment similarity information. We developed a case study based on a collection of a Brazilian local music tradition called Forr\u221a\u2265, that emphasizes the analytical and musicological potential of the musical disruption metric to describe and explain a genre trajectory over time.","affiliations":"Universidade Federal de Campina Grande|Universidade Federal de Campina Grande|UFMG|Universidade Federal de S\u221a\u00a3o Carlos|Universidade Federal de Campina Grande","authors":"Felipe V Falc\u221a\u00a3o|Nazareno Andrade|Flavio Figueiredo|Diego Furtado Silva|Fabio Morais","keywords":"Evaluation, datasets, and reproducibility|Evaluation metrics|Applications|Music retrieval systems|Evaluation methodology|MIR tasks|Novel datasets and use cases|Similarity metrics","primary_author":"Felipe V Falc\u221a\u00a3o","primary_email":"felipevf@copin.ufcg.edu.br","session":"1A","title":"Measuring disruption in song similarity networks","type":"paper"},{"UID":"78","abstract":"Recent work has proposed the use of tensor decomposition to model repetitions and to separate tracks in loop-based electronic music. The present work investigates further on the ability of Nonnegative Tucker Decompositon (NTD) to uncover musical patterns and structure in pop songs in their audio form.Exploiting the fact that NTD tends to express the content of bars as linear combinations of a few patterns, we illustrate the ability of the decomposition to capture and single out repeated motifs in the corresponding compressed space, which can be interpreted from a musical viewpoint. The resulting features also turn out to be efficient for structural segmentation, leading to experimental results on the RWC Pop data set which are potentially challenging state-of-the-art approaches that rely on extensive example-based learning schemes.","affiliations":"Univ Rennes 1||CNRS|IRISA","authors":"Axel Marmoret|Jeremy Cohen|Fr\u221a\u00a9d\u221a\u00a9ric Bimbot|Nancy Bertin","keywords":"MIR fundamentals and methodology|Music signal processing|Musical features and properties|Structure, segmentation, and form","primary_author":"Axel Marmoret","primary_email":"axel.marmoret@inria.fr","session":"1A","title":"Uncovering audio patterns in music with Nonnegative Tucker Decomposition for structural segmentation","type":"paper"},{"UID":"80","abstract":"Grammatical models which represent the hierarchical structure of chord sequences have proven very useful in recent analyses of Jazz harmony. A critical resource for building and evaluating such models is a ground-truth database of syntax trees that encode hierarchical analyses of chord sequences. In this paper, we introduce the Jazz Harmony Treebank (JHT),  a dataset of hierarchical analyses of complete Jazz standards. The analyses were created and checked by experts, based on lead sheets from the open iRealPro collection. The JHT is publicly available in JavaScript Object Notation (JSON), a human-understandable and machine-readable format for structured data. We additionally discuss statistical properties of the corpus and present a simple open-source web application for the graphical creation and editing of trees which was developed during the creation of the dataset.","affiliations":"\u221a\u00e2cole Polytechnique F\u221a\u00a9d\u221a\u00a9rale de Lausanne|EPFL|EPFL|McGill University|Ecole Polytechnique F\u221a\u00a9d\u221a\u00a9rale de Lausanne","authors":"Daniel Harasim|Christoph Finkensiep|Petter Ericson|Timothy J. O'Donnell|Martin Rohrmeier","keywords":"Evaluation, datasets, and reproducibility|Novel datasets and use cases|Domain knowledge|Computational music theory and musicology|Representations of music|Annotation protocols|Musical features and properties|Harmony, chords, and tonality|Structure, segmentation, and form","primary_author":"Daniel Harasim","primary_email":"daniel.harasim@epfl.ch","session":"1A","title":"The Jazz Harmony Treebank","type":"paper"},{"UID":"88","abstract":"This paper addresses the novel task of detecting chorus sections in English and Japanese lyrics text. Although chorus-section detection using audio signals has been studied, whether chorus sections can be detected from text-only lyrics is an open issue. Another open issue is whether patterns of repeating lyric lines such as those appearing in chorus sections depend on language. To investigate these issues, we propose a neural network-based model for sequence labeling. It can learn phrase repetition and linguistic features to detect chorus sections in lyrics text. It is, however, difficult to train this model since there was no dataset of lyrics with chorus-section annotations as there was no prior work on this task. We therefore generate a large amount of training data with such annotations by leveraging pairs of musical audio signals and their corresponding manually time-aligned lyrics; we first automatically detect chorus sections from the audio signals and then use their temporal positions to transfer them to the line-level chorus-section annotations for the lyrics. Experimental results show that the proposed model with the generated data contributes to detecting the chorus sections, that the model trained on Japanese lyrics can detect chorus sections surprisingly well in English lyrics and that patterns of repeating lyric lines are language-independent.","affiliations":"National Institute of Advanced Industrial Science and Technology|National Institute of Advanced Industrial Science and Technology","authors":"Kento Watanabe|Masataka Goto","keywords":"MIR fundamentals and methodology|Lyrics and other textual data, web mining, and natural language |Musical features and properties|Structure, segmentation, and form","primary_author":"Kento Watanabe","primary_email":"kento.watanabe@aist.go.jp","session":"1A","title":"A Chorus-Section Detection Method for Lyrics Text","type":"paper"},{"UID":"89","abstract":"Music arrangement generation is a subtask of automatic music generation, which involves reconstructing and re-conceptualizing a piece with new compositional techniques. Such a generation process inevitably requires reference from the original melody, chord progression, or other structural information. Despite some promising models for arrangement, they lack more refined data to achieve better evaluations and more practical results. In this paper, we propose POP909, a dataset which contains multiple versions of the piano arrangements of 909 popular songs created by professional musicians. The main body of the dataset contains the vocal melody, the lead instrument melody, and the piano accompaniment for each song in MIDI format, which are aligned to the original audio files. Furthermore, we provide the annotations of tempo, beat, key, and chords, where the tempo curves are hand-labeled and others are done by MIR algorithms. Finally, we conduct several baseline experiments with this dataset using standard deep music generation algorithms.","affiliations":"NYU Shanghai|University of California San Diego|Carnegie Mellon University|New York University|University of Florida|Carnegie Mellon University|New York University Shanghai","authors":"Ziyu Wang|Ke Chen|Junyan Jiang|Yiyi Zhang|Maoran Xu|Shuqi Dai|Gus Xia","keywords":"Evaluation, datasets, and reproducibility|Novel datasets and use cases|Applications|Music composition, performance, and production|Domain knowledge|Machine learning/Artificial intelligence for music|MIR tasks|Music transcription and annotation|Musical features and properties|Musical style and genre","primary_author":"Ziyu Wang","primary_email":"zz2417@nyu.edu","session":"1A","title":"POP909: A Pop-Song Dataset for Music Arrangement Generation","type":"paper"},{"UID":"90","abstract":"Chord symbols, typically notating the root note and the chord quality, are extensively used yet oversimplified representation of tonal harmony and chord progressions in popular music. In spite of its convenience, the chord symbol notation only provides basic information about the chordal configuration, and leaves much room for interpretation. With such limitations, an algorithm generating merely chord symbols is usually insufficient for a wide range of music genres such as jazz. To solve this problem, we propose chord jazzification, a process to generate realistic chord configurations in jazz style. With deep learning approaches, we decompose chord jazzification into coloring and voicing. Coloring concerns the choice of color tones, while voicing concerns the configurations of chords. We also create a new dataset featuring interpretations of chord symbols in pop-jazz compositions. By conducting experiments on the new dataset, we show that 1) the two-stage process outperforms an end-to-end generation approach in modeling chord configurations, and 2) attention-based models are better at capturing the structure of chord sequences in comparison with recurrent neural networks.","affiliations":"Academia Sinica|National Institute of Advanced Industrial Science and Technology|National Institute of Advanced Industrial Science and Technology|Academia Sinica","authors":"Tsung-Ping Chen|Satoru Fukayama|Masataka Goto|Li Su","keywords":"Musical features and properties|Harmony, chords, and tonality|Evaluation, datasets, and reproducibility|Novel datasets and use cases|MIR fundamentals and methodology|Symbolic music processing","primary_author":"Tsung-Ping Chen","primary_email":"tearfulcanon@iis.sinica.edu.tw","session":"1A","title":"Chord Jazzification: Learning Jazz Interpretations of Chord Symbols","type":"paper"},{"UID":"94","abstract":"While deep generative models have become the leading methods for algorithmic composition, it remains a challenging problem to control the generation process because the latent variables of most deep-learning models lack good interpretability. Inspired by the content-style disentanglement idea, we design a novel architecture, under the VAE framework, that effectively learns two interpretable latent factors of polyphonic music: chord and texture. The current model focuses on learning 8-beat long piano composition segments. We show that such chord-texture disentanglement provides a controllable generation pathway leading to a wide spectrum of applications, including compositional style transfer, texture variation, and accompaniment arrangement. Both objective and subjective evaluations show that our method achieves a successful disentanglement and high quality controlled music generation.","affiliations":"NYU Shanghai|New York University Shanghai|NYU Shanghai|New York University Shanghai","authors":"Ziyu Wang|Dingsu Wang|Yixiao Zhang|Gus Xia","keywords":"Applications|Music composition, performance, and production|Domain knowledge|Machine learning/Artificial intelligence for music|Representations of music|MIR fundamentals and methodology|Symbolic music processing","primary_author":"Ziyu Wang","primary_email":"zz2417@nyu.edu","session":"1A","title":"Learning Interpretable Representation for Controllable Polyphonic Music Generation","type":"paper"},{"UID":"95","abstract":"We present Connective Fusion, a music generation scheme by transformational joining of two musical sequences for creative purposes. Given two shorter sequences as inputs, our model transforms each of them such that their concatenation is more coherent to form a longer sequence, while each of the transformed shorter sequences retains meaningful similarity with the corresponding input sequence. In short, our model connects and fuses two contextually unrelated sequences in a coherent way. This transformation can be applied iteratively to gradually fuse the input sequences. The style latent space is simultaneously learned, allowing users to control how the two sequences are merged. Our approach comprises two steps of unsupervised learning: a deep generative model with a latent space is learned, followed by adversarial learning of the transformation function in the latent space. We demonstrate the usefulness of our method through the task of melody creation using a symbolic music dataset.","affiliations":"Sony CSL","authors":"Taketo Akama","keywords":"Domain knowledge|Machine learning/Artificial intelligence for music|Applications|Music composition, performance, and production","primary_author":"Taketo Akama","primary_email":"Taketo.Akama@sony.com","session":"1A","title":"Connective Fusion: Learning Transformational Joining of Sequences with Application to Melody Creation","type":"paper"},{"UID":"96","abstract":"The dominant approach for music representation learning involves the deep unsupervised model family variational autoencoder (VAE). However, most, if not all, viable attempts on this problem have largely been limited to monophonic music. Normally composed of richer modality and more complex musical structures, the polyphonic counterpart has yet to be addressed in the context of music representation learning. In this work, we propose the PianoTree VAE, a novel tree-structure extension upon VAE aiming to fit the polyphonic music learning. The experiments prove the validity of the PianoTree VAE via (i)-semantically meaningful latent code for polyphonic segments; (ii)-more satisfiable reconstruction aside of decent geometry learned in the latent space; (iii)-this model\u201a\u00c4\u00f4s benefits to the variety of the downstream music generation.","affiliations":"NYU Shanghai|New York University|NYU Shanghai|NYU Shanghai|University of California Irvine|New York University Shanghai|New York University","authors":"Ziyu Wang|Yiyi Zhang|Yixiao Zhang|Junyan Jiang|Ruihan Yang|Gus Xia|Junbo Zhao","keywords":"Applications|Music composition, performance, and production|Domain knowledge|Machine learning/Artificial intelligence for music|Representations of music|MIR fundamentals and methodology|Symbolic music processing","primary_author":"Ziyu Wang","primary_email":"zz2417@nyu.edu","session":"1A","title":"PIANOTREE VAE: Structured Representation Learning for Polyphonic Music","type":"paper"},{"UID":"98","abstract":"As the field of Music Information Retrieval grows, it is important to take into consideration the multi-modality of music and how aspects of musical engagement such as movement and gesture might be taken into account. Bodily movement is universally associated with music and reflective of important individual features related to music preference such as personality, mood, and empathy. Future multimodal MIR systems may benefit from taking these aspects into account. The current study addresses this by identifying individual differences, specifically Big Five personality traits, and scores on the Empathy and Systemizing Quotients (EQ/SQ) from participants\u201a\u00c4\u00f4 free dance movements. Our model successfully explored the unseen space for personality as well as EQ, SQ, which has not previously been accomplished for the latter. R2 scores for personality, EQ, and SQ were 76.3%, 77.1%, and 86.7% respectively. As a follow-up, we investigated which bodily joints were most important in defining these traits. We discuss how further research may explore how the mapping of these traits to movement patterns can be used to build a more personalized, multi-modal recommendation system, as well as potential therapeutic applications.","affiliations":" IIIT Hyderabad|IIIT Hyderabad|University of Jyv\u221a\u00a7skyl\u221a\u00a7|University of Jyv\u221a\u00a7skyl\u221a\u00a7|IIIT - Hyderabad","authors":"Yudhik Agrawal|Samyak Jain|Emily Carlson|Petri Toiviainen|Vinoo  Alluri","keywords":"Human-centered MIR|User behavior analysis and mining, user modeling|Domain knowledge|Machine learning/Artificial intelligence for music|Human-computer interaction and interfaces|Personalization|MIR fundamentals and methodology|Multimodality","primary_author":"Yudhik Agrawal","primary_email":"yudhik.agrawal@research.iiit.ac.in","session":"1A","title":"Towards Multimodal MIR: Predicting individual differences from music-induced movement","type":"paper"},{"UID":"99","abstract":"Recent machine learning techniques have enabled a large variety of novel music generation processes. However, most approaches do not provide any form of interpretable control over musical attributes, such as pitch and rhythm. Obtaining control over the generation process is critically important for its use in real-life creative setups. Nevertheless, this problem remains arduous, as there are no known functions nor differentiable approximations to transform symbolic music with control of musical attributes.In this work, we propose a novel method that enables attributes-aware music transformation from any set of musical annotations, without requiring complicated derivative implementation. By relying on an adversarial confusion criterion on given musical annotations, we force the latent space of a generative model to abstract from these features. Then, reintroducing these features as conditioning to the generative function, we obtain a continuous control over them. To demonstrate our approach, we rely on sets of musical attributes computed by the jSymbolic library as annotations and conduct experiments that show that our method outperforms previous methods in control. Finally, comparing correlations between attributes and the transformed results show that our method can provide explicit control over any continuous or discrete annotation.","affiliations":"The University of Tokyo|IRCAM|The University of Tokyo / RIKEN","authors":"Lisa Kawai|Philippe  Esling|Tatsuya Harada","keywords":"Musical features and properties|Musical style and genre|Applications|Music composition, performance, and production|MIR fundamentals and methodology|Metadata, tags, linked data, and semantic web|Symbolic music processing|Harmony, chords, and tonality","primary_author":"Lisa Kawai","primary_email":"kawai@mi.t.u-tokyo.ac.jp","session":"1A","title":"Attributes-Aware Deep Music Transformation","type":"paper"},{"UID":"101","abstract":"Lyrics-to-audio alignment methods have recently reported impressive results, opening the door to practical applications such as karaoke and within song navigation. However, most studies focus on a single language - usually English - for which annotated data are abundant. The question of their ability to generalize to other languages, especially in low (or even zero) training resource scenarios has been so far left unexplored. In this paper, we address the lyrics-to-audio alignment task in a generalized multilingual setup. More precisely, this investigation presents the first (to the best of our knowledge) attempt to create a language-independent lyrics-to-audio alignment system. Building on a RNN model trained with a CTC algorithm, we study the relevance of different intermediate representations, either character or phoneme, along with several strategies to design a training set. The evaluation is conducted on multiple languages with a varying amount of data available, from plenty to zero. Results show that learning from diverse data and using a universal phoneme set as an intermediate representation yield the best generalization performances.","affiliations":"Deezer / Telecom ParisTech|DEEZER|DEEZER|Telecom Paristech|T\u221a\u00a9l\u221a\u00a9com Paris","authors":"Andrea Vaglio|Romain Hennequin|Manuel Moussallam|Gael Richard|Florence d\u201a\u00c4\u00f4Alch\u221a\u00a9-Buc","keywords":"MIR tasks|Alignment, synchronization, and score following","primary_author":"Andrea Vaglio","primary_email":"avaglio@deezer.com","session":"1A","title":"Multilingual lyrics-to-audio alignment","type":"paper"},{"UID":"105","abstract":"Many sounds that humans encounter are hierarchical in nature; a piano note is one of many played during a performance, which is one of many instruments in a band, which might be playing in a bar with other noises occurring. Inspired by this, we re-frame the musical source separation problem as hierarchical, combining similar instruments together at certain levels and separating them at other levels. This allows us to deconstruct the same mixture in multiple ways, depending on the appropriate level of the hierarchy for a given application. In this paper, we present various methods for hierarchical musical instrument separation, with some methods focusing on separating specific instruments (like guitars) and other methods that determine what to separate based on a user-supplied audio example. We additionally show that separating all hierarchy levels is possible even when training data is limited at fine-grained levels of the hierarchy.","affiliations":"Northwestern University|Mitsubishi Electric Research Laboratories|Mitsubishi Electric Research Laboratories","authors":"Ethan Manilow|Gordon Wichern|Jonathan LeRoux","keywords":"MIR tasks|Sound source separation","primary_author":"Ethan Manilow","primary_email":"ethanmanilow@gmail.com","session":"1A","title":"Hierarchical Musical Instrument Separation","type":"paper"},{"UID":"110","abstract":"A Dhrupad vocal concert comprises a composition section that is interspersed with improvised episodes of increased rhythmic activity involving the interaction between the vocals and the percussion. Tracking the changing rhythmic density, in relation to the underlying metric tempo of the piece, thus facilitates the detection and labeling of the improvised sections in the concert structure. This work concerns the automatic detection of the musically relevant rhythmic densities as they change in time across the bandish (composition) performance. An annotated dataset of Dhrupad bandish concert sections is presented. We implement a CNN-based system, trained to detect local tempo relationships, and follow it with temporal smoothing. We also employ audio source separation as a pre-processing step to the detection of the individual surface densities of the vocals and the percussion. This helps us obtain the complete musical description of the concert sections in terms of capturing the changing rhythmic interaction of the two performers.","affiliations":"Indian Institute of Technology Bombay|Indian Institute of Technology Bombay|IIT Bombay","authors":"Rohit M A|Vinutha T P|Preeti Rao","keywords":"Musical features and properties|Structure, segmentation, and form|Domain knowledge|Machine learning/Artificial intelligence for music|Evaluation, datasets, and reproducibility|Novel datasets and use cases|MIR fundamentals and methodology|Music signal processing|Rhythm, beat, tempo","primary_author":"Rohit M A","primary_email":"rohitma@ee.iitb.ac.in","session":"1A","title":"Structural Segmentation of Dhrupad Vocal Bandish Audio based on Tempo","type":"paper"},{"UID":"113","abstract":"\"Maracatu de baque solto\" is a Carnival performance combining music, poetry, and dance, occurring in the Zona da Mata Norte region of Pernambuco (Northeast Brazil). Maracatu percussive music is strongly repetitive, and is played as loud and as fast as possible. Both from an MIR and ethnomusicological perspective this makes a complex musical scene to analyse and interpret. In this paper we focus on the extraction of microtiming profiles towards the longer term goal of understanding how rhythmic performance in Maracatu is used to promote health and well-being. To conduct this analysis we use a set of recordings acquired with contact microphones which minimise the interference between performers. Our analysis reveals that the microtiming profiles differ substantially from those observed in more widely studied South American music. In particular, we highlight the presence of dynamic microtiming profiles as well as the importance of the choice of time-keeper instrument, which dictates how the performances can be understood. Throughout this work, we emphasize the importance of a multidisciplinary approach in which MIR, audio engineering, and ethnomusicology must interact to provide meaningful insight about this music. ","affiliations":"CISUC - Centre for Informatics and Systems of the University of Coimbra|New York University|FEUP|FEUP / INESC TEC|FEUP |Universidade Nova de Lisboa","authors":"Matthew Davies|Magdalena Fuentes|Jo\u221a\u00a3o Fonseca|Luis Aly|Marco Jer\u221a\u2265nimo|Filippo Bonini Baraldi","keywords":"Musical features and properties|Expression and performative aspects of music|Applications|Music and health, well-being, and therapy|Rhythm, beat, tempo","primary_author":"Matthew Davies","primary_email":"mepdavies@dei.uc.pt","session":"1A","title":"Moving in Time: Computational Analysis of Microtiming in Maracatu de Baque Solto","type":"paper"},{"UID":"118","abstract":"Annotating music items with music genres is crucial for music recommendation and information retrieval, yet challenging given that music genres are subjective concepts. Recently, in order to explicitly consider this subjectivity, the annotation of music items was modeled as a translation task: predict for a music item its music genres within a target vocabulary or taxonomy (tag system) from a set of music genre tags originating from other tag systems. However, without a parallel corpus, previous solutions could not handle tag systems in other languages, being limited to the English-language only. Here, by learning multilingual music genre embeddings, we enable cross-lingual music genre translation without relying on a parallel corpus. First, we apply compositionality functions on pre-trained word embeddings to represent multi-word tags. Second, we adapt the tag representations to the music domain by leveraging multilingual music genres graphs with a modified retrofitting algorithm. Experiments show that our method: 1) is effective in translating music genres across tag systems in multiple languages (English, French and Spanish); 2) outperforms the previous baseline in an English-language multi-source translation task.","affiliations":"Deezer Research|\u221a\u00e2cole polytechnique|DEEZER","authors":"Elena V. Epure|Guillaume Salha|Romain Hennequin","keywords":"MIR fundamentals and methodology|Metadata, tags, linked data, and semantic web|Domain knowledge|Computational music theory and musicology|Representations of music|Lyrics and other textual data, web mining, and natural language processing|Musical features and properties|Musical style and genre","primary_author":"Elena V. Epure","primary_email":"eepure@deezer.com","session":"1A","title":"Multilingual Music Genre Embeddings for Effective Cross-Lingual Music Item Annotation","type":"paper"},{"UID":"123","abstract":"Musical form and syntax in Western classical music are hierarchically organised on different timescales. One of the most important features of this structure is the organisation of modulations between different keys throughout a piece. Music theoretical research has established taxonomies of prototypical modulation plans for different modes and musical forms. However, these prototypes still require empirical validation based on quantitative statistical methods and cannot be retrieved automatically so far.In this paper, we present a novel method to infer prototypical modulation plans from musical corpora. A modulation plan is formalised as a transposition-invariant probabilistic model over the underlying pitch class distributions based on a hierarchical pitch scape representation. Prototypical modulation plans can be learned in an unsupervised manner by training a mixture model (similar to a Gaussian mixture model) on the data, so that different prototypes appear as distinct clusters.We evaluate our approach by performing hierarchical clustering on a corpus of more than 150 Baroque pieces, with the extracted clusters showing excellent agreement with the most common prototypes postulated in music theory. Our method bears a great potential for modelling, analysis and discovery of hierarchical key structure and prototypes in corpora across a broad range of musical styles. An accompanying library is available at: github.com/robert-lieck/pitchscapes.","affiliations":"EPFL|Ecole Polytechnique F\u221a\u00a9d\u221a\u00a9rale de Lausanne","authors":"Robert Lieck|Martin Rohrmeier","keywords":"Domain knowledge|Machine learning/Artificial intelligence for music|Computational music theory and musicology|Representations of music|MIR tasks|Automatic classification|Musical features and properties|Harmony, chords, and tonality|Structure, segmentation, and form","primary_author":"Robert Lieck","primary_email":"robert.lieck@epfl.ch","session":"1A","title":"Modelling Hierarchical Key Structure With Pitch Scapes","type":"paper"},{"UID":"124","abstract":"Musical schemata constitute important structural building blocks used across historical styles and periods.They consist of two or more melodic lines that are combined to form specific successions of intervals. This paper tackles the problem of recognizing voice-leading schemata in polyphonic music.Since schema types and subtypes can be realized in a wide variety of ways on the musical surface,finding schemata in an automated fashion is a challenging task.To perform schema inference we employ a skipgram model that computes schema candidates, which are then classified using a binary classifier on musical features related to pitch and rhythm.This model is evaluated on a novel dataset of schema annotations in Mozart's piano sonatas produced byexpert annotators, which is published alongside this paper.The features are chosen to encode music-theoretically predicted properties of schema instances.We assess the relevance of each feature for the classification task, thus contributing to the theoretical understanding of complex musical objects.","affiliations":"EPFL|EPFL|Anton Bruckner Privatuniversit\u221a\u00a7t Linz|Ecole Polytechnique F\u221a\u00a9d\u221a\u00a9rale de Lausanne","authors":"Christoph Finkensiep|Ken D\u221a\u00a9guernel|Markus Neuwirth|Martin Rohrmeier","keywords":"MIR tasks|Pattern matching and detection|Domain knowledge|Computational music theory and musicology|MIR fundamentals and methodology|Symbolic music processing|Musical features and properties|Harmony, chords, and tonality|Melody and motives|Rhythm, beat, tempo","primary_author":"Christoph Finkensiep","primary_email":"christoph.finkensiep@epfl.ch","session":"1A","title":"Voice-Leading Schema Recognition Using Rhythm and Pitch Features","type":"paper"},{"UID":"125","abstract":"Audio embeddings of musical similarity are often used for music recommendations and autoplay discovery. These embeddings are typically learned using co-listen data to train a deep neural network, to provide consistent tripletloss distances. Instead of directly using these co-listen\u201a\u00c4\u00ecbased embeddings, we explore making recommendations based on a second, smaller embedding space of human-intelligible musical attributes. To do this, we use the co-listen\u201a\u00c4\u00ecbased audio embeddings as inputs to small attribute classifiers, trained on a small hand-labeled dataset. These classifiers map from the original embedding space to a new interpretable attribute coordinate system that provides a more useful distance measure for downstream applications. The attributes and attribute embeddings allow us to provide a search interface and more intelligible recommendations for music curators. We examine the relative performance of these two embedding spaces (the co-listen\u201a\u00c4\u00ecaudio embedding and the attribute embedding) for the mathematical separation of thematic playlists. We also report on the usefulness of recommendations from the attribute-embedding space to human curators for automatically extending thematic playlists.","affiliations":"YouTube Music|YouTube Music|YouTube Music|YouTube Music|Google","authors":"Ayush Patwari|Nicholas Kong|Jun Wang|Ullas Gargi|Michele Covell","keywords":"Applications|Music recommendation and playlist generation|MIR tasks|Automatic classification|Musical features and properties|Musical affect, emotion, and mood","primary_author":"Ullas Gargi","primary_email":"ullas@google.com","session":"1A","title":"Semantically Meaningful Attributes from Co-Listen Embeddings for Playlist Exploration and Expansion","type":"paper"},{"UID":"126","abstract":"The human ability to track musical downbeats is robust to changes in tempo, and it extends to tempi never previously encountered.  We propose a deterministic time-warping operation that enables this skill in a convolutional neural network (CNN) by allowing the network to learn rhythmic patterns independently of tempo.  Unlike conventional deep learning approaches, which learn rhythmic patterns at the tempi present in the training dataset, the patterns learned in our model are tempo-invariant, leading to better tempo generalisation and more efficient usage of the network capacity.We test the generalisation property on a synthetic dataset created by rendering the Groove MIDI Dataset using FluidSynth, split into a training set containing the original performances and a test set containing tempo-scaled versions rendered with different SoundFonts (test-time augmentation).The proposed model generalises nearly perfectly to unseen tempi (F-measure of 0.89 on both training and test sets), whereas a comparable conventional CNN achieves similar accuracy only for the training set (0.89) and drops to 0.54 on the test set.The generalisation advantage of the proposed model extends to real music, as shown by results on the GTZAN and Ballroom datasets.","affiliations":"Apple|Apple|Apple","authors":"Bruno Di Giorgi|Matthias Mauch|Mark Levy","keywords":"Domain knowledge|Machine learning/Artificial intelligence for music|MIR tasks|Music transcription and annotation|Musical features and properties|Rhythm, beat, tempo","primary_author":"Bruno Di Giorgi","primary_email":"bdigiorgi@apple.com","session":"1A","title":"Downbeat Tracking with Tempo Invariant Convolutional Neural Networks","type":"paper"},{"UID":"127","abstract":"In this paper we present Aligned Scores and Performances (ASAP): a new dataset of 222 digital musical scores aligned with 1068 performances (more than 92 hours) of Western classical piano music.The scores are provided as paired MusicXML files and quantized MIDI files, and the performances as paired MIDI files and partially as audio recordings.Scores and performances are aligned with downbeat, beat, time signature, and key signature annotations. ASAP has been obtained thanks to a new annotation workflow that combines score analysis andalignment algorithms, with the goal of reducing the time for manual annotation. The dataset itself is, to our knowledge, the largest that includes an alignment of music scores to MIDI and audio performance data. As such, it is a useful resource for a wide variety of MIR applications, from those that target the complete audio-to-score Automatic Music Transcription task, to others that target more specific aspects (e.g., key signature estimation and beat or downbeat tracking from both MIDI and audio representations).","affiliations":"CNAM|EPFL|CNAM|Inria, CNAM-C\u221a\u00a9dric lab, Paris|Nagoya Univ.","authors":"Francesco Foscarin|Andrew McLeod|Philippe Rigaux|Florent Jacquemard|Masahiko Sakai","keywords":"Evaluation, datasets, and reproducibility|Novel datasets and use cases|Annotation protocols|MIR tasks|Music transcription and annotation","primary_author":"Francesco Foscarin","primary_email":"foscarin.francesco@gmail.com","session":"1A","title":"ASAP: a dataset of aligned scores and performances for piano transcription","type":"paper"},{"UID":"128","abstract":"Musical preferences have been considered a mirror of the self. In this age of Big Data, online music streaming services allow us to capture ecologically valid music listening behavior and provide a rich source of information to identify several user-specific aspects. Studies have shown musical engagement to be an indirect representation of internal states including internalized symptomatology and depression. The current study aims at unearthing patterns and trends in the individuals at risk for depression as it manifests in naturally occurring music listening behavior. Mental-well being scores, musical engagement measures, and listening histories of Last.fm users (N=541) were acquired. Social tags associated with each listener's most popular tracks were analyzed to unearth the mood/emotions and genres associated with the users. Results revealed that social tags prevalent in the users at risk for depression were predominantly related to emotions depicting Sadness associated with genre tags representing neo-psychedelic-, avant garde-, dream-pop. This study will open up avenues for an MIR-based approach to characterizing and predicting risk for depression which can be helpful in early detection and additionally provide bases for designing music recommendations accordingly.","affiliations":"International Institute of Information Technology, Hyderabad|International Institute of Information Technology, Hyderabad|IIIT Hyderabad|University of Jyv\u221a\u00a7skyl\u221a\u00a7|IIIT - Hyderabad","authors":"Aayush Surana|Yash Goyal|Manish Shrivastava|Suvi H Saarikallio|Vinoo  Alluri","keywords":"Applications|Music and health, well-being, and therapy|Domain knowledge|Cognitive MIR|Human-centered MIR|User behavior analysis and mining, user modeling|MIR fundamentals and methodology|Metadata, tags, linked data, and semantic web|Musical features and properties|Musical affect, emotion, and mood|Musical style and genre","primary_author":"Aayush Surana","primary_email":"aayush.surana@research.iiit.ac.in","session":"1A","title":"Tag2Risk: Harnessing Social Music Tags for Characterizing Depression Risk","type":"paper"},{"UID":"131","abstract":"Informed source separation has recently gained renewed interest with the introduction of neural networks and the availability of large multitrack datasets containing both the mixture and the separated sources.These approaches use prior information about the target source to improve separation.Historically, Music Information Retrieval ({MIR}) researchers have focused primarily on score-informed source separation, but more recent approaches explore lyrics-informed source separation.However, because of the lack of multitrack datasets with time-aligned lyrics, models use weak conditioning with the non-aligned lyrics.In this paper, we present a multimodal multitrack dataset with lyrics aligned in time at the word level with phonetic information as well as explore strong conditioning using the aligned phonemes.Our model follows a {U-Net} architecture and takes as input both the magnitude spectrogram of a musical mixture and a matrix with aligned phoneme information.The phoneme matrix is embedded to obtain the parameters that control Feature-wise Linear Modulation ({FiLM}) layers.These layers condition the {U-Net} feature maps to adapt the separation process to the presence of different phonemes via affine transformations.We show that phoneme conditioning can be successfully applied to improve singing voice source separation.","affiliations":"IRCAM|LTCI - T\u221a\u00a9l\u221a\u00a9com Paris, IP Paris","authors":"Gabriel Meseguer Brocal|Geoffroy Peeters","keywords":"MIR tasks|Sound source separation|Evaluation, datasets, and reproducibility|Novel datasets and use cases|MIR fundamentals and methodology|Lyrics and other textual data, web mining, and natural language processing|Multimodality","primary_author":"Gabriel Meseguer Brocal","primary_email":"gabriel.meseguer.brocal@gmail.com","session":"1A","title":"Content based singing voice source separation via strong conditioning using aligned phonemes","type":"paper"},{"UID":"132","abstract":"A major bottleneck in the evaluation of music generation is that music appreciation is a highly subjective matter. When considering an average appreciation as an evaluation metric, user studies can be helpful. The challenge of generating personalized content, however, has been examined only rarely in the literature. In this paper, we address generation of personalized music and propose a novel pipeline for music generation that learns and optimizes user-specific musical taste. We focus on the task of symbol-based, monophonic, harmony-constrained jazz improvisations. Our personalization pipeline begins with BebopNet, a music language model trained on a corpus of jazz improvisations by Bebop giants. BebopNet is able to generate improvisations based on any given chord progression. We then assemble a personalized dataset, labeled by a specific user, and train a user-specific metric that reflects this user's unique musical taste. Finally, we employ a personalized variant of beam-search with BebopNet to optimize the generated jazz improvisations for that user. We present an extensive empirical study in which we apply this pipeline to extract individual models as implicitly defined by several human listeners. Our approach enables an objective examination of subjective personalized models whose performance is quantifiable. The results indicate that it is possible to model and optimize personal jazz preferences and offer a foundation for future research in personalized generation of art. We also briefly discuss opportunities, challenges, and questions that arise from our work, including issues related to creativity.","affiliations":"Technion|Technion|Technion","authors":"Shunit Haviv Hakimi|Nadav Bhonker|Ran El-Yaniv","keywords":"Domain knowledge|Machine learning/Artificial intelligence for music|Applications|Music composition, performance, and production|Human-centered MIR|Personalization","primary_author":"Shunit Haviv Hakimi","primary_email":"shunithaviv@gmail.com","session":"1A","title":"BebopNet: Deep Neural Models for Personalized Jazz Improvisations","type":"paper"},{"UID":"133","abstract":"More and more music is becoming available digitally, increasing the need to navigate through large numbers of audio tracks easily. One approach for improving the browsing experience is music thumbnailing: the procedure of finding a continuous fragment that can represent the whole musical piece. This paper proposes a human-centred approach to creating thumbnails based on listeners' perception, directly asking listeners to identify the most characteristic fragment. We carried out a user study to assign representativeness scores to multiple fragments from a selection of popular music tracks. To strengthen the results, we performed a replication of the same user study with new participants and a different set of music. Thereafter, we used audio features, the segmentation algorithm, and participants' overall familiarity with the songs to predict representativeness scores. The results suggest that neither segmentation nor familiarity have a significant impact on users' thumbnail preferences: even segments with starting points that pay no regard to song structure can be suitable thumbnails. Three high-level audio characteristics, however, do impact the perceived representativeness of a fragment: Raw Intensity, Melodic Conventionality, and Conventionally of Intensity. Based on these findings, we propose a new, easy-to-apply method for music thumbnailing.","affiliations":"None|University of Amsterdam|Utrecht University|Utrecht University","authors":"Arianne N. van Nieuwenhuijsen|John Ashley Burgoyne|Frans Wiering|Mick Sneekes","keywords":"MIR tasks|Music summarization|Applications|Music retrieval systems|Human-centered MIR|User behavior analysis and mining, user modeling","primary_author":"Arianne N. van Nieuwenhuijsen","primary_email":"anvannieuwenhuijsen@gmail.com","session":"1A","title":"A Simple Method for User-Driven Music Thumbnailing","type":"paper"},{"UID":"134","abstract":"In May 2015, a consultant for country radio revealed a decades\u201a\u00c4\u00f4 long practice of limiting space for songs by female artists. He encouraged program directors to avoid playing songs by women back-to-back and advocated for programming their songs at 13-15% of station playlists. His words sparked debate within the industry and drew attention to growing inequalities on radio and within the genre. The majority of these discussions have centered on US country radio, with limited attention to the growing imbalance on the format in Canada. While country format radio in both countries subscribe to a practice of gender-based programming, Canadian program directors are governed by the federal Broadcasting Act, which regulates dissemination of Canadian content. Using metadata extracted from one of the main radio monitoring services \u201a\u00c4\u00ec Mediabase, this paper examines gender-related trends on Canadian country format radio between 2005 and 2019. Through data-driven analysis of Mediabase\u201a\u00c4\u00f4s weekly re-ports, this paper shows declining representation of songs by women on Canadian country radio and addresses the impact of Canadian content regulations on this process. ","affiliations":"University of Ottawa","authors":"Jada E Watson","keywords":"Philosophical and ethical discussions|Legal and societal aspects of MIR|Applications|Music recommendation and playlist generation|Music retrieval systems|Evaluation, datasets, and reproducibility|Evaluation methodology|MIR fundamentals and methodology|Metadata, tags, linked data, and semantic web","primary_author":"Jada E Watson","primary_email":"jwatso4@uottawa.ca","session":"1A","title":"PROGRAMMING INEQUALITY: GENDER REPRESENTATION ON CANADIAN COUNTRY RADIO (2005-2019)","type":"paper"},{"UID":"135","abstract":"Choral music recordings are a particularly challenging target for source separation due to the choral blend and the inherent acoustical complexity of the \u201a\u00c4\u00f2choral timbre\u201a\u00c4\u00f4. Due to the scarcity of publicly available multi-track choir recordings, we create a dataset of synthesized Bach chorales. We apply data augmentation to alter the chorales so that they more faithfully represent music from a broader range of choral genres. For separation we employ Wave-U-Net, a time-domain convolutional neural network (CNN) originally proposed for vocals and accompaniment separation. We show that Wave-U-Net outperforms a baseline implemented using score-informed NMF (non-negative matrix factorization). We introduce score-informed Wave-U-Net to incorporate the musical score into the separation process. We experiment with different score conditioning methods and show that conditioning on the score leads to improved separation results. We propose a \u201a\u00c4\u00f2score-guided\u201a\u00c4\u00f4 model variant in which separation is guided by the score alone, bypassing the need to specify the identity of the extracted source. Finally, we evaluate our models (trained on synthetic data only) on real choir recordings and find that in the absence of a large training set of real recordings, NMF still performs better than Wave-U-Net in this setting. To our knowledge, this paper is the first to study source separation of choral music.","affiliations":"McGill University|McGill University","authors":"Matan Gover|Philippe Depalle","keywords":"MIR tasks|Sound source separation|Domain knowledge|Machine learning/Artificial intelligence for music|MIR fundamentals and methodology|Music signal processing","primary_author":"Matan Gover","primary_email":"matangover@gmail.com","session":"1A","title":"Score-Informed Source Separation of Choral Music","type":"paper"},{"UID":"137","abstract":"Copyright restrictions prevent the widespread sharing of commercial music audio. Therefore, the availability of resharable pre-computed music audio features has become critical. In line with this, the AcousticBrainz platform offers a dynamically growing, open and community-contributed large-scale resource of locally computed low-level and high-level music descriptors. Beyond enabling research reuse, the availability of such an open resource allows for renewed reflection on the music descriptors we have at hand: while they were validated to perform successfully under lab conditions, they now are being run \u201a\u00c4\u00f2in the wild\u201a\u00c4\u00f4. Their response to these more ecological conditions can shed light on the degree to which they truly had construct validity. In this work, we seek to gain further understanding into this, by analyzing high-level classifier-based music descriptor output in AcousticBrainz. While no hard ground truth is available on what the true value of these descriptors should be, some oracle information can still be derived, relying on semantic redundancies between several descriptors, and multiple feature submissions being available for the same recording. We report on multiple unexpected patterns found in the data, indicating that the descriptor values should not be taken as absolute truth, and hinting at directions for more comprehensive descriptor testing that are overlooked in common machine learning evaluation and quality assurance setups.","affiliations":"Delft University of Technology|Delft University of Technology","authors":"Cynthia C. S. Liem|Chris Mostert","keywords":"Evaluation, datasets, and reproducibility|Reproducibility|Domain knowledge|Machine learning/Artificial intelligence for music|Evaluation methodology|Musical features and properties|Musical affect, emotion, and mood|Musical style and genre|Philosophical and ethical discussions|Philosophical and methodological foundations","primary_author":"Cynthia C. S. Liem","primary_email":"c.c.s.liem@tudelft.nl","session":"1A","title":"Can't trust the feeling? How open data reveals unexpected behavior of high-level music descriptors","type":"paper"},{"UID":"139","abstract":"This paper studies the problem of automatically generating Youtube piano score following videos given an audio recording and raw sheet music images.  Whereas previous works focus on synthetic sheet music where the data has been cleaned and preprocessed, we instead focus on developing a system that can cope with the messiness of raw, unprocessed sheet music PDFs from IMSLP.  We investigate how well existing systems cope with real scanned sheet music, filler pages and unrelated pieces or movements, and discontinuities due to jumps and repeats.  We find that a significant bottleneck in system performance is handling jumps and repeats correctly.  In particular, we find that a previously proposed Jump DTW algorithm does not perform robustly when jump locations are unknown a priori.  We propose a novel alignment algorithm called Hierarchical DTW that can handle jumps and repeats even when jump locations are not known.  It first performs alignment at the feature level on each sheet music line, and then performs a second alignment at the segment level.  By operating at the segment level, it is able to encode domain knowledge about how likely a particular jump is.  Through carefully controlled experiments on unprocessed sheet music PDFs from IMSLP, we show that Hierarachical DTW significantly outperforms Jump DTW in handling various types of jumps.","affiliations":"Harvey Mudd College|Harvey Mudd College","authors":"Mengyi Shan|Timothy Tsai","keywords":"MIR tasks|Alignment, synchronization, and score following|MIR fundamentals and methodology|Multimodality|Music signal processing","primary_author":"Timothy Tsai","primary_email":"ttsai@g.hmc.edu","session":"1A","title":"Improved Handling of Repeats and Jumps in Audio--Sheet Image Synchronization","type":"paper"},{"UID":"142","abstract":"In this paper, we propose the use of speaker embedding networks to perform zero-shot singing voice conversion, and suggest two architectures for its realization.  The use of speaker embedding networks not only enables the capability to adapt to new voices on-the-fly, but also allows for model training on unlabeled data.  This not only facilitates the collection of suitable singing voice data, but also allows networks to be pretrained on large speech corpora before being refined on singing voice datasets, improving network generalization.  We illustrate the effectiveness of the proposed zero-shot singing voice conversion algorithms by both qualitative and quantitative means.","affiliations":"iZotope, Inc.","authors":"Shahan Nercessian","keywords":"MIR tasks|Music synthesis and transformation|Domain knowledge|Machine learning/Artificial intelligence for music|Musical features and properties|Timbre, instrumentation, and voice","primary_author":"Shahan Nercessian","primary_email":"shahan@izotope.com","session":"1A","title":"Zero-Shot Singing Voice Conversion","type":"paper"},{"UID":"144","abstract":"We propose and explore the novel task of dance beat tracking, which can be regarded as a fundamental topic in the Dance Information Retrieval (DIR) research field. Dance beat tracking aims at detecting musical beats from a dance video by using its visual information without using its audio information (i.e., dance music). The visual analysis of dances is important to achieve general machine understanding of dances, not limited to dance music. As a sub-area of Music Information Retrieval (MIR) research, DIR also shares similar goals with MIR and needs to extract various high-level semantics from dance videos. While audio-based beat tracking has been thoroughly studied in MIR, there has not been visual-based beat tracking for dance videos.We approach dance beat tracking as a time series classification problem and conduct several experiments using a Temporal Convolutional Neural Network (TCN) using the AIST Dance Video Database. We evaluate the proposed solution considering different data splits based on either \"dancer\" or \"music\". Moreover, we propose a periodicity-based loss that considerably improves the overall beat tracking performance according to several evaluation metrics.","affiliations":"National Institute of Advanced Industrial Science and Technology|National Institute of Advanced Industrial Science and Technology","authors":"Fabrizio Pedersoli|Masataka Goto","keywords":"Musical features and properties|Rhythm, beat, tempo|MIR fundamentals and methodology|Multimodality","primary_author":"Fabrizio Pedersoli","primary_email":"fabrizio.pedersoli@aist.go.jp","session":"1A","title":"Dance Beat Tracking from Visual Information Alone","type":"paper"},{"UID":"146","abstract":"Drawing an analogy with automatic image completion systems, we propose Music SketchNet, a neural network framework that allows users to specify partial musical ideas guiding automatic music generation. We focus on generating the missing measures in incomplete monophonic musical pieces, conditioned on surrounding context, and optionally guided by user-specified pitch and rhythm snippets. First, we introduce SketchVAE, a novel variational autoencoder that explicitly factorizes rhythm and pitch contour to form the basis of our proposed model. Then we introduce two discriminative architectures, SketchInpainter and SketchConnector, that in conjunction perform the guided music completion, filling in representations for the missing measures conditioned on surrounding context and user-specified snippets. We evaluate SketchNet on a standard dataset of Irish folk music and compare with models from recent works. When used for music completion, our approach outperforms the state-of-the-art both in terms of objective metrics and subjective listening tests. Finally, we demonstrate that our model can successfully incorporate user-specified snippets during the generation process. ","affiliations":"University of California San Diego|Smule, Inc|University of California San Diego|UC San Diego","authors":"Ke Chen|Cheng-i Wang|Taylor Berg-Kirkpatrick|Shlomo Dubnov","keywords":"Domain knowledge|Machine learning/Artificial intelligence for music|Applications|Music composition, performance, and production|Representations of music|Human-centered MIR|Human-computer interaction and interfaces|Musical features and properties|Melody and motives|Rhythm, beat, tempo","primary_author":"Ke Chen","primary_email":"knutchen@ucsd.edu","session":"1A","title":"Music SketchNet: Controllable Music Generation via Factorized Representations of Pitch and Rhythm","type":"paper"},{"UID":"147","abstract":"Much of the existing research on user aspects in the music information retrieval field tends to focus on general user needs or behavior related to music information seeking, music listening and sharing, or other use of commercial music services. However, we have a limited understanding of the personal and social contexts of music fans who enthusiastically support musicians and are often avid users of commercial music services. In this study, we aim to better understand the contextual complexities surrounding music fans through a case study of the group BTS and its fan community, ARMY. In particular, we are interested in discovering factors that influence the interactions of music fans with music services, especially in the current environment where the prevalence of social media and other tools/technologies influences musical enjoyment. Through virtual ethnography and content analysis, we identified four factors that affect music fans\u201a\u00c4\u00f4 interactions with commercial music services: 1) perception of music genres, 2) participatory fandom, 3) desire for agency and transparency, and 4) importance of non-musical factors. The discussion of each aspect is followed by design implications for commercial music services to consider.","affiliations":"University of Washington|University of Washington","authors":"Jin Ha Lee|Anh Thu Nguyen","keywords":"Human-centered MIR|Human-computer interaction and interfaces","primary_author":"Jin Ha Lee","primary_email":"jinhalee@uw.edu","session":"1A","title":"How Music Fans Shape Commercial Music Services:  A Case Study of BTS and ARMY","type":"paper"},{"UID":"148","abstract":"This study examines gender representation in current music streaming, utilizing one of the world\u201a\u00c4\u00f4s largest streaming services. First, we found listeners generally stream fewer female or mixed-gender creator groups than male artists, with differences per genre. Second, while still relatively low, we found that recommendation-based streaming has a slightly higher proportion of female creators than \u201a\u00c4\u00faorganic\u201a\u00c4\u00f9 listening (i.e., tracks that are not recommended by editors or algorithms). Third, we examined streaming data from 200,000 US users to determine the proportion of female artists in organic and recommended streams over a 28-day period and the relationship between recommended streams and users\u201a\u00c4\u00f4 future organic listening. The proportion of female artists in recommended streaming appears predictive of the proportion of female artists in organic streaming; these effects are moderated by gender and age. Fourth, this study also samples creators across different popularity levels, seeing more female and multi-gender groups at lower levels than in the middle tiers. However, solo female artists are better represented again in the superstars category, suggesting influence of selected superstars and genres. We conclude by discussing potential avenues in algorithmic auditing.","affiliations":"Harvard University|Spotify|Spotify","authors":"Avriel C Epps-Darling|Henriette Cramer|Romain Takeo Bouyer","keywords":"Philosophical and ethical discussions|Ethical issues related to designing and implementing MIR to|Applications|Music recommendation and playlist generation|Human-centered MIR|Human-computer interaction and interfaces|User behavior analysis and mining, user modeling|MIR fundamentals and methodology|Metadata, tags, linked data, and semantic web","primary_author":"Avriel C Epps-Darling","primary_email":"avrielepps@g.harvard.edu","session":"1A","title":"FEMALE ARTIST REPRESENTATION IN MUSIC STREAMING","type":"paper"},{"UID":"150","abstract":"The mood of a song is a highly relevant feature for exploration and recommendation in large collections of music. These collections tend to require automatic methods for predicting such moods. In this work, we show that listening-based features outperform content-based ones when classifying moods: embeddings obtained through matrix factorization of listening data appear to be more informative of a track mood than embeddings based on its audio content. To demonstrate this, we compile a subset of the Million Song Dataset, totalling 67k tracks, with expert annotations of 188 different moods collected from AllMusic. Our results on this novel dataset not only expose the limitations of current audio-based models, but also aim to foster further reproducible research on this timely topic","affiliations":"Pandora Media, Inc.|Pandora|Pandora|Universitat Pompeu Fabra|Pandora Media Inc.|Netflix, Inc.","authors":"Filip Korzeniowski|Oriol Nieto|Matthew McCallum|Minz Won|Sergio Oramas|Erik Schmidt","keywords":"Musical features and properties|Musical affect, emotion, and mood|Applications|Music retrieval systems|MIR tasks|Automatic classification","primary_author":"Oriol Nieto","primary_email":"oriol.nieto@gmail.com","session":"1A","title":"Mood Classification Using Listening Data","type":"paper"},{"UID":"155","abstract":"In this paper, we propose a method of utilizing aligned lyrics as additional information to improve the performance of singing voice separation. We have combined the highway network-based lyrics encoder into Open-unmix separation network and show that the model trained with the aligned lyrics indeed results in a better performance than the model that was not informed. The question now remains whether the increase of performance is actually due to the phonetic contents that lie in the informed aligned lyrics or not. To this end, we investigated the source of performance increase in multifaceted ways by observing the change of performance when incorrect lyrics were given to the model. Experiment results show that the model can use not only just vocal activity information but also the phonetic contents from the aligned lyrics.","affiliations":"Seoul National University|Seoul National University|Seoul National University","authors":"Chang-Bin Jeon|Hyeong-Seok Choi|Kyogu Lee","keywords":"MIR tasks|Sound source separation|Domain knowledge|Machine learning/Artificial intelligence for music|MIR fundamentals and methodology|Lyrics and other textual data, web mining, and natural language processing","primary_author":"Chang-Bin Jeon","primary_email":"vinyne@snu.ac.kr","session":"1A","title":"EXPLORING ALIGNED LYRICS-INFORMED SINGING VOICE SEPARATION","type":"paper"},{"UID":"156","abstract":"The bambuco, one of the national rhythms of Colombia, is characterized by the presence of sesquialteras or the superposition of rhythmic elements from two meters. In this work, we analyze sesquialteras in bambucos from two perspectives. First, we analyze the perception of beat and meter by asking 10 Colombian musicians to perform beat annotations in a dataset of bambucos. Results show  great diversity in the annotations: a total of five different meters or meter combinations were found in the annotations, with each bambuco in the study being annotated in at least two different meters. Second, we perform a beat tracking analysis in a dataset of bambucos with two state-of-the-art algorithms.  Given that the algorithms used in the analysis were designed to deal with the rhythmic regularity of a single meter, it is not surprising that tracking performance is not very high (~42% mean F-measure). However, a deeper analysis of  the onset detection functions used for beat tracking, indicate that there is enough information on the signal level to characterize the bi-metric behavior of bambucos. With this in mind, we highlight possibilities for computational analysis of rhythm in bambucos.","affiliations":"Fraunhofer IDMT|Universidad de Antioquia|Universidad de Antioquia|Universidad Pontificia Bolivariana|Universidad Pontificia Bolivariana|Universidad de Antioquia|Universidad de Antioquia","authors":"Estefania Cano|Fernando Mora \u221a\u00c5ngel|Gustavo Adolfo L\u221a\u2265pez Gil|Jos\u221a\u00a9 Ricardo Zapata|Antonio Escamilla|Juan Fernando Alzate Londo\u221a\u00b1o|Moises Betancur Pelaez","keywords":"Domain knowledge|Computational music theory and musicology|Applications|Digital libraries and archives|Evaluation, datasets, and reproducibility|Novel datasets and use cases|Musical features and properties|Rhythm, beat, tempo","primary_author":"Estefania Cano","primary_email":"estefaniacano@gmail.com","session":"1A","title":"Sesquialtera in the Colombian bambuco: Perception and estimation of beat and meter","type":"paper"},{"UID":"157","abstract":"Music is hierarchically structured, in which the global attributes (e.g., the determined tonal structure, musical form) dominate the distribution of local elements (e.g., pitch, playing technique arrangement). Existing methods for instrumental playing technique detection mostly focus on the local features extracted from audio. However, we argue that structural information is critical for both global and local tasks, particularly considering the characteristics of Guqin music. Incorporating mode and playing technique analysis, this study demonstrates that the structural relationship between notes is crucial for detecting mode, and such information also provides extra guidance for the playing technique detection in local-level. In this study, a new dataset is compiled for Guqin performance analysis. The mode detection is achieved by pattern matching, and the predicted results are conjoined with audio features to be inputted into a neural network for playing technique detection. Advanced techniques are developed to optimize the extracted pitch contour from the audio. It is manifest in the results that the global and local features are inter-connected in Guqin music. Our analysis identifies key components affecting the recognition of mode and playing technique, and challenging cases resulting from unique properties in Guqin audio signal are discussed for further research.","affiliations":"Academia Sinica|Department of Traditional Music, Taipei National University of the Arts|Institute of Information Science Academia Sinica|Academia Sinica","authors":"Yu-Fen Huang|Jeng-I Liang|I-CHIEH WEI|Li Su","keywords":"Domain knowledge|Computational music theory and musicology|MIR tasks|Automatic classification|Musical features and properties|Expression and performative aspects of music","primary_author":"Yu-Fen Huang","primary_email":"yufen91@gmail.com","session":"1A","title":"Joint analysis of mode and playing technique in Guqin performance with machine learning","type":"paper"},{"UID":"159","abstract":"A score-following program traces the notes in a musical score during a performance. This capability is essential to many meaningful applications that synchronize audio with a score in an on-line fashion. Existing algorithms often stumble on certain difficult cases, one of which is piano music. This paper presents a new method to tackle such cases. The method treats tempo as a variable rather than a constant (with constraints), allowing the program to adapt to live performance variations. This is first expressed by a Kalman filter model at the note level, and then by an almost equivalent switching state-space model at the audio frame level. The latter contains both discrete and continuous hidden variables, and is computationally intractable. We show how certain reasonable approximations make the computation manageable. This new method is tested on a dataset of 50 piano excerpts. Compared with a previously established state-of-the-art algorithm, the new method shows more stable and accurate results: it reduces fatal score-following errors, and improves accuracy from 65.0% to 69.1%.","affiliations":"University of Richmond","authors":"Yucong Jiang","keywords":"MIR tasks|Alignment, synchronization, and score following|Domain knowledge|Machine learning/Artificial intelligence for music|MIR fundamentals and methodology|Music signal processing","primary_author":"Yucong Jiang","primary_email":"yjiang3@richmond.edu","session":"1A","title":"Score following with hidden tempo using a switching state-space model","type":"paper"},{"UID":"160","abstract":"The lack of labeled data is a major obstacle in many music information retrieval tasks such as melody extraction, where labeling is extremely laborious or costly. Semi-supervised learning (SSL) provides a solution to alleviate the issue by leveraging a large amount of unlabeled data. In this paper, we propose an SSL method using teacher-student models for vocal melody extraction. The teacher model is pre-trained with labeled data and guides the student model to make identical predictions given unlabeled input in a self-training setting. We examine three setups of teacher-student models with different data augmentation schemes and loss functions. Also, considering the scarcity of labeled data in the test phase, we artificially generate large-scale testing data with pitch labels from unlabeled data using an analysis-synthesis method. The results show that the SSL method significantly increases the performance against supervised learning only and the improvement depends on the teacher-student models, the size of unlabeled data, the number of self-training iterations, and other training details. We also find that it is essential to ensure that the unlabeled audio has vocal parts. Finally, we show that the proposed SSL method allows a simple convolutional recurrent neural network model to achieve performance comparable to state-of-the-arts.","affiliations":"KAIST|Academia Sinica|Academia Sinica|KAIST","authors":"Sangeun Kum|Jing-Hua Lin|Li Su|Juhan Nam","keywords":"Musical features and properties|Melody and motives|Evaluation, datasets, and reproducibility|Novel datasets and use cases|MIR tasks|Music transcription and annotation","primary_author":"Sangeun Kum","primary_email":"keums@kaist.ac.kr","session":"1A","title":"Semi-supervised learning using teacher-student models for vocal melody extraction","type":"paper"},{"UID":"162","abstract":"Disentangling factors of variation aims to uncover latent variables that underlie the process of data generation. In this paper, we propose a framework that achieves unsupervised pitch and timbre disentanglement for isolated musical instrument sounds without relying on data annotations or pre-trained neural networks. Our framework, based on variational auto-encoders, takes as input a spectral frame, and encodes pitch and timbre as categorical and continuous variables, respectively. The input is then reconstructed by combining those variables. Under an unsupervised training setting, a major challenge is that encoders are tasked to capture factors of interest with distinct latent representations, without access to the corresponding ground-truth labels. We therefore introduce auxiliary tasks and objectives which leverage pitch shifting as a strategy to create surrogate labels, thereby encouraging the disentanglement of pitch and timbre. Through an ablation study we analyze the impact of the proposed objectives. The evaluation shows the efficacy of the proposed framework for learning disentangled representations, and verifies its applicability to unsupervised pitch classification and conditional spectral synthesis.","affiliations":"Singapore University of Technology and Design|Singapore University of Technology and Design|National Institute of Advanced Industrial Science and Technology|National Institute of Advanced Industrial Science and Technology|Singapore University of Technology and Design","authors":"Yin-Jyun Luo|Kin Wai Cheuk|Tomoyasu Nakano|Masataka Goto|Dorien Herremans","keywords":"Domain knowledge|Machine learning/Artificial intelligence for music","primary_author":"Yin-Jyun Luo","primary_email":"yinjyun_luo@mymail.sutd.edu.sg","session":"1A","title":"Unsupervised Disentanglement of Pitch and Timbre for Isolated Musical Instrument Sounds","type":"paper"},{"UID":"165","abstract":"When making judgements, humans are known to be better at choosing a preferred option amongst a small number of options, rather than giving an absolute ranking of all the options. This preference-based judgment rank-ordering method is called Best-Worst Scaling (BWS). Inspired by this concept, we propose a preference-based framework to generate a relative rank-ordering of singing vocals, and therefore, singers. We adopt a twin-neural network (Siamese) that learns to choose a preferred candidate in terms of singing quality between two inputs. With a few such pairwise comparisons, this method generates a relative rank-order of a complete list of singers. Additionally, we incorporate a knowledge-based musically-relevant pitch histogram representation, as a conditioning vector, to provide explicit musical information to the network. The experiments show that this method is able to reliably evaluate singing quality and rank-order singing vocals, independent of the song or the singer. The results suggest that the twin-neural network learns the underlying discerning properties relevant to singing quality, instead of being specific to the content of a song or singer.","affiliations":"National University of Singapore|National University of Singapore|National University of Singapore","authors":"Chitralekha Gupta|Lin Huang|Haizhou Li","keywords":"Domain knowledge|Machine learning/Artificial intelligence for music|Applications|Music training and education|Evaluation, datasets, and reproducibility|Evaluation methodology|MIR fundamentals and methodology|Music signal processing|MIR tasks|Similarity metrics|Musical features and properties|Timbre, instrumentation, and voice","primary_author":"Chitralekha Gupta","primary_email":"chitralekha@nus.edu.sg","session":"1A","title":"Automatic Rank-Ordering of Singing Vocals with Twin-Neural Network","type":"paper"},{"UID":"167","abstract":"Machine learning is challenging the way we make music. Although research in deep generative models has dramatically improved the capability and fluency of music models, recent work has shown that it can be challenging for humans to partner with this new class of algorithms. In this paper, we present findings on what 13 musician/developer teams, a total of 61 users, needed when co-creating a song with AI, the challenges they faced, and how they leveraged and repurposed existing characteristics of AI to overcome some of these challenges. Many teams adopted modular approaches, such as independently running multiple smaller models that align with the musical building blocks of a song, before re-combining their results. As ML models are not easily steerable, teams also generated massive numbers of samples and curated them post-hoc, or used a range of strategies to direct the generation or algorithmically ranked the samples. Ultimately, teams not only had to manage the ``flare and focus'' aspects of the creative process, but also juggle that with a parallel process of exploring and curating multiple ML models and outputs. These findings reflect a need to design machine learning-powered music interfaces that are more decomposable, steerable, interpretable, and adaptive, which in return will enable artists to more effectively explore how AI can extend their personal expression.  ","affiliations":"Google Brain|RTL Netherlands|ByteDance|Google Brain|","authors":"Cheng-Zhi Anna Huang|Hendrik Vincent Koops|Ed Newton-Rex|Monica  Dinculescu|Carrie Cai","keywords":"Human-centered MIR|User-centered evaluation|Domain knowledge|Machine learning/Artificial intelligence for music","primary_author":"Cheng-Zhi Anna Huang","primary_email":"chengzhiannahuang@gmail.com","session":"1A","title":"Human-AI Co-Creation in Songwriting","type":"paper"},{"UID":"172","abstract":"For recommending songs to a user, one effective approach is to represent artists and songs with latent vectors and predict the user's preference toward the songs. Although the latent vectors represent the characteristics of artists and songs well, they have typically been used only for computing the preference score. In this paper, we discuss how we can leverage these vectors for realizing applications that enable users to search for songs from new perspectives. To this end, by embedding song/artist vectors into the same feature space, we first propose two concepts of artist-song relationships: overall similarity and prominent affinity. Overall similarity is the degree to which the characteristics of a song are similar overall to the characteristics of the artist; while prominent affinity is the degree to which a song prominently represents the characteristics of the artist. By using Last.fm play logs for two years, we analyze the characteristics of the concepts. Moreover, based on the analysis results, we propose three applications for song search. Through case studies, we demonstrate that our proposed applications are beneficial for searching for songs according to the users' various search intents.","affiliations":"National Institute of Advanced Industrial Science and Technology|National Institute of Advanced Industrial Science and Technology","authors":"Kosetsu Tsukuda|Masataka Goto","keywords":"Applications|Music retrieval systems","primary_author":"Kosetsu Tsukuda","primary_email":"k.tsukuda@aist.go.jp","session":"1A","title":"Analysis of Song/Artist Latent Features and Its Application for Song Search","type":"paper"},{"UID":"182","abstract":"In this paper, we introduce the MIDI Degradation Toolkit (MDTK), containing functions which take as input a musical excerpt (a set of notes with pitch, onset time, and duration), and return a ``degraded'' version of that excerpt with some error (or errors) introduced. Using the toolkit, we create the Altered and Corrupted MIDI Excerpts dataset version 1.0 (ACME v1.0), and propose four tasks of increasing difficulty to detect, classify, locate, and correct the degradations. We hypothesize that models trained for these tasks can be useful in (for example) improving automatic music transcription performance if applied as a post-processing step. To that end, MDTK includes a script that measures the distribution of different types of errors in a transcription, and creates a degraded dataset with similar properties. MDTK's degradations can also be applied dynamically to a dataset during training (with or without the above script), generating novel degraded excerpts each epoch. MDTK could also be used to test the robustness of any system designed to take MIDI (or similar) data as input (e.g. systems designed for voice separation, metrical alignment, or chord detection) to such transcription errors or otherwise noisy data. The toolkit and dataset are both publicly available online, and we encourage contribution and feedback from the community.","affiliations":"EPFL|University of Edinburgh|Kyoto University","authors":"Andrew McLeod|James Owers|Kazuyoshi Yoshii","keywords":"Evaluation, datasets, and reproducibility|Novel datasets and use cases|MIR fundamentals and methodology|Symbolic music processing|MIR tasks|Music transcription and annotation","primary_author":"Andrew McLeod","primary_email":"andrew.mcleod@epfl.ch","session":"1A","title":"The MIDI Degradation Toolkit: Symbolic Music Augmentation and Correction","type":"paper"},{"UID":"187","abstract":"In this paper, we present MusPy, an open source Python library for symbolic music generation. MusPy provides easy-to-use tools for essential components in a music generation system, including dataset management, data I/O, data preprocessing and model evaluation. In order to showcase its potential, we present statistical analysis of the eleven datasets currently supported by MusPy. Moreover, we conduct a cross-dataset generalizability experiment by training an autoregressive model on each dataset and measuring held-out likelihood on the others---a process which is made easier by MusPy's dataset management system. The results provide a map of domain overlap between various commonly used datasets and show that some datasets contain more representative cross-genre samples than others. Along with the dataset analysis, these results might serve as a guide for choosing datasets in future research. Source code and documentation are available at https://github.com/salu133445/muspy .","affiliations":"University of California San Diego|University of California San Diego|University of California, San Diego|UCSD","authors":"Hao-Wen Dong|Ke Chen|Julian McAuley|Taylor Berg-Kirkpatrick","keywords":"MIR fundamentals and methodology|Symbolic music processing|Applications|Music composition, performance, and production|Domain knowledge|Machine learning/Artificial intelligence for music|Representations of music|Evaluation, datasets, and reproducibility|Evaluation metrics|Reproducibility","primary_author":"Hao-Wen Dong","primary_email":"hwdong@ucsd.edu","session":"1A","title":"MusPy: A Toolkit for Symbolic Music Generation","type":"paper"},{"UID":"188","abstract":"Current state-of-art results in Music Information Retrieval are largely dominated by deep learning approaches. These provide unprecedented accuracy across all discriminative tasks. However, the consistently overlooked downside of these models is their stunningly massive complexity, which seems concomitantly crucial to their success. In this paper, we address this issue by developing a new approach based on the recent lottery ticket hypothesis. We modify the original lottery approach to allow for explicitly removing parameters, through structured trimming of entire units, instead of simply masking individual weights. This allows to obtain models which are effectively lighter in terms of size, memory and number of operations.We show that our proposal allows to remove up to 95% of the models parameters without loss of accuracy, leading to ultra-light deep MIR models. We confirm the surprising result that, at smaller compression ratios (removing up to 90% of the network), lighter models consistently outperform their heavier counterpart. We exhibit these results on a large array of MIR tasks including audio classification, pitch recognition, chord extraction, drum transcription and onset estimation. These resulting ultra-light deep models for MIR can run on CPU, and can even fit on embedded devices with minimal degradation of accuracy. ","affiliations":"IRCAM|Sony CSL / IRCAM|IRCAM|Ircam|IRCAM","authors":"Philippe  Esling|Th\u221a\u00a9is Bazin|Adrien Bitton|Tristan J. J. Carsault|Ninon Devis","keywords":"Domain knowledge|Machine learning/Artificial intelligence for music|Applications|Music retrieval systems|Representations of music|Evaluation, datasets, and reproducibility|MIR tasks|Automatic classification","primary_author":"Philippe  Esling","primary_email":"esling@ircam.fr","session":"1A","title":"Ultra-light deep MIR by trimming lottery tickets","type":"paper"},{"UID":"189","abstract":"Data cleansing is a well studied strategy for cleaning erroneous labels in datasets, which has not yet been widely adopted in Music Information Retrieval.Previously proposed data cleansing models do not consider structured (e.g. time varying) labels, such as those common to music data.We propose a novel data cleansing model for time-varying, structured labels which exploits the local structure of the labels, and demonstrate its usefulness for vocal note event annotations in music.Our model is trained in a contrastive learning manner by automatically contrasting likely correct labels pairs against local deformations of them.We demonstrate that the accuracy of a transcription model improves greatly when trained using our proposed data cleaning strategy compared with the accuracy when trained using the original dataset.Additionally we use our model to estimate the annotation error rates in the DALI dataset, and highlight other potential uses for this type of model.","affiliations":"IRCAM|Spotify|Spotify|Spotify","authors":"Gabriel Meseguer Brocal|Rachel Bittner|Simon Durand|Brian Brost","keywords":"Domain knowledge|Machine learning/Artificial intelligence for music|Evaluation, datasets, and reproducibility|Novel datasets and use cases|MIR tasks|Music transcription and annotation","primary_author":"Gabriel Meseguer Brocal","primary_email":"gabriel.meseguer.brocal@gmail.com","session":"1A","title":"Data Cleansing with Contrastive Learning for Vocal Note Event Annotations","type":"paper"},{"UID":"190","abstract":"Tagging a musical excerpt with an emotion label may result in a vague and ambivalent exercise. This subjectivity entangles several high-level music description tasks when the computational models built to address them produce predictions on the basis of a \"ground truth\". In this study, we investigate the relationship between emotions perceived in pop and rock music (mainly in Euro-American styles) and personal characteristics from the listener, using language as a key feature. Our goal is to understand the influence of lyrics comprehension on music emotion perception and use this knowledge to improve Music Emotion Recognition (MER) models. We systematically analyze over 30K annotations of 22 musical fragments to assess the impact of individual differences on agreement, as defined by Krippendorff's coefficient. We employ personal characteristics to form group-based annotations by assembling ratings with respect to listeners' familiarity, preference, lyrics comprehension, and music sophistication. Finally, we study our group-based annotations in a two-fold approach: (1) assessing the similarity within annotations using manifold learning algorithms and unsupervised clustering, and (2) analyzing their performance by training classification models with diverse \"ground truths\". Our results suggest that a) applying a broader categorization of taxonomies and b) using multi-label, group-based annotations based on language, can be beneficial for MER models.","affiliations":"Universitat Pompeu Fabra|Fraunhofer IDMT|Universitat Pompeu Fabra|Universitat Pompeu Fabra","authors":"Juan S. G\u221a\u2265mez-Ca\u221a\u00b1\u221a\u2265n|Estefania Cano|Perfecto Herrera|Emilia Gomez","keywords":"Musical features and properties|Musical affect, emotion, and mood|Domain knowledge|Cognitive MIR|Evaluation, datasets, and reproducibility|Annotation protocols|Evaluation methodology|Human-centered MIR|User-centered evaluation","primary_author":"Juan S. G\u221a\u2265mez-Ca\u221a\u00b1\u221a\u2265n","primary_email":"juansebastian.gomez@upf.edu","session":"1A","title":"Joyful for you and tender for us: the influence of individual characteristics and language on emotion labeling and classification","type":"paper"},{"UID":"207","abstract":"The digitization of the content within musical manuscripts allows the possibility of preserving, disseminating, and exploiting that cultural heritage. The automation of this process has been object of study for a long time in the field of Optical Music Recognition (OMR), with a wide variety of proposed solutions. Currently, there is a tendency to use machine learning strategies based on neural networks because of their high performance and flexibility to adapt to different scenarios by changing only the training data. However, most of the recent literature addresses only specific parts of the traditional OMR workflow such as music object detection or symbol classification. In this paper, we progress one step further by proposing a full-page OMR system for Mensural notation scores that consists of simply two processes, which are enough to extract the symbolic music information from a full page. More precisely, our pipeline uses Selectional Auto-Encoders to extract single staff regions, combined with end-to-end staff-level recognition based on Convolutional Recurrent Neural Networks for retrieving the music notation. The results confirm the adequacy of our method, reporting a successful behavior on two Mensural collections (Capitan and Seils datasets) with a straightforward implementation.","affiliations":"University of Alicante|University of Alicante|University of Alicante","authors":"Francisco J. Castellanos|Jorge Calvo-Zaragoza|Jose M. Inesta","keywords":"MIR tasks|Optical Music Recognition (OMR)|Applications|Music retrieval systems","primary_author":"Francisco J. Castellanos","primary_email":"fcastellanos@dlsi.ua.es","session":"1A","title":"A Neural Approach for Full-Page Optical Music Recognition of Mensural Documents","type":"paper"},{"UID":"211","abstract":"The GrooveToolbox is a new Python library implementing numerous algorithms, both novel and pre-existing, for the analysis of symbolic drum loops, including rhythm features, similarity metrics and microtiming features. As part of the GrooveToolbox we introduce two new metrics of rhythm similarity and four new features for describing the perceptually salient properties of microtiming deviations in drum loops. Based on a two-part perceptual evaluation, we show these four new microtiming features can each correlate to similarity perception, and be used along with rhythm similarity metrics to improve personalized similarity models for complex drum loops. A new measure of structural rhythmic similarity is also shown to correlate more strongly to similarity perception of drum loops than the more commonly used Hamming distance. These results point to the potential application of the GrooveToolbox and its new features in drum loop analysis for intelligent music production tools. The GrooveToolbox may be found at:  https://github.com/fredbru/GrooveToolbox","affiliations":"Queen Mary University of London|RITMO, University of Oslo|inMusic Brands|Queen Mary University of London","authors":"Fred Bruford|Olivier Lartillot|SKoT McDonald|Mark B. Sandler","keywords":"Musical features and properties|Rhythm, beat, tempo|Applications|Music retrieval systems|Evaluation, datasets, and reproducibility|Novel datasets and use cases|Human-centered MIR|Personalization|MIR fundamentals and methodology|Symbolic music processing|MIR tasks|Similarity metrics","primary_author":"Fred Bruford","primary_email":"fred.bruford@roli.com","session":"1A","title":"Multidimensional similarity modelling of complex drum loops using the GrooveToolbox","type":"paper"},{"UID":"219","abstract":"The human response to music combines low-level expectations that are driven by the perceptual characteristics of audio with high-level expectations from the context and the listener's expertise. This paper discusses surprisal based music representation learning with a hierarchical predictive neural network. In order to inspect the cognitive validity of the network's predictions along their time-scales, we use the network's prediction error to segment electroencephalograms (EEG) based on the audio signal. Using the NMED-T dataset on passive natural music listening we explore the automatic segmentation of audio and EEG into events using the suggested model. By averaging only the EEG signal at predicted locations, we were able to visualize auditory evoked potentials connected to local and global musical structures. This indicates the potential of unsupervised predictive learning with deep neural networks as means to retrieve musical structure from audio and as a basis to uncover the corresponding cognitive processes in the human brain.","affiliations":"Otto von Guericke University|Otto von Guericke University","authors":"Andr\u221a\u00a9 Ofner|Sebastian Stober","keywords":"Domain knowledge|Machine learning/Artificial intelligence for music|Cognitive MIR|Representations of music|Human-centered MIR|Personalization|MIR fundamentals and methodology|Multimodality|Musical features and properties|Rhythm, beat, tempo","primary_author":"Andr\u221a\u00a9 Ofner","primary_email":"ofner@ovgu.de","session":"1A","title":"Modeling perception with hierarchical prediction: Auditory segmentation with deep predictive coding locates candidate evoked potentials in EEG","type":"paper"},{"UID":"222","abstract":"High-level musical qualities (such as emotion) are often abstract, subjective, and hard to quantify. Given these difficulties, it is not easy to learn good feature representations with supervised learning techniques, either because of the insufficiency of labels, or the subjectiveness (and hence large variance) in human-annotated labels. In this paper, we present a framework that can learn high-level feature representations with a limited amount of data, by first modelling their corresponding quantifiable low-level attributes. We refer to our proposed framework as Music FaderNets, which is inspired by the fact that low-level attributes can be continuously manipulated by separate \"sliding faders\" through feature disentanglement and latent regularization techniques. High-level features are then inferred from the low-level representations through semi-supervised clustering using Gaussian Mixture Variational Autoencoders (GM-VAEs). Using arousal as an example of a high-level feature, we show that the \"faders\" of our model are disentangled and change linearly w.r.t. the modelled low-level attributes of the generated output music. Furthermore, we demonstrate that the model successfully learns the intrinsic relationship between arousal and its corresponding low-level attributes (rhythm and note density), with only 1% of the training set being labelled. Finally, using the learnt high-level feature representations, we explore the application of our framework in style transfer tasks across different arousal states. The effectiveness of this approach is verified through a subjective listening test.","affiliations":"Singapore University of Technology and Design|Singapore University of Technology and Design","authors":"HAO HAO TAN|Dorien Herremans","keywords":"Domain knowledge|Machine learning/Artificial intelligence for music|MIR tasks|Music synthesis and transformation","primary_author":"HAO HAO TAN","primary_email":"haohao_tan@sutd.edu.sg","session":"1A","title":"Music FaderNets: Controllable Music Generation Based On High-Level Features via Low-Level Feature Modelling","type":"paper"},{"UID":"223","abstract":"In this paper, we undertake a critical assessment of a state-of-the-art deep neural network approach for computational rhythm analysis. Our methodology is to deconstruct this approach, analyse its constituent parts, and then reconstruct it. To this end, we devise a novel multi-task approach for the simultaneous estimation of tempo, beat, and downbeat. In particular, we seek to embed more explicit musical knowledge into the design decisions in building the network. We additionally reflect this outlook when training the network, and include a simple data augmentation strategy to increase the network's exposure to a wider range of tempi, and hence beat and downbeat information. Via an in-depth comparative evaluation, we present state-of-the-art results over all three tasks, with performance increases of up to 6% points over existing systems.","affiliations":"enliteAI|CISUC - Centre for Informatics and Systems of the University of Coimbra","authors":"Sebastian B\u221a\u2202ck|Matthew Davies","keywords":"Musical features and properties|Rhythm, beat, tempo|Domain knowledge|Machine learning/Artificial intelligence for music|MIR fundamentals and methodology|Music signal processing|MIR tasks|Automatic classification","primary_author":"Sebastian B\u221a\u2202ck","primary_email":"s.boeck@enlite.ai","session":"1A","title":"Deconstruct, Analyse, Reconstruct: How to improve Tempo, Beat, and Downbeat Estimation","type":"paper"},{"UID":"225","abstract":"Music producers who use loops may have access to thousands in loop libraries, but finding ones that are compatible is a time-consuming process; we hope to reduce this burden with automation. State-of-the-art systems for estimating compatibility, such as AutoMashUpper, are mostly rule-based and could be improved on with machine learning. To train a model, we need a large set of loops with ground truth compatibility values. No such dataset exists, so we extract loops from existing music to obtain positive examples of compatible loops, and propose and compare various strategies for choosing negative examples. For reproducibility, we curate data from the Free Music Archive. Using this data, we investigate two types of model architectures for estimating the compatibility of loops: one based on a Siamese network, and the other a pure convolutional neural network (CNN). We conducted a user study in which participants rated the quality of the combinations suggested by each model, and found the CNN to outperform the Siamese network. Both model-based approaches outperformed the rule-based one. We have opened source the code for building the models and the dataset.","affiliations":"Academia Sinica|TikTok|Academia Sinica","authors":"Bo-Yu Chen|Jordan B. L. Smith|Yi-Hsuan Yang","keywords":"Applications|Music composition, performance, and production|Domain knowledge|Machine learning/Artificial intelligence for music|Human-centered MIR|User-centered evaluation","primary_author":"Bo-Yu Chen","primary_email":"bernie40916@gmail.com","session":"1A","title":"Neural Loop Combiner: Neural Network Models for Assessing the Compatibility of Loops","type":"paper"},{"UID":"226","abstract":"The task of melodic segmentation is a long-standing MIR task that has not yet been solved. In this paper, a rule mining algorithm is employed to find rule sets that classify notes within their local context as phrase boundaries. Both the discovered rule set and a Random Forest Classifier trained on the same data set outperform previous methods on the task of melodic segmentation of melodies from the Essen Folk Song Collection, the Meertens Tune Collections, and the set of Bach Chorales. By inspecting the rules, some important clues are revealed about what constitutes a melodic phrase boundary, notably a prevalence of rhythm features over pitch features.","affiliations":"Utrecht University, Meertens Institute","authors":"Peter Van Kranenburg","keywords":"Musical features and properties|Structure, segmentation, and form|Domain knowledge|Computational music theory and musicology|Machine learning/Artificial intelligence for music|Evaluation, datasets, and reproducibility|MIR tasks|MIR fundamentals and methodology|Symbolic music processing|Melody and motives","primary_author":"Peter Van Kranenburg","primary_email":"peter.van.kranenburg@meertens.knaw.nl","session":"1A","title":"Rule Mining for Local Boundary Detection in Melodies","type":"paper"},{"UID":"228","abstract":"Psychology research has shown that song lyrics are a rich source of data, yet they are often overlooked in the field of MIR compared to audio. In this paper, we provide an initial assessment of the usefulness of features drawn from lyrics for various fields, such as MIR and Music Psychology. To do so, we assess the performance of lyric-based text features on 3 MIR tasks, in comparison to audio features. Specifically, we draw sets of text features from the field of Natural Language Processing and Psychology. Further, we estimate their effect on performance while statistically controlling for the effect of audio features, by using a hierarchical regression statistical model. Lyric-based features show a small but statistically significant effect, that anticipates further research. Implications and directions for future studies are discussed.","affiliations":"Delft University of Technology|Delft University of Technology|Delft University of Technology|Musixmatch|Delft University of Technology","authors":"Jaehun Kim|Andrew M. Demetriou|Sandy Manolios|M. Stella Tavella|Cynthia C. S. Liem","keywords":"MIR fundamentals and methodology|Lyrics and other textual data, web mining, and natural language |Applications|Music recommendation and playlist generation|Domain knowledge|Machine learning/Artificial intelligence for music|Evaluation, datasets, and reproducibility|MIR tasks|Automatic classification","primary_author":"Jaehun Kim","primary_email":"j.h.kim@tudelft.nl","session":"1A","title":"\"Butter Lyrics Over Hominy Grit\": Comparing Audio and Psychology-Based Text Features in MIR Tasks","type":"paper"},{"UID":"232","abstract":"Many musics across the world are structured around multiple modes, which hold a middle ground between scales and melodies. We study whether we can classify mode in a corpus of 20,865 medieval plainchant melodies from the Cantus database. We revisit the traditional \u201a\u00c4\u00f2textbook\u201a\u00c4\u00f4 classification approach (using the final, the range and initial note) as well as the only prior computational study we are aware of, which uses pitch profiles. Both approaches work well, but largely reduce modes to scales and ignore their melodic character. Our main contribution is a model that reaches 93\u201a\u00c4\u00ec95% F1 score on mode classification, compared to 86\u201a\u00c4\u00ec90% using traditional pitch-based musicological methods. Importantly, it reaches 81\u201a\u00c4\u00ec83% even when we discard all absolute pitch information and reduce a melody to its contour. The model uses tf\u201a\u00c4\u00ecidf vectors and strongly depends on the choice of units: i.e., how the melody is segmented. If we borrow the syllable or word structure from the lyrics, the model outperforms all of our baselines. This suggests that, like language, music is made up of \u201a\u00c4\u00f2natural\u201a\u00c4\u00f4 units, in our case between the level of notes and complete phrases, a finding that may well be useful in other musics.","affiliations":"University of Amsterdam|ILLC, UvA|University of Amsterdam","authors":"Bas Cornelissen|Willem Zuidema|John Ashley Burgoyne","keywords":"Musical features and properties|Melody and motives|Domain knowledge|Computational music theory and musicology|Harmony, chords, and tonality|Structure, segmentation, and form","primary_author":"Bas Cornelissen","primary_email":"mail@bascornelissen.nl","session":"1A","title":"Mode classification and natural units in plainchant","type":"paper"},{"UID":"235","abstract":"In this paper, we describe a workflow of successive corrections on Optical Music Recognition (OMR) generated MusicXML files and their respective outputs under Music Information Retrieval tasks. The original OMR-generated files of six Mendelssohn String Quartets were initially corrected by individual members of this interdisciplinary group, then reviewed by others to further standardize the quality and music analysis priorities of the team. Four MIR tasks are applied to each round of corrections on this collection: cadence detection, chord labeling, key finding, and monophonic pattern discovery.We measure changes in the outputs of these four MIR tasks from one round of correction to the next in order to evaluate the impact of corrections. Results show that expert revision is more beneficial to some MIR tasks than to others. The resulting corpus of curated MusicXML files is available as an open-source repository under a Creative Commons Attribution 4.0 International License for further MIR research.","affiliations":"McGill University|McGill University|Universit\u221a\u00a9 de Lille|McGill University|McGill University|McGill University|McGill University|McGill University, CIRMMT|McGill University","authors":"Jacob deGroot-Maggetti|Timothy R de Reuse|Laurent Feisthauer|Samuel Howes|Yaolong Ju|Suzuka Kokubu|Sylvain Margot|N\u221a\u00a9stor N\u221a\u00b0poles L\u221a\u2265pez|Finn Upham","keywords":"Evaluation, datasets, and reproducibility|Novel datasets and use cases|Applications|Music retrieval systems|Domain knowledge|Computational music theory and musicology|MIR fundamentals and methodology|Symbolic music processing|MIR tasks|Music transcription and annotation","primary_author":"Laurent Feisthauer","primary_email":"laurent.feisthauer@univ-lille.fr","session":"1A","title":"Data Quality Matters: Iterative Corrections on a Corpus of Mendelssohn String Quartets and Implications for MIR Analysis","type":"paper"},{"UID":"236","abstract":"We introduce CONLON, a pattern-based MIDI generation method that employs a new lossless pianoroll-like data description in which velocities and durations are stored in separate channels. CONLON uses Wasserstein autoencoders as the underlying generative model. Its generation strategy is similar to interpolation, where MIDI pseudo-songs are obtained by concatenating patterns decoded from smooth trajectories in the embedding space, but aims to produce a smooth result in the pattern space by computing optimal trajectories as the solution of a widest-path problem. A set of surveys enrolling 69 professional musicians shows that our system, when trained on datasets of carefully selected and coherent patterns, is able to produce pseudo-songs that are musically consistent and potentially useful for professional musicians. Additional materials can be found at https://paolo-f.github.io/CONLON/ .","affiliations":"University of Florence|Eindhoven University of Technology|Musica Combinatoria|University of Florence","authors":"Luca Angioloni|Valentijn Borghuis|Lorenzo  Brusci|Paolo Frasconi","keywords":"Domain knowledge|Machine learning/Artificial intelligence for music|Applications|Music composition, performance, and production","primary_author":"Luca Angioloni","primary_email":"luca.angioloni@unifi.it","session":"1A","title":"CONLON: A Pseudo-Song Generator Based on a New Pianoroll, Wasserstein Autoencoders, and Optimal Interpolations","type":"paper"},{"UID":"239","abstract":"Recent works have addressed the automatic cover detection problem from a metric learning perspective. They employ different input representations, aiming to exploit melodic or harmonic characteristics of songs and yield promising performances. In this work, we propose a comparative study of these different representations and show that systems combining melodic and harmonic features drastically outperform those relying on a single input representation. We illustrate how these features complement each other with both quantitative and qualitative analyses. We finally investigate various fusion schemes and propose methods yielding state-of-the-art performances on two publicly-available large datasets.","affiliations":"Ircam|Universitat Pompeu Fabra|Dolby Laboratories|Universitat Pompeu Fabra|Telecom ParisTech","authors":"Guillaume Doras|Furkan Yesiler|Joan Serra|Emilia Gomez|Geoffroy Peeters","keywords":"Applications|Music retrieval systems|Domain knowledge|Machine learning/Artificial intelligence for music|MIR tasks|Automatic classification|Similarity metrics|Musical features and properties|Harmony, chords, and tonality|Melody and motives","primary_author":"Guillaume Doras","primary_email":"guillaume.doras@ircam.fr","session":"1A","title":"Combining musical features for cover detection","type":"paper"},{"UID":"241","abstract":"Music loops are essential ingredients in electronic music production, and there is a high demand for pre-recorded loops in a variety of styles. Several commercial and community databases have been created to meet this demand, but most of them are not suitable for research due to their strict licensing. In this paper, we present the Freesound Loop Dataset (FSLD), a new large-scale dataset of music loops annotated by experts. The loops originate from Freesound, a community database of audio recordings released under Creative Commons licenses, so the audio in our dataset may be redistributed. The annotations include instrument, meter, key and genre tags. We describe the methodology used to assemble and annotate the data, and report on the distribution of tags in the data and inter-annotator agreement. We also present to the community an online loop annotator tool that we developed. To illustrate the usefulness of FSLD, we present short case studies on using it to estimate tempo and key, generate new loops, and evaluate a loop separation algorithm. We anticipate that the community will find yet more uses for the data, in applications from automatic loop characterisation to algorithmic composition.","affiliations":"Universitat Pompeu Fabra|Music Technology Group - Universitat Pompeu Fabra|Universitat Pompeu Fabra|TikTok|Academia Sinica|Academia Sinica|Academia Sinica|Academia Sinica|Academia Sinica|Universitat Pompeu Fabra","authors":"Antonio Ramires|Frederic Font|Dmitry Bogdanov|Jordan B. L. Smith|Yi-Hsuan Yang|Joann Ching|Bo-Yu Chen|Yueh-Kao Wu|Hsu Wei-Han|Xavier Serra","keywords":"Evaluation, datasets, and reproducibility|Novel datasets and use cases|Applications|Music composition, performance, and production|Annotation protocols|Reproducibility","primary_author":"Antonio Ramires","primary_email":"antonio.ramires@upf.edu","session":"1A","title":"The Freesound Loop Dataset and Annotation Tool","type":"paper"},{"UID":"244","abstract":"Version identification systems aim to detect different renditions of the same underlying musical composition (loosely called cover songs). By learning to encode entire recordings into plain vector embeddings, recent systems have made significant progress in bridging the gap between accuracy and scalability, which has been a key challenge for nearly two decades. In this work, we propose to further narrow this gap by employing a set of data distillation techniques that reduce the embedding dimensionality of a pre-trained state-of-the-art model. We compare a wide range of techniques and propose new ones, from classical dimensionality reduction to more sophisticated distillation schemes. With those, we obtain 99% smaller embeddings that, moreover, yield up to a 3% accuracy increase. Such small embeddings can have an important impact in retrieval time, up to the point of making a real-world system practical on a standalone laptop.","affiliations":"Universitat Pompeu Fabra|Dolby Laboratories|Universitat Pompeu Fabra","authors":"Furkan Yesiler|Joan Serra|Emilia Gomez","keywords":"Applications|Music retrieval systems|MIR tasks|Indexing and querying|Similarity metrics","primary_author":"Furkan Yesiler","primary_email":"furkan.yesiler@upf.edu","session":"1A","title":"Less is more: Faster and better music version identification with embedding distillation","type":"paper"},{"UID":"245","abstract":"Music tags are commonly used to describe and categorize music. Various auto-tagging models and datasets have been proposed for the automatic music annotation with tags. However, the past approaches often neglect the fact that many of these tags largely depend on the user, especially the tags related to the context of music listening. In this paper, we address this problem by proposing a user-aware music auto-tagging system and evaluation protocol. Specifically, we use both the audio content and user information extracted from the user listening history to predict contextual tags for a given user/track pair. We propose a new dataset of music tracks annotated with contextual tags per user. We compare our model to the traditional audio-based model and study the influence of user embeddings on the classification quality. Our work shows that explicitly modeling the user listening history into the automatic tagging process could lead to more accurate estimation of contextual tags.","affiliations":"Telecom-Paristech|Deezer Research|LTCI - T\u221a\u00a9l\u221a\u00a9com Paris, IP Paris|Telecom Paristech","authors":"Karim M. Ibrahim|Elena V. Epure|Geoffroy Peeters|Gael Richard","keywords":"Human-centered MIR|Personalization|Evaluation, datasets, and reproducibility|Evaluation methodology|Novel datasets and use cases|User-centered evaluation|MIR fundamentals and methodology|Metadata, tags, linked data, and semantic web|MIR tasks|Automatic classification","primary_author":"Karim M. Ibrahim","primary_email":"karim.m.ibraheem@gmail.com","session":"1A","title":"Should we consider the users in contextual music auto-tagging models?","type":"paper"},{"UID":"246","abstract":"Most commercial music services rely on collaborative filtering to recommend artists and songs. While this method is effective for popular artists with large fanbases, it can present difficulties for recommending novel, lesser known artists due to a relative lack of user preference data. In this paper, we therefore seek to understand how content-based approaches can be used to more effectively recommend songs from these lesser known artists. Specifically, we conduct a user study to answer three questions. Firstly, do most users agree which songs are most acoustically similar? Secondly, is acoustic similarity a good proxy for how an individual might construct a playlist or recommend music to a friend? Thirdly, if so, can we find acoustic features that are related to human judgments of acoustic similarity? To answer these questions, our study asked 117 test subjects to compare two unknown candidate songs relative to a third known reference song. Our findings show that 1) judgments about acoustic similarity are fairly consistent, 2) acoustic similarity is highly correlated with playlist selection and recommendation, but not necessarily personal preference, and 3) we identify a subset of acoustic features from the Spotify Web API that is particularly predictive of human similarity judgments.","affiliations":"Cornell University|Cornell University|Ithaca College","authors":"Derek S Cheng|Thorsten Joachims|Douglas Turnbull","keywords":"Applications|Music recommendation and playlist generation|Human-centered MIR|User-centered evaluation|MIR tasks|Similarity metrics|Musical features and properties|Musical style and genre|Rhythm, beat, tempo","primary_author":"Derek S Cheng","primary_email":"dsc252@cornell.edu","session":"1A","title":"Exploring Acoustic Similarity for Novel Music Recommendation","type":"paper"},{"UID":"247","abstract":"We describe a novel approach for generating music using a self-correcting, non-chronological, autoregressive model. We represent music as a sequence of edit events, each of which denotes either the addition or removal of a note---even a note previously generated by the model. During inference, we generate one edit event at a time using direct ancestral sampling. Our approach allows the model to fix previous mistakes such as incorrectly sampled notes and prevent accumulation of errors which autoregressive models are prone to have. Another benefit of our approach is a finer degree of control during human and AI collaboration as our approach is notewise online. We show through quantitative metrics and human survey evaluation that our approach generates better results than orderless NADE and Gibbs sampling approaches.","affiliations":"Amazon|Amazon|Amazon|Amazon|Amazon","authors":"Wayne Chi|Prachi Kumar|Suri Yaddanapudi|Suresh Rahul|Umut Isik","keywords":"Domain knowledge|Machine learning/Artificial intelligence for music|Applications|Music composition, performance, and production|Representations of music|MIR tasks|Music synthesis and transformation","primary_author":"Wayne Chi","primary_email":"waynchi@amazon.com","session":"1A","title":"Generating Music with a Self-Correcting Non-Chronological Autoregressive Model","type":"paper"},{"UID":"254","abstract":"This paper addresses the extraction of multiple F0 values from polyphonic and a cappella vocal performances using convolutional neural networks (CNNs). We address the major challenges of ensemble singing, i.e., all melodic sources are vocals and singers sing in harmony. We build upon an existing architecture to produce a pitch salience function of the input signal, where the harmonic constant-Q transform (HCQT) and its associated phase differentials are used as an input representation. The pitch salience function is subsequently thresholded to obtain a multiple F0 estimation output. For training, we build a dataset that comprises several multi-track datasets of vocal quartets with F0 annotations.This work proposes and evaluates a set of CNNs for this task in diverse scenarios and data configurations, including recordings with additional reverb. Our models outperform a state-of-the-art method intended for the same music genre when evaluated with an increased F0 resolution, as well as a general-purpose method for multi-F0 estimation. We conclude with a discussion on future research directions.","affiliations":"Universitat Pompeu Fabra|New York University|Universitat Pompeu Fabra","authors":"Helena Cuesta|Brian McFee|Emilia Gomez","keywords":"MIR tasks|Music transcription and annotation|MIR fundamentals and methodology|Music signal processing|Musical features and properties|Melody and motives","primary_author":"Helena Cuesta","primary_email":"helena.cuesta@upf.edu","session":"1A","title":"Multiple F0 Estimation in Vocal Ensembles using Convolutional Neural Networks","type":"paper"},{"UID":"255","abstract":"Synthetic creation of drum sounds (e.g., in drum machines) is commonly performed using analog or digital synthesis, allowing a musician to sculpt the desired timbre modifying various parameters. Typically, such parameters control low-level features of the sound and often have no musical meaning or perceptual correspondence. With the rise of Deep Learning, data-driven processing of audio emerges as an alternative to traditional signal processing. This new paradigm allows controlling the synthesis process through learned high-level features or by conditioning a model on musically relevant information. In this paper, we apply a Generative Adversarial Network to the task of audio synthesis of drum sounds. By conditioning the model on perceptual features computed with a publicly available feature-extractor, intuitive control is gained over the generation process. The experiments are carried out on a large collection of kick, snare, and cymbal sounds. We show that, compared to a specific prior work based on a U-Net architecture, our approach considerably improves the quality of the generated drum samples, and that the conditional input indeed shapes the perceptual characteristics of the sounds. Also, we provide audio examples and release the code used in our experiments.","affiliations":"Sony CSL|Sony Computer Science Laboratories, Paris|LTCI, Paris Tech","authors":"Javier Nistal|Stefan Lattner|Ga\u221a\u00b4l Richard","keywords":"MIR tasks|Music synthesis and transformation|Domain knowledge|Machine learning/Artificial intelligence for music|Human-centered MIR|Human-computer interaction and interfaces","primary_author":"Javier Nistal","primary_email":"j.nistalhurle@gmail.com","session":"1A","title":"DRUMGAN: SYNTHESIS OF DRUM SOUNDS WITH TIMBRAL FEATURE CONDITIONING USING GENERATIVE ADVERSARIAL NETWORKS","type":"paper"},{"UID":"257","abstract":"Human annotation is still an essential part of modern transcription workflows for digitizing music scores, either as a standalone approach where a single expert annotator transcribes a complete score, or for supporting an automated Optical Music Recognition (OMR) system. Research on human computation has shown the effectiveness of crowdsourcing for scaling out human work by defining a large number of microtasks which can easily be distributed and executed. However, microtask design for music transcription is a research area that remains unaddressed. This paper focuses on the design of a crowdsourcing task to detect errors in a score transcription which can be deployed in either automated or human-driven transcription workflows. We conduct an experiment where we study two design parameters: 1) the size of the score to be annotated and 2) the modality in which it is presented in the user interface. We analyze the performance and reliability of non-specialised crowdworkers on Amazon Mechanical Turk with respect to these design parameters, differentiated by worker experience and types of transcription errors. Results are encouraging, and pave the way for scalable and efficient crowd-assisted music transcription systems.","affiliations":"Delft University of Technology|Delft University of Technology|TU Delft|Delft University of Technology|TU Delft|Delft University of Technology","authors":"Ioannis Petros Samiotis|Sihang Qiu|Andrea Mauri|Cynthia C. S. Liem|Christoph Lofi|Alessandro  Bozzon","keywords":"Domain knowledge|Representations of music|Evaluation, datasets, and reproducibility|Annotation protocols|Human-centered MIR|Human-computer interaction and interfaces|MIR fundamentals and methodology|Multimodality|MIR tasks|Music transcription and annotation|Optical Music Recognition (OMR)","primary_author":"Ioannis Petros Samiotis","primary_email":"i.p.samiotis@tudelft.nl","session":"1A","title":"Microtask Crowdsourcing for Music Score Transcriptions: An Experiment with Error Detection","type":"paper"},{"UID":"259","abstract":"Unison singing is the name given to an ensemble of singers simultaneously singing the same melody and lyrics. While each individual singer in a unison sings the same principle melody, there are slight timing and pitch deviations between the singers, which, along with the ensemble of timbres, give the listener a perceived sense of \"unison\". In this paper, we present a study of unison singing in the context of choirs; utilising some recently proposed deep-learning based methodologies, we analyse the fundamental frequency (F0) distribution of the individual singers in recordings of unison mixtures. Based on the analysis, we propose a system for synthesising a unison signal from an a cappella input and a single voice prototype representative of a unison mixture. We use subjective listening test to evaluate perceptual factors of our proposed system for synthesis, including quality, adherence to the melody as well the degree of perceived unison.","affiliations":"Universitat Pompeu Fabra|Universitat Pompeu Fabra|Universitat Pompeu Fabra","authors":"Pritish Chandna|Helena Cuesta|Emilia Gomez","keywords":"MIR tasks|Music synthesis and transformation|Applications|Music retrieval systems|Evaluation, datasets, and reproducibility|Human-centered MIR|User-centered evaluation|Musical features and properties|Expression and performative aspects of music","primary_author":"Pritish Chandna","primary_email":"pritish.chandna@upf.edu","session":"1A","title":"A Deep Learning Based Analysis-Synthesis Framework For Unison Singing","type":"paper"},{"UID":"260","abstract":"Open-source software libraries for audio/music analysis and feature extraction have a significant impact on the development of Audio Signal Processing and Music Information Retrieval (MIR) systems. Despite the abundance of such tools on the native computing platforms, there is a lack of an extensive and easy-to-use reference library for audio feature extraction on the Web. In this paper, we present Essentia.js, an open-source JavaScript (JS) library for audio and music analysis on both web clients and JS-based servers. Along with the Web Audio API, it can be used for efficient and robust real-time audio feature extraction on the web browsers. Essentia.js is modular, lightweight, and easy-to-use, deploy, maintain, and integrate into the existing plethora of JS libraries and Web technologies. It is powered by a WebAssembly back-end of the Essentia C++ library, which facilitates a JS interface to a wide range of low-level and high-level audio features. It also provides a higher-level JS API and add-on MIR utility modules along with extensive documentation, usage examples, and tutorials. We benchmark the proposed library on two popular web browsers, Node.js engine, and Android devices, comparing it to the native performance of Essentia and Meyda JS library.","affiliations":"Universitat Pompeu Fabra|Universitat Pompeu Fabra|SonoSuite|Universitat Pompeu Fabra","authors":"Albin Correya|Dmitry Bogdanov|Luis Joglar-Ongay|Xavier Serra","keywords":"MIR fundamentals and methodology|Music signal processing|Applications|Digital libraries and archives|Music composition, performance, and production|Music recommendation and playlist generation|Music retrieval systems|Music training and education","primary_author":"Albin Correya","primary_email":"albin.correya@upf.edu","session":"1A","title":"Essentia.js: A JavaScript Library for Music and Audio Analysis on the Web","type":"paper"},{"UID":"275","abstract":"We analyze and identify collaboration profiles in success-based music genre networks. Such networks are built upon data recently collected from both global and regional Spotify weekly charts.  Overall, our findings reveal an increase in the number of distinct successful genres from high-potential markets, pointing out that local repertoire is more important than ever on building the global music ecosystem. We also detect collaboration patterns mapped into four different profiles: Solid, Regular, Bridge and Emerging, wherein the two first depict higher average success. These findings indicate  great opportunities for the music industry by revealing the driving power of inter-genre collaborations within regional and global markets.","affiliations":"Universidade Federal de Minas Gerais|Universidade Federal de Minas Gerais|Universidade Federal de Minas Gerais|Universidade Federal de Minas Gerais|Universidade Federal de Minas Gerais","authors":"Gabriel Oliveira|Mariana Santos|Danilo B Seufitelli|Anisio Lacerda|Mirella M Moro","keywords":"MIR tasks|Pattern matching and detection|Domain knowledge|Machine learning/Artificial intelligence for music|Evaluation, datasets, and reproducibility|Novel datasets and use cases|Musical features and properties|Musical style and genre","primary_author":"Mirella M Moro","primary_email":"mirella@dcc.ufmg.br","session":"1A","title":"Detecting Collaboration Profiles in Success-based Music Genre Networks","type":"paper"},{"UID":"276","abstract":"Choral singing is a widely practiced form of ensemble singing wherein a group of people sing simultaneously in polyphonic harmony. The most commonly practiced setting for choir ensembles consists of four parts; Soprano, Alto, Tenor and Bass (SATB), each with its own range of fundamental frequencies (F0s). The task of source separation for this choral setting entails separating the SATB mixture into the constituent parts. Source separation for musical mixtures is well studied and many deep learning based methodologies have been proposed for the same. However, most of the research has been focused on a typical case which consists in separating vocal, percussion and bass sources from a mixture, each of which has a distinct spectral structure. In contrast, the simultaneous and harmonic nature of ensemble singing leads to high structural similarity and overlap between the spectral components of the sources in a choral mixture, making source separation for choirs a harder task than the typical case. This, along with the lack of an appropriate consolidated dataset has led to a dearth of research in the field so far. In this paper we first assess how well some of the recently developed methodologies for musical source separation perform for the case of SATB choirs. We then propose a novel domain-specific adaptation for conditioning the recently proposed U-Net architecture for musical source separation using the fundamental frequency contour of each of the singing groups and demonstrate that our proposed approach surpasses results from domain-agnostic architectures.","affiliations":"Universitat Pompeu Fabra|Universitat Pompeu Fabra|Universitat Pompeu Fabra|Universitat Pompeu Fabra|Universitat Pompeu Fabra","authors":"Darius Petermann|Pritish Chandna|Helena Cuesta|Jordi Bonada|Emilia Gomez","keywords":"MIR tasks|Music synthesis and transformation|Applications|Music composition, performance, and production|Music retrieval systems|Evaluation, datasets, and reproducibility|Novel datasets and use cases|MIR fundamentals and methodology|Music signal processing","primary_author":"Darius Petermann","primary_email":"dariusarthur.petermann01@estudiant.upf.edu","session":"1A","title":"Deep Learning Based Source Separation Applied To Choir Ensembles","type":"paper"},{"UID":"279","abstract":"A piece of music can be expressively performed, or interpreted, in a variety of ways. With the help of an online questionnaire, the Con Espressione Game, we collected some 1,500 descriptions of expressive character relating to 45 performances of 9 excerpts from classical piano pieces, played by different famous pianists. More specifically, listeners were asked to describe, using freely chosen words (preferably: adjectives), how they perceive the expressive character of the different performances. In this paper, we offer a first account of this new data resource for expressive performance research, and provide an exploratory analysis, addressing three main questions: (1) how similarly do different listeners describe a performance of a piece? (2) what are the main dimensions (or axes) for expressive character emerging from this?; and (3) how do measurable parameters of a performance (e.g., tempo, dynamics) and mid- and high-level features that can be predicted by machine learning models (e.g., articulation, arousal) relate to these expressive dimensions? The dataset that we publish along with this paper was enriched by adding hand-corrected score-to-performance alignments, as well as descriptive audio features such as tempo and dynamics curves.","affiliations":"Austrian Research Institute for Artificial Intelligence|JKU|Johannes Kepler University Linz|University of Tartu|Johannes Kepler University","authors":"Carlos Eduardo Cancino-Chac\u221a\u2265n|Silvan Peter|Shreyan Chowdhury|Anna Aljanaki|Gerhard Widmer","keywords":"Musical features and properties|Expression and performative aspects of music|MIR fundamentals and methodology|Lyrics and other textual data, web mining, and natural language processing|Musical affect, emotion, and mood","primary_author":"Carlos Eduardo Cancino-Chac\u221a\u2265n","primary_email":"carlos.cancino@ofai.at","session":"1A","title":"On the Characterization of Expressive Performance in Classical Music: First Results of the Con Espressione Game","type":"paper"},{"UID":"281","abstract":"Data-driven approaches to automatic drum transcription (ADT) are often limited to a predefined, small vocabulary of percussion instrument classes. Such models cannot recognize out-of-vocabulary classes nor are they able to adapt to finer-grained vocabularies. In this work, we address open vocabulary ADT by introducing few-shot learning to the task. We train a Prototypical Network on a synthetic dataset and evaluate the model on multiple real-world ADT datasets with polyphonic accompaniment. We show that, given just a handful of selected examples at inference time, we can match and in some cases outperform a state-of-the-art supervised ADT approach under a fixed vocabulary setting. At the same time, we show that our model can successfully generalize to finer-grained or extended vocabularies unseen during training, a scenario where supervised approaches cannot operate at all. We provide a detailed analysis of our experimental results, including a breakdown of performance by sound class and by polyphony.","affiliations":"NYU|Adobe Research|New York University|Adobe Research|New York University","authors":"Yu Wang|Justin Salamon|Mark Cartwright|Nicholas J. Bryan|Juan P Bello","keywords":"MIR tasks|Music transcription and annotation|Domain knowledge|Machine learning/Artificial intelligence for music","primary_author":"Yu Wang","primary_email":"wangyu@nyu.edu","session":"1A","title":"Few-shot Drum Transcription in Polyphonic Music","type":"paper"},{"UID":"300","abstract":"Representation learning focused on disentangling the underlying factors of variation in given data has become an important area of research in machine learning. However, most of the studies in this area have relied on datasets from the computer vision domain and thus, have not been readily extended to music. In this paper, we present a new symbolic music dataset that will help researchers working on disentanglement problems demonstrate the efficacy of their algorithms on diverse domains. This will also provide a means for evaluating algorithms specifically designed for music. To this end, we create a dataset comprising of 2-bar monophonic melodies where each melody is the result of a unique combination of nine latent factors that span ordinal, categorical, and binary types. The dataset is large enough (\u201a\u00e2\u00e0 1.3 million data points) to train and test deep networks for disentanglement learning. In addition, we present benchmarking experiments using popular unsupervised dis- entanglement algorithms on this dataset and compare the results with those obtained on an image-based dataset.","affiliations":"Georgia Institute of Technology|Georgia Institute of Technology|Georgia Tech Center for Music Technology","authors":"Ashis Pati|Siddharth Kumar Gururani|Alexander Lerch","keywords":"Domain knowledge|Machine learning/Artificial intelligence for music|Representations of music|Evaluation, datasets, and reproducibility|Novel datasets and use cases|Reproducibility|MIR fundamentals and methodology|Symbolic music processing","primary_author":"Ashis Pati","primary_email":"ashis.pati@gatech.edu","session":"1A","title":"dMelodies: A Music Dataset for Disentanglement Learning","type":"paper"},{"UID":"304","abstract":"Deep representation learning offers a powerful paradigm for mapping input data onto an organized embedding space and is useful for many music information retrieval tasks. Two central methods for representation learning include deep metric learning and classification, both having the same goal of learning a representation that can generalize well across tasks. Along with generalization, the emerging concept of disentangled representations is also of great interest, where multiple semantic concepts (e.g., genre, mood, instrumentation) are learned jointly but remain separable in the learned representation space. In this paper we present a single representation learning framework that elucidates the relationship between metric learning, classification, and disentanglement in a holistic manner. For this, we (1) outline past work on the relationship between metric learning and classification, (2) extend this relationship to multi-label data by exploring three different learning approaches and their disentangled versions, and (3) evaluate all models on four tasks (training time, similarity retrieval, auto-tagging, and triplet prediction). We find that classification-based models are generally advantageous for training time, similarity retrieval, and auto-tagging, while deep metric learning exhibits better performance for triplet-prediction. Finally, we show that our proposed approach yields state-of-the-art results for music auto-tagging.","affiliations":"KAIST|Adobe Research|Adobe Research|Adobe Research|KAIST","authors":"Jongpil Lee|Nicholas J. Bryan|Justin Salamon|Zeyu Jin|Juhan Nam","keywords":"MIR tasks|Similarity metrics|Automatic classification","primary_author":"Jongpil Lee","primary_email":"richter@kaist.ac.kr","session":"1A","title":"Metric learning vs classification for disentangled music representation learning","type":"paper"},{"UID":"307","abstract":"The assessment of music performances in most cases takes into account the underlying musical score being performed. While there have been several automatic approaches for objective music performance assessment (MPA) based on extracted features from both the performance audio and the score, deep neural network-based methods incorporating score information into MPA models have not yet been investigated. In this paper, we introduce three different models capable of score-informed performance assessment. These are (i) a convolutional neural network that utilizes a simple time-series input comprising of aligned pitch contours and score, (ii) a joint embedding model which learns a joint latent space for pitch contours and scores, and (iii) a distance matrix-based convolutional neural network which utilizes patterns in the distance matrix between pitch contours and musical score to predict assessment ratings. Our results provide insights into the suitability of different architectures and input representations and demonstrate the benefits of score-informed models as compared to score-independent models.","affiliations":"Georgia Institute of Technology|Georgia Institute of Technology|Georgia Institute of Technology|Georgia Institute of Technology|Georgia Tech Center for Music Technology","authors":"Jiawen Huang|Yun-Ning Hung|Ashis Pati|Siddharth Kumar Gururani|Alexander Lerch","keywords":"Musical features and properties|Expression and performative aspects of music|Applications|Music training and education|Domain knowledge|Machine learning/Artificial intelligence for music|MIR fundamentals and methodology|Multimodality","primary_author":"Jiawen Huang","primary_email":"jhuang448@gatech.edu","session":"1A","title":"Score-informed Networks for Music Performance Assessment","type":"paper"},{"UID":"308","abstract":"Despite the manifold developments in music emotion recognition and related areas, estimating the emotional impact of music still poses many challenges. These are often associated to the complexity of the acoustic codes to emotion and the lack of large amounts of data with robust golden standards. In this paper, we propose a new computational model (EmoMucs) that considers the role of different musical voices in the prediction of the emotions induced by music. We combine source separation algorithms for breaking up music signals into independent song elements (vocals, bass, drums, other) and end-to-end state-of-the-art machine learning techniques for feature extraction and emotion modelling (valence and arousal regression). Through a series of computational experiments on a benchmark dataset using source-specialised models trained independently and different fusion strategies, we demonstrate that EmoMucs outperforms state-of-the-art approaches with the advantage of providing insights into the relative contribution of different musical elements to the emotions perceived by listeners.","affiliations":"University of Manchester|University of Manchester|University of Liverpool","authors":"Jacopo de Berardinis|Angelo Cangelosi|Eduardo Coutinho","keywords":"Musical features and properties|Musical affect, emotion, and mood|Domain knowledge|Machine learning/Artificial intelligence for music|MIR tasks|Sound source separation","primary_author":"Jacopo de Berardinis","primary_email":"jacopo.deberardinis@postgrad.manchester.ac.uk","session":"1A","title":"The multiple voices of musical emotions: source separation for improving music emotion recognition models and their interpretability","type":"paper"},{"UID":"310","abstract":"We present a fast and high-fidelity method for music generation, based on specified f0 and loudness, such that the synthesized audio mimics the timbre and articulation of a target instrument. The generation process consists of learned source-filtering networks, which reconstruct the signal at increasing resolutions. The model optimizes a multi-resolution spectral loss as the reconstruction loss, an adversarial loss to make the audio sound more realistic, and a perceptual f0 loss to align the output to the desired input pitch contour. The proposed architecture enables high-quality fitting of an instrument, given a sample that can be as short as a few minutes, and the method demonstrates state-of-the-art timbre transfer capabilities. Code and audio samples are shared at https://github.com/mosheman5/timbre_painting.","affiliations":"Tel Aviv University|Tel Aviv University, Israel","authors":"Michael M Michelashvili|Lior Wolf","keywords":"Domain knowledge|Machine learning/Artificial intelligence for music|Representations of music|MIR fundamentals and methodology|Music signal processing|MIR tasks|Music synthesis and transformation","primary_author":"Michael M Michelashvili","primary_email":"mosheman5@gmail.com","session":"1A","title":"Hierarchical Timbre-painting and Articulation Generation","type":"paper"},{"UID":"311","abstract":"While many researchers have proposed various ways of quantifying recommendation list diversity, these approaches have had little input from users on their own perceptions and preferences in seeking diversity. Through an exploratory user study, we provide a better understanding of how users view the concept of diversity in music recommendations, and how they might optimise levels of intra-list diversity themselves. In our study, 17 participants interacted with and rated the suggestions from two different recommendation systems. One provided static top-7 collaborative filtering recommendations, and the other provided an interactive slider to re-rank these recommendations based on a continuous diversity scale. We also asked participants a series of free-form questions on music discovery and diversity in semi-structured interviews. User-preferred levels of diversity varied widely both within and between subjects. Although most users agreed that diversity is beneficial in music discovery, they also noted a risk of dissatisfaction from too much diversity. A key finding is that preference for diversification was often linked to user mood. Participants also expressed a clear distinction between diversity within existing preferences, and outside of existing preferences. These ideas of inner and outer diversity are not well defined within the bounds of current diversity metrics, and we discuss their implications.","affiliations":"University of Waterloo|University of Waterloo|Johannes Kepler University Linz","authors":"Kyle Robinson|Dan Brown|Markus Schedl","keywords":"Human-centered MIR|User-centered evaluation|Applications|Music recommendation and playlist generation|Evaluation, datasets, and reproducibility|Evaluation metrics|Human-computer interaction and interfaces|Personalization","primary_author":"Kyle Robinson","primary_email":"kyle.robinson@uwaterloo.ca","session":"1A","title":"USER INSIGHTS ON DIVERSITY IN MUSIC RECOMMENDATION LISTS","type":"paper"},{"UID":"323","abstract":"This paper presents exploratory work investigating the suitability of the Music Ontology - the most widely used formal specification of the music domain - for modelling non-Western musical traditions. Four contrasting case studies from a variety of musical cultures are analysed: Dutch folk song research, reconstructive performance of rural Russian traditions, contemporary performance and composition of Persian classical music, and recreational use of a personal world music collection. We propose semantic models describing the respective do- mains and examine the applications of the Music Ontology for these case studies: which concepts can be successfully reused, where they need adjustments, and which parts of the reality in these case studies are not covered by the Mu- sic Ontology. The variety of traditions, contexts and modelling goals covered by our case studies sheds light on the generality of the Music Ontology and on the limits of generalisation \u201a\u00c4\u00fafor all musics\u201a\u00c4\u00f9 that could be aspired for on the Semantic Web.","affiliations":"Queen Mary University of London|Utrecht University|University of Waikato|Queen Mary University of London","authors":"Polina Proutskova|Anja Volk|Peyman Heidarian|Gyorgy Fazekas","keywords":"MIR fundamentals and methodology|Metadata, tags, linked data, and semantic web|Applications|Digital libraries and archives|Domain knowledge|Representations of music|Evaluation, datasets, and reproducibility|Novel datasets and use cases|Philosophical and ethical discussions|Philosophical and methodological foundations","primary_author":"Polina Proutskova","primary_email":"proutskova@googlemail.com","session":"1A","title":"From Music Ontology towards Ethno-Music-Ontology","type":"paper"},{"UID":"324","abstract":"Labeling a music recording according to its genre is an intuitive and familiar way to describe its content. Music genres are valuable information, especially for music organization, personalized listening experience, and playlist generation. Automatically classifying music genres is a challenging endeavor due to the inherent ambiguity and subjectivity. Most efforts on music genre classification consider the complete independence between labels. However, music genres typically respect a hierarchical structure based on the influences or origins of each style. Conversely, many of the methods available for hierarchical classification are based on assumptions about the class hierarchy, such as the need for multiple children in each hierarchy's node, which may limit their use in music applications. Also, the local classifier per node approach that would be the most suitable for this scenario is costly regarding time and memory. In this paper, we present two local hierarchical classification approaches and show how to combine them to obtain a single one that is more robust and faithful to the music genre classification scenario. We evaluate our proposal in a music dataset hierarchically labeled with 120 music genres. As shown, compared to state-of-the-art approaches, our approach has a lower computational cost and can achieve competitive performances.","affiliations":"Universidade de S\u221a\u00a3o Paulo|Universidade Federal de S\u221a\u00a3o Carlos|","authors":"Antonio R. Parmezan|Diego Furtado Silva|Gustavo Batista","keywords":"MIR tasks|Automatic classification|Domain knowledge|Machine learning/Artificial intelligence for music|MIR fundamentals and methodology|Music signal processing|Musical features and properties|Musical style and genre","primary_author":"Antonio R. Parmezan","primary_email":"antoniorafaelparmezan@gmail.com","session":"1A","title":"A Combination of Local Approaches for Hierarchical Music Genre Classification","type":"paper"},{"UID":"326","abstract":"EarSketch is an online environment for learning introductory computing concepts through code-driven, sample-based music production. This paper details the design and implementation of a module to perform code and music analyses on projects on the EarSketch platform. This analysis module combines inputs in the form of symbolic metadata, audio feature analysis, and user code to produce comprehensive models of user projects. The module performs a detailed analysis of the abstract syntax tree of a user's code to model use of computational concepts. It uses music information retrieval (MIR) and symbolic metadata to analyze users' musical design choices. These analyses produce a model containing users' coding and musical decisions, as well as qualities of the algorithmic music created by those decisions. The models produced by this module will support future development of CAI, a Co-creative Artificial Intelligence. CAI is designed to collaborate with learners and promote increased competency and engagement with topics in the EarSketch curriculum. Our module combines code analysis and MIR to further the educational goals of CAI and EarSketch and to explore the application of multimodal analysis tools to education.","affiliations":"Georgia Tech|Georgia Tech|Georgia Institute of Technology|Georgia Institute of Technology|University of Florida|The Findings Group","authors":"Jason Smith|Erin Truesdell|Jason Freeman|Brian Magerko|Kristy Boyer|Tom Mcklin","keywords":"Applications|Music training and education|Domain knowledge|Representations of music|Human-centered MIR|User behavior analysis and mining, user modeling|MIR fundamentals and methodology|Multimodality|Symbolic music processing|Musical features and properties|Structure, segmentation, and form","primary_author":"Jason Smith","primary_email":"jas1238smith@gmail.com","session":"1A","title":"Modeling Music and Code Knowledge to Support a Co-creative AI Agent for Education","type":"paper"},{"UID":"334","abstract":"Music source separation is a core task in music information retrieval which has seen a dramatic improvement in the past years. Nevertheless, most of the existing systems focus exclusively on the problem of source separation itself and ignore the utilization of other~---possibly related---~MIR tasks which could lead to additional quality gains. In this work, we propose a novel multitask structure to investigate using instrument activation information to improve source separation performance. Furthermore, we investigate our system on six independent instruments, a more realistic scenario than the three instruments included in the widely-used MUSDB dataset, by leveraging a combination of the MedleyDB and Mixing Secrets datasets. The results show that our proposed multitask model outperforms the baseline Open-Unmix model on the mixture of Mixing Secrets and MedleyDB dataset while maintaining comparable performance on the MUSDB dataset.","affiliations":"Georgia Tech|Georgia Tech Center for Music Technology","authors":"Yun-Ning Hung|Alexander Lerch","keywords":"MIR tasks|Sound source separation|Domain knowledge|Machine learning/Artificial intelligence for music|Evaluation, datasets, and reproducibility|Novel datasets and use cases|Musical features and properties|Timbre, instrumentation, and voice","primary_author":"Yun-Ning Hung","primary_email":"yhung33@gatech.edu","session":"1A","title":"MULTITASK LEARNING FOR INSTRUMENT ACTIVATION AWARE MUSIC SOURCE SEPARATION","type":"paper"},{"UID":"339","abstract":"This paper presents the Jazz Transformer, a generative model that utilizes a neural sequence model called the Transformer-XL for modeling lead sheets of Jazz music. Moreover, the model endeavors to incorporate structural events present in the Weimar Jazz Database (WJazzD) for inducing structures in the generated music. While we are able to reduce the training loss to a low value, our listening test suggests however a clear gap between the average ratings of the generated and real compositions. We therefore go one step further and conduct a series of computational analysis of the generated compositions from different perspectives. This includes analyzing the statistics of the pitch class, grooving, and chord progression, assessing the structureness of the music with the help of the fitness scape plot, and evaluating the model's understanding of Jazz music through a MIREX-like continuation prediction task. Our work presents in an analytical manner why machine-generated music to date still falls short of the artwork of humanity, and sets some goals for future work on automatic composition to further pursue.","affiliations":"National Taiwan University|Academia Sinica","authors":"Shih-Lun Wu|Yi-Hsuan Yang","keywords":"Evaluation, datasets, and reproducibility|Evaluation metrics|Applications|Music composition, performance, and production|Domain knowledge|Machine learning/Artificial intelligence for music","primary_author":"Shih-Lun Wu","primary_email":"b06902080@csie.ntu.edu.tw","session":"1A","title":"The Jazz Transformer on the Front Line: Exploring the Shortcomings of AI-composed Music through Quantitative Measures","type":"paper"},{"UID":"341","abstract":"Recent advances in polyphonic piano transcription have been made primarily by a deliberate design of neural network architectures that detect different note states such as onset or sustain and model the temporal evolution of the states. The majority of them, however, use separate neural networks for each note state, thereby optimizing multiple loss functions, and also they handle the temporal evolution of note states by abstract connections between the state-wise neural networks or using a post-processing module. In this paper, we propose a unified neural network architecture where multiple note states are predicted as a softmax output with a single loss function and the temporal order is learned by an auto-regressive connection within the single neural network. This compact model allows to increase note states without architectural complexity. Using the MAESTRO dataset, we examine various combinations of multiple note states including on, onset, sustain, re-onset, offset, and off. We also show that the autoregressive module effectively learns inter-state dependency of notes. Finally, we show that our proposed model achieves performance comparable to state-of-the-arts with fewer parameters. ","affiliations":"KAIST|SK Telecom|KAIST","authors":"Taegyun Kwon|Dasaem Jeong|Juhan Nam","keywords":"MIR tasks|Music transcription and annotation|Applications|Music composition, performance, and production|Music retrieval systems|MIR fundamentals and methodology|Music signal processing","primary_author":"Taegyun Kwon","primary_email":"ilcobo2@kaist.ac.kr","session":"1A","title":"Polyphonic Piano Transcription Using Autoregressive Multi-State Note Model","type":"paper"},{"UID":"343","abstract":"Audio alignment is a fundamental preprocessing step in many MIR pipelines. For two audio clips with M and N frames, respectively, the most popular approach, dynamic time warping (DTW), has O(MN) requirements in both memory and computation, which is prohibitive for frame-level alignments at reasonable rates. To address this, a variety of memory efficient algorithms exist to approximate the optimal alignment under the DTW cost. To our knowledge, however, no exact algorithms exist that are guaranteed to break the quadratic memory barrier.  In this work, we present a divide and conquer algorithm that computes the exact globally optimal DTW alignment using O(M+N) memory. Its runtime is still O(MN), trading off memory for a 2x increase in computation.  However, the algorithm can be parallelized up to a factor of min(M, N) with the same memory constraints, so it can still run more efficiently than the textbook version with an adequate GPU. We use our algorithm to compute exact alignments on a collection of orchestral music, which we use as ground truth to benchmark the alignment accuracy of several popular approximate alignment schemes at scales that were not previously possible.","affiliations":"Ursinus College|Ursinus College","authors":"Christopher J Tralie|Elizabeth Dempsey","keywords":"MIR tasks|Alignment, synchronization, and score following|Applications|Music retrieval systems|Evaluation, datasets, and reproducibility|Evaluation metrics","primary_author":"Christopher J Tralie","primary_email":"ctralie@alumni.princeton.edu","session":"1A","title":"Exact, Parallelizable Dynamic Time Warping Alignment with Linear Memory","type":"paper"},{"UID":"349","abstract":"Recent years have witnessed great progress in using deep learning algorithms to learn to compose music in the form of a MIDI file.  However, whether such algorithms apply equally well to compose guitar tabs, which are quite different from MIDIs, remain relatively unexplored. To address this, we build a model for composing fingerstyle guitar tabs with a neural sequence model architecture called the Transformer-XL. With this model, we investigate the following research questions. First, whether the neural net generates note sequences with meaningful fingering (i.e., string-fret combinations), which is important for tabs but not for MIDIs. Second, whether it generates compositions with coherent rhythmic grooving, which is crucial for fingerstyle guitar music. And, finally, how pleasant the composed music is in comparison to real, human-made compositions. Our work provides preliminary empirical evidence for the promise of deep learning for guitar tab composition, and suggests areas for future study.","affiliations":"National Taiwan University|National Taiwan University|Taiwan AI Labs|Academia Sinica","authors":"Yu-Hua Chen|Yu-Siang Huang|Wen-Yi Hsiao|Yi-Hsuan Yang","keywords":"Applications|Music composition, performance, and production|Domain knowledge|Machine learning/Artificial intelligence for music|Representations of music|MIR fundamentals and methodology|Symbolic music processing|MIR tasks|Music synthesis and transformation|Musical features and properties|Rhythm, beat, tempo","primary_author":"Yu-Hua Chen","primary_email":"r08946011@ntu.edu.tw","session":"1A","title":"Automatic Composition of Guitar Tabs by Transformers and Groove Modeling","type":"paper"},{"UID":"352","abstract":"A DJ mix is a sequence of music tracks concatenated seamlessly, typically rendered for audiences in a live setting by a DJ on stage. As a DJ mix is produced in a studio or the live version is recorded for music streaming services, computational methods to analyze DJ mixes, for example, extracting track information or understanding DJ techniques, have drawn research interests. Many of previous works are, however, limited to identifying individual tracks in a mix or segmenting it, and the sizes of the datasets are usually small. In this paper, we provide an in-depth analysis of DJ music by aligning a mix to its original music tracks. We set up the subsequence alignment such that the audio features are less sensitive to the tempo or key change of the original track in a mix. This approach provides temporally tight mix-to-track matching from which we can obtain cue-points, transition length, mix segmentation, and musical changes in DJ performance. Using 1,557 mixes from 1001Tracklists including 13,728 tracks and 20,765 transitions, we conduct the proposed analysis and show a wide range of statistics, which may elucidate the creative process of DJ music making.","affiliations":"KAIST|KAIST|1001Tracklists|Academia Sinica|KAIST","authors":"Taejun Kim|Minsuk Choi|Evan Sacks|Yi-Hsuan Yang|Juhan Nam","keywords":"Applications|Music composition, performance, and production|Evaluation, datasets, and reproducibility|Novel datasets and use cases|MIR tasks|Alignment, synchronization, and score following|Musical features and properties|Structure, segmentation, and form","primary_author":"Taejun Kim","primary_email":"taejun@kaist.ac.kr","session":"1A","title":"A Computational Analysis of Real-World DJ Mixes using Mix-To-Track Subsequence Alignment","type":"paper"},{"UID":"354","abstract":"Dynamic prediction of perceived emotions of music is a challenging problem with interesting applications. Utilization of relevant context in audio sequence is essential for effective prediction. Existing methods have used LSTMs with modest success. In this work we describe three attentive LSTM based approaches for dynamic emotion prediction from music clips. We validate our models through extensive experimentation on standard dataset annotated with arousal-valence values in continuous time, and choose the best performer. We find that the LSTM based attention models perform better than the state of the art transformers for the dynamic emotion prediction task, both in terms of R2 and Kendall-Tau metrics. We explore individual smaller feature sets in search of a more effective one and to understand how different features contribute to perceived emotion. The spectral features are found to perform at par with the generic ComPare feature set [1]. Through attention map analysis we visualize how attention is distributed over music clips\u201a\u00c4\u00f4 frames for emotion prediction. It is observed that the models attend to frames which contribute to changes in reported arousal-valence values and chroma to produce better emotion predictions, effectively capturing long-term dependencies.","affiliations":"Indian Institute of Technology, Kharagpur, India|IIT Kharagpur|Indian Institute of Technology Kharagpur|Indian Institute of Technology Kharagpur","authors":"Sanga Chaki|Pranjal Doshi|Sourangshu Bhattacharya|Prof. Priyadarshi Patnaik","keywords":"Musical features and properties|Musical affect, emotion, and mood|Applications|Music recommendation and playlist generation|Music retrieval systems|Domain knowledge|Machine learning/Artificial intelligence for music|MIR tasks|Automatic classification|Pattern matching and detection","primary_author":"Sanga Chaki","primary_email":"s.chaki27@gmail.com","session":"1A","title":"Explaining Perceived Emotion Predictions in Music: An Attentive Approach","type":"paper"},{"UID":"359","abstract":"This paper develops the hypothesis that symbolic drum patterns can be represented in a reduced form as a simple oscillation between two states, a Low state (commonly associated with kick drum events) and a High state (often associated with either snare drum or high hat). Both an onset time and an accent time is associated to each state. The systematic inference of the reduced form is formalized. This enables the specification of a rhythmic structural similarity measure on drum patterns, where reduced patterns are compared through alignment. The two-state representation allows a low computational cost alignment, once the complex topological formalization is fully taken into account. A comparison with the Hamming distance, as well as similarity ratings collected from listeners on a drum loop dataset, indicates that the bistate reduction enables to convey subtle aspects that goes beyond surface-level comparison of rhythmic textures. ","affiliations":"RITMO, University of Oslo|Queen Mary University of London / Roli","authors":"Olivier Lartillot|Fred Bruford","keywords":"Musical features and properties|Rhythm, beat, tempo|Domain knowledge|Computational music theory and musicology|Representations of music|MIR fundamentals and methodology|Symbolic music processing|MIR tasks|Similarity metrics","primary_author":"Olivier Lartillot","primary_email":"olivier.lartillot@imv.uio.no","session":"1A","title":"Bistate reduction and comparison of drum patterns","type":"paper"},{"UID":"362","abstract":"The COVID-19 pandemic causes a massive global health crisis and produces substantial economic and social distress, which in turn may cause stress and anxiety among people. Real-world events play a key role in shaping collective sentiment in a society. As people listen to music daily everywhere in the world, the sentiment of music being listened to can reflect the mood of the listeners and serve as a measure of collective sentiment. However, the exact relationship between real-world events and the sentiment of music being listened to is not clear. Driven by this research gap, we use the unexpected outbreak of COVID-19 as a natural experiment to explore how users' sentiment of music being listened to evolves before and during the outbreak of the pandemic. We employ causal inference approaches on an extended version of the LFM-1b dataset of listening events shared on Last.fm, to examine the impact of the pandemic on the sentiment of music listened to by users in different countries. We find that, after the first COVID-19 case in a country was confirmed, the sentiment of artists users listened to becomes more negative. This negative effect is pronounced for males while females' music emotion is less influenced by the outbreak of the COVID-19 pandemic. We further find a negative association between the number of new weekly COVID-19 cases and users' music sentiment. Our results provide empirical evidence that public sentiment can be monitored based on collective music listening behaviors, which can contribute to research in related disciplines.","affiliations":"The University of Hong Kong|University of Innsbruck|The University of Hong Kong|Johannes Kepler University Linz|Johannes Kepler University Linz","authors":"Meijun Liu|Eva Zangerle|Xiao Hu|Alessandro Melchiorre|Markus Schedl","keywords":"Applications|Music retrieval systems|Music and health, well-being, and therapy|Music recommendation and playlist generation|Musical features and properties|Musical affect, emotion, and mood","primary_author":"Meijun Liu","primary_email":"meijun@hku.hk","session":"1A","title":"Pandemics, music, and collective sentiment: evidence from the outbreak of COVID-19","type":"paper"},{"UID":"368","abstract":"Temporality lies at the very heart of music, and the play with rhythmic and metrical structures constitutes a major device across musical styles and genres. Rhythmic and metrical structure are closely intertwined, particularly in the tonal idiom. While there have been many approaches for modeling musical tempo, beat and meter and their inference, musical rhythm and its complexity have been comparably less explored and formally modeled. The model formulates a generative grammar of symbolic rhythmic musical structure and its internal recursive substructure. The approach characterizes rhythmic groups in alignment with meter in terms of the recursive subdivision of temporal units, as well as dependencies established by recursive operations such as preparation and different kinds of shifting (such as anticipation and delay). The model is formulated in terms of an abstract context-free grammar and applies for monophonic rhythms and harmonic rhythm. ","affiliations":"Ecole Polytechnique F\u221a\u00a9d\u221a\u00a9rale de Lausanne","authors":"Martin Rohrmeier","keywords":"Musical features and properties|Rhythm, beat, tempo|MIR fundamentals and methodology|Symbolic music processing|Philosophical and ethical discussions|Philosophical and methodological foundations","primary_author":"Martin Rohrmeier","primary_email":"martin.rohrmeier@epfl.ch","session":"1A","title":"Towards a Formalization of Musical Rhythm","type":"paper"}]
