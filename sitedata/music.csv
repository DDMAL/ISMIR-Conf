UID,title,abstract,first_name,last_name,affiliation,bio,web_link,session,yt_id,bb_id,db_link,authors,still_image,release_consent
389,A Room with Chaconne (Bach),"This work is a real-time, audio-driven system for augmented violin performance. Its paradigm is spatial (hence the title, “A Room with…”) rather than instrumental. State changes interpolate between unique parameter configurations of the space composed through trial-and-error experimentation using themes/variations from J. S. Bach’s Chaconne (Partita #2). Bespoke textural and ambient microsonic processing of the violin’s sound by the composer is driven primarily by pitch estimation and other spectral features. In the sound design, Tim Hecker, William Basinski, Vangelis, Gas, and Klaus Schulze are all points of reference, and the ethos of the work is informed by the references of the latter two to late German Romantic music. The intention with performing this system using a famous piece from the violin repertoire (rather than free improvisation, the typical modus operandi of the composer) is to demonstrate to classically-trained musicians how they might discover novelty with the use of elaborate spatial augmentation systems as demonstrated by this performance. This seems especially important at the present moment, when musicians are sidelined from ensembles and seeking new ways to extend, rediscover, or reinvent their solo playing.",Seth,Thorn,Arizona State University,"Dr. Seth Thorn is an American violinist whose research encompasses interaction design and philosophical approaches to computational media. He has published in premier journals and top-tier conferences spanning HCI, music, and critical theory, including Leonardo Music Journal, Qui Parle (UC Berkeley), ACM Tangible, Embedded and Embodied Interaction, ACM Creativity & Cognition, ACM Movement & Computing, NIME, and ICMC; presented technology at the Guthman Competition at Georgia Institute of Technology; and performed at the New York City Electroacoustic Musical Festival (NYCEMF), the New York City Electroacoustic Improvisation Summit (NYCEIS), and other events. Seth has worked with members of the East-West Divan Orchestra and world-renowned conductor Daniel Barenboim. He holds a patent for haptic technology for augmented violin and leads a startup company he runs with two other professors at Arizona State University. He earned a PhD at Brown University in 2018 in Computer Music and Multimedia, has additional graduate degrees in political theory and German studies, and held a Fulbright research fellowship in Germany to study philosophy and music/media. He currently teaches at the School of Arts, Media + Engineering at Arizona State University.",http://www.seththorn.net,1,JOm1zXYyme8,BV1mz4y1Z7pq,https://www.dropbox.com/s/aqzufyzr7hn351w/ISMIR-389.mp4?dl=0,,,1
392,DigiTral,"DigiTral by Matheos & Georgios is conceptualized by the transformation of the natural society to the digitalization of human life. Presently, we are at a middle point that is departing an organic living to a digitally assisted one in which both composers were capable of collaboration from across the world. There is a delicate intrusion from the immersive and digital technologies which comforts our journey into this new uncharted world. Matheos composed using the assistance of machine learning to inform his decisions and digitally generated sounds. Georgios composed with his intuition adding from his personal archive of field recordings from around the world. This piece presents a sonic experience with the interpretation of natural and digital sounds that are in a current state of suspension. With each passing day, our human journey is challenged on new frontiers as we slowly all become digital beings.",Artist group: Matheos & Georgios,Full names: Matheos Zaharopoulos | Georgios Varoutsos,Matheos: Concordia University | Georgios: Queen's University Belfast,"Matheos & Georgios is a partnership composed of two life-long friends who have multiple interests in sound design, acousmatic composition, and sonic arts. Through their studies in Electroacoustics at Concordia University (Montreal, Canada), they have acquired a different set of interests and passions. This includes avenues of exploration into sound research, performance, media manipulation, recording, sound practices, and technology. By merging their skills, they have ventured into projects connected and concerned with history, culture, society, and art history. Whether that is in the construction or compositions of their projects, they focus on intertwining old with new.

As a team, they view their art as a medium to present real-life issues or topics that concern the masses and areas of human experiences. Utilizing their friendship alongside their mixed media team provides an ability to venture into large-scale projects and investigate issues relatable to each other or as a whole.",https://matheos-georgios.com/,1,jk9pZFZmV08,BV1uA41177ZW,https://www.dropbox.com/s/ooyym4tdgo8clin/ISMIR-392.mp4?dl=0,,https://drive.google.com/open?id=1zy9LQnOWCpbTcB_h7Xkqu9LJ7oOTrscs,0
393,(Collaborative Electroacoustic Composition with Intelligent Agents),"CECIA is an innovative music project that integrates the creative agency of 5 composers and Machine Learning algorithms, leading to the creation of a unique composition of electroacoustic music. The project explores collaborative music creation, harnessing the creativity of electroacoustic music composers and Intelligent Agents through an online platform. The collaborative process was conducted remotely in an iterative fashion, in which the composers anonymously submitted and evaluated sound material/ideas/suggestions. These data were used for the training of the machine learning algorithms, which generated new sonic structures, which in turn were fed back to the composers as suggestive material. The project implements a synergistic framework between human and algorithms, introducing a novel experimental sound practice for the creation of  electroacoustic music.",CECIA team ,"Panayiotis Kokoras, Mariam Gviniashvili, Juan Carlos Vasquez, Martyna Kosecka, Erik Nyström, Artemi – Maria Gioti, Kosmas Giannoutakis",ZKM | Center for Art and Media,"The CECIA team is composed of internationally active composers, sound artists and researchers who remotely worked on the project in 2019. The project was organized by ZKM within the framework of the »Interfaces« project with the support of the Creative Europe program of the European Union. ",,1,yLUDwNQo-jQ,BV1Th41197r5,https://www.dropbox.com/s/0skxfknmeg62czs/ISMIR-393.mp4?dl=0,,https://drive.google.com/open?id=1KPg39P0mZPfLl_DwxIMBzCLWbp16j69E,1
405,Mosaicing,"Mosaicing is a sound composition for flute and fixed electronics. The piece establishes a sound ecosys-tem full of energetic gestures with accents and harmonic sweeps exploring both the intrinsic qualities but also the contextual potential of the sound material. There are moments where the heavy breathy flute sound evolves, almost instantly, into Gustav Holst’s strings continuum to a lion’s roar; fast gestural flute passages are competing with zip sounds and angry dogs under the pulses of Stravinsky’s Rite of Spring or Stockhausen’s meditative vocals of Stimmung. The title Mosaicing refers to the process of re-composing the temporal evolution of a given flute audio file from segments cut out of selected audio materials using MIR tools.",Panayiotis,Kokoras,University of North Texas,"Kokoras is an internationally award-winning composer and computer music innovator, and currently an Associate Professor of composition and CEMI director (Center for Experimental Music and Intermedia) at the University of North Texas. Born in Greece, he studied classical guitar and composition in Athens, Greece and York, England; he taught for many years at Aristotle University in Thessaloniki. Kokoras's sound compositions use sound as the only structural unit. His concept of ""holophonic musical texture"" describes his goal that each independent sound (phonos), contributes equally into the synthesis of the total (holos). In both instrumental and electroacoustic writing, his music calls upon a ""virtuosity of sound,"" a hyper-idiomatic writing which emphasizes on the precise production of variable sound possibilities and the correct distinction between one timbre and another to convey the musical ideas and structure of the piece. His compositional output is also informed by musical research in Music Information Retrieval compositional strategies, Extended techniques, Tactile sound, Hyperidiomaticity, Robotics, Sound and Consciousness. ",http://www.panayiotiskokoras.com,1,ca6zR-S-OOU,BV1D54y1171D,https://www.dropbox.com/s/fzmq6sow1wh2vbq/ISMIR-405.mp4?dl=0,,https://drive.google.com/open?id=1IZBSR09MTWJy5Rkgck5IaMLSLxMTfZxU,1
410,Spectre (for processed solo voice) ,"Spectre is a piece for processed solo voice using Vocal PerFormants (VPF), a custom-built software performance system that uses vocal feature extraction and spectral analysis for intelligent voice processing.  With its vast timbral range, the human voice provides an immense world of possibilities in electroacoustic music. Despite the rich history of live vocal feature extraction, existing work frequently puts more focus on complex vocal sounds and textures, while engaging very little with the core fundamentals of standard vocalization – namely, the five primary singing vowels. Hence, VPF uses Max 8’s spectral descriptors libraries in conjunction with Wekinator’s machine learning models to train the system to detect one of the five core vowels (“ooh”, “ee”, “eh”, “ah”, or “oh”). The predicted vowel, along with the raw audio features themselves, are then used to control various parameters of live input processing, including harmonization, delay time (with feedback), and stereo spread.  Using this performance system, Spectre was composed as an improvisatory, experimental vocal work showcasing the hauntingly compelling sound worlds that can be created using Vocal PerFormants. The work largely aims to highlight and juxtapose VPF’s response to the core vowels in relation to more “complex” vocal sounds such as diphthongs, vocal fry, and unpitched breathy sounds. It also serves as a way to engage with a notion of aestheticizing “errors” in classification. ",Max,Addae,Oberlin College & Conservatory,"Max Addae (he/him, b. 1998) is a composer, vocalist, arranger, and creative programmer from Bloomfield, New Jersey. He is entering his 5th year at Oberlin College & Conservatory as a Double Degree student in Computer Science and Technology in Music and Related Arts (TIMARA), studying with Dr. Eli Stine. Max’s work often explores the intersection of computer programming, human-computer interaction, and electroacoustic performance/composition — he sees technology and software as an immensely powerful tool for extending the ways musicians create, teach, and interact with music and sound. 

Given his extensive background in vocal music, his work often explores the area of experimental vocal electroacoustic performance through custom-built software. He was recently selected as the 2020 Presser Scholar recipient by Oberlin Conservatory, and his original work and sound design has been featured in the Danenberg Honors Recital, the Collaborative SōSI festival, the Society of Composers (SCI) Summer Student Mixtape, and various Oberlin theatre productions. ",http://www.maxaddae.com/,1,LECvBC-z3k0,BV1CT4y1c7bu,https://www.dropbox.com/s/udvg5emzeo3ro3j/ISMIR-410.mp4?dl=0,,,1
411,Super Colliders,"Super Colliders for three pitched instruments and a computer is an audiovisual gamified composition, in which the musicians must perform particular sounds to control their avatars which should be collided with moving blobs representing musical notes on an animated scrolling score projected on a screen. The game system of the piece features a second-order Markov chain algorithm that tailors the level of challenges posed by the computer to the performers’ competence. The algorithm learns the avatars’ behaviours in real-time during the performance, then it generates mimic behaviors of the moving squares. The optimization process aims to engender the situation of a close battle between the musicians and the computer which contributes to achieving an immersive gameplay experience for the musicians.",Takuto,Fukuda,"McGill University, CIRMMT","Takuto Fukuda(b.1984/Japan) is a composer, sound artist and gestural controller performer. He has been exploring non-existent performances that engenders the sense of authentic performances in the age of technologically extended reality. Focusing on the vestige of performers' embodied intentionality in live concerts today, he approaches them through, among others, gestural control of music, mixed reality composition, interactive audiovisual installations and Game Pieces - compositions whose course is determined in real-time according to rules, chance operations and competitive strifes between performing opponents toward a goal.

His pieces have been prized at several competitions such as Andrew Svoboda Memorial Prize(Canada), CCMC 2011(Japan), WOCMAT 2013(Taiwan) and Musica Nova 2010(Czech), selected for performance at numerous music festivals in Europe, Asia, North and South America such as Ars Electronica(Austria), ISCM World Music Days 2016(Korea) and ICMCs(Slovenia, Greece, USA), and performed at prestigious institutes such as IRCAM(France), ZKM(Germany), ina-GRM(France) and CCRMA(USA).

He has been pursuing his D.Mus in composition at McGill University in Canada with Prof. Sean Ferguson, thanks to a FRQSC Doctoral Research Scholarship.",https://takutofukuda.webs.com/,1,LZYafVnRzmk,BV1UT4y1c7Br,https://www.dropbox.com/s/yy2t3hvx8hnvtku/ISMIR-411.mp4?dl=0,,,1
416,"I'll Marry You, Punk Come","""Lying dead on the floor.. No head rest assured"" It didn’t exactly win the popular vote, but our runner-up submission to the 2020 Eurovision AI Song Competition was the judge’s favorite for best creative use of AI. We used 7 neural net models, including Wave-U-Net, SampleRNN, Coconet, MusicAutobot, and GPT2, to generate lyrics, midi, and audio. These elements were curated and combined in an Ableton session as part of a music production workflow. Story: Human girl and AI boy fall in love, while a ruthless virulent monster wrecks their lives. Pop heats into rave, mutates smoothly into death metal, surrenders, and lulls gently back to pop. Honestly, AI music production feels natural to us. Compared to the tedious bricklaying of sequencers, it is an adventurous hunt and gather. There is no less creativity. Producers can use their aesthetic taste to curate collages of machine dada and audio floatsam into works of art. We think these tools will just become part of normal music production.",CJ,Carr,Dadabots X Portrait XO,"Originally from the US, Dadabots and Portrait XO joined forces at the Factory Berlin artist residency to compete in the Eurovision 2020 AI Song Contest. 

Rania Kim, aka Portrait XO, is a singer and multi-media artist. She is the founder of Sound Obsessed, an arts collective featuring musicians working with emerging tech. 

Dadabots is the music-hacker duo of CJ Carr and Zack Zukowski. They are widely known for their 24/7 death metal generator Relentless Doppelganger livestreaming on YouTube (it's still going!). Zack manages the machine learning team at Pex, an audio-video search engine. CJ has competed in 65+ hackathons which is way too many all-nighters drinking tea and coding. Together they share an absurd sense of humor, a love for underground music, and play no less than 5 instruments. They publish research on raw audio neural nets, but their main focus is collaborating with artists they admire.",http://dadabots.com,1,RKxeAAhG-Rw,,https://www.dropbox.com/s/ullclv2ao74z242/ISMIR-416.mp4?dl=0,CJ Carr; Zack Zukowski; Rania Kim ,,0
376,bell / boom,"Network modulation synthesis is a recent framework aimed at improving the usability and creative potential of autoencoders that produce musical audio. As a whole, the framework offers algorithms to improve parameter tweaking, create time-variant audio from non-autoregressive models, and synthesize multiple channels of monophonic audio linked by a shared audio target. This composition, bell / boom, heavily emphasizes the third use case. A complex synthesis tree of five channels, each using alterations of the network modulation synthesis algorithm, is used to create groups of audio samples that differ in timbre but are linked by their connection to the root audio. These audio groups, along with other standalone samples created using network modulation, are played back using a custom Max/MSP patch and a MIDI controller. The resulting composition features rich audio textures and allows for sequence and timing improvisation from the performer, while all audio in the piece was generated using algorithmic methods and the CANNe autoencoder for musical synthesis. Technical details of the network modulation synthesis algorithm can be found in the author’s upcoming ICMC paper.",Jeremy,Hyrkas,University of California San Diego,"Jeremy Hyrkas is a music researcher focused on generative models for composition. He is particularly interested in models that directly create audio and methods for musicians to more easily use statistical models in their creative process. His research has been presented at ICMC and ISMIR and he has performed as part of the Computer Music Ensemble at NYU at the Interactive Performance Art Series. Jeremy holds computer science degrees from the University of Washington and Colorado State University, and previously worked at Microsoft and Google. Combining his technical background with a lifelong passion for music composition and performance, Jeremy earned a Master of Music degree from NYU Steinhardt’s Music Technology program, and now joins the University of California San Diego’s Music department as a PhD student in Computer Music.",https://www.jeremyhyrkas.com/About,2,KTbNj_ywvyQ,BV1Yk4y1C75Q,https://www.dropbox.com/s/1rq9e1e5iauwmi7/ISMIR-376.mp4?dl=0,,,0
377,Blue Sky Catastrophe,"Blue Sky Catastrophe for piano, live computer processing, and 8 channel sound, is based on performer-driven generative musical processes. Musical phrases are algorithmically generated live by the computer and the performer is asked to sight read them. The computer then assesses the performance and generates the next phrase of music for the pianist – either more or less difficult based on the pianist’s performance. The pianist is asked to read the music accurately while also being given latitude to influence the computer’s musical decisions and pace. Therefore, the score of the work is entirely performer-driven, different in every performance. The computer is set up to react not only to accuracy but also to aspects of the playing that will control live generated electronics such as harmonic attractors and spatialization. The performer and computer are engaged in a feedback loop that explores degrees of stability, periodicity, non-periodicity, mirco-tuning, and quirky, chaotic potential.",Seth,Shafer,University of Nebraska at Omaha,"Seth Shafer is a composer and researcher whose work focuses on real-time notation, interactive music, and algorithmic art. His compositions have been performed internationally (London, South Korea, Athens, Hamburg, Shanghai, Kraków, Spoleto, and Rio de Janeiro) and across the USA (New York, Los Angeles, Boston, Dallas). His sound installations have been shown at Kaneko (Omaha), the Perot Museum of Nature and Science (Dallas), and the Long Beach Museum of Art. Seth is Assistant Professor of Music Technology at the University of Nebraska at Omaha and holds degrees from the University of North Texas and California State University, Long Beach.",https://sethshafer.com/,2,SBAKTbmB6EM,BV1MV41127AQ,https://www.dropbox.com/s/9pto4mzd969by30/ISMIR-377.mp4?dl=0,Martin Herman; Seth Shafer,,1
381,Generative Sibelius,"“Generative Sibelius” is an experimental video piece featuring an audiovisual transformation of a 1950’s film of composer Jean Sibelius in his home in Finland, recreating his everyday life in self-imposed isolation. The final video was exported from a custom piece of generative software that performs audio manipulations of Sibelius’ Op. 75 No. 5, a piece for solo piano known as “The Spruce”. The audio is analyzed with a number of spectral descriptors related to the level of noise, and the resulting data used as control parameters for a series of complex visual transformations of the film.   This piece is part of a research project that investigates art appropriation and digital fragmentation as a bridge between music technology and the classical tradition, by performing extensive digital alterations on music from the common practice period.",Juan Carlos,Vasquez,University of Virginia,"Juan Carlos Vasquez (www.jcvasquez.com) is an award-winning composer, sound artist, and researcher. His electroacoustic music works are performed constantly around the world and to date have premiered in 29 countries across the Americas, Europe, Asia, and Australia. Vasquez has received grants and commissions from numerous institutions, including the ZKM, the International Computer Music Association, the Nokia Research Center, the Ministry of Culture of Colombia, the Arts Promotion Centre in Finland, the Finnish National Gallery, and CW+ in partnership with the Royal College of Music in London, UK. Some of the events and venues that have featured Vasquez’s works include Ars Electronica (AU), the Ateneum Art Museum (FI), The New York City Library for Performing Arts (Lincoln Center, NY, USA), the Berklee College of Music, Matera Intermedia Festival (IT), Sonorities Festival Belfast (UK), BEAST FEaST (UK) and the New Music Miami ISCM Festival (USA) along with a large number of academic events held by universities across the globe.

As a researcher, Vasquez’s writings can be found in the Computer Music Journal, Leonardo Music Journal, and the proceedings of all the standard conferences of the field. Vasquez received his education at the Sibelius Academy (FI), Aalto University (FI), and the University of Virginia (US). Vasquez’s music is distributed by Naxos, MIT Press (US), Important Records (US), and Phasma Music (Poland).",http://www.jcvasquez.com ,2,-qxsukC-XbU,BV15h41197UB,https://www.dropbox.com/s/ghl6lcnrbl11jv5/ISMIR-381.mp4?dl=0,,,1
388,#otherbeats,"For #otherbeats, Marcel Zaes prompted contributors across the world to send him homemade beats, “alternative” metronomes and skewed pulses, recorded from their shelter-in-place locations with lo-fi gear. This archive of rhythm collected via social networks is displayed in a web arts project: an experimental ‘space’ that lives on a website and makes sound.  The piece is involved with different notions of ‘time grids.’ Using human ‘data’ for an internet-driven sound project leaves the listener with an ambiguous sonic world that oscillates between periodicity, rhythmic deviance, and what might be called a defiant networked system of arbitrary connections. #otherbeats reflects on contemporary scholarly discourses on notions of ‘technological/techno’ and ‘organic,’ of ‘grids as resistance’ and ‘otherness,’ ‘broken’ and ‘failure,’ and it does so by loosely referring to the queer, African American musical undergrounds of the 1970 via its visual language, thereby acknowledging how deeply dance music rhythms are owed to white cultural appropriation from these scenes. Zaes, by way of designing ‘alternate’ systems of networked time grids, proposes an idiosyncratic mode of thinking ‘time grids’ in digital, networked electronic music performance. #otherbeats might be neither ‘techno’ nor ‘organic,’ but in fact, both. The piece is made exclusively with the Web Audio API/JavaScript under HTML5 and uses merely filtering, convolution reverb, synthesis, compression and live mixing as its techniques. #otherbeats takes on to ‘democratize’ electronic music and sound art since it replaces expensive and specialized software at elite institutions with tools as cheap, ubiquitous and ‘accessible’ as the web browser.",Marcel,Zaes,Brown University,"Marcel Zaes (b. 1984 in Bern, Switzerland), is a performer-composer, artist, and artistic researcher. He holds an M.A. in Music & Media Arts from Bern University of the Arts, an M.A. in Music Composition from Zurich University of the Arts and has additionally completed composition studies with Alvin Curran in Rome and with Peter Ablinger in Berlin. Currently, he is pursuing his Ph.D. in Music & Multimedia Composition at Brown University. Marcel explores rhythm in an interdisciplinary framework that encompasses its socio-cultural backgrounds, its politics and perception, and the use of mechanical rhythm machines in music making – such as metronomes, drum machines and step sequencers. Marcel creates textures and beats that emerge as installation pieces, sound performances, concert music for ensembles or as electronic solo performances. For his work, Marcel Zaes has been awarded a number of grants and prizes, has played numerous concerts and taken part in group exhibitions internationally, has repeatedly been an artist in residence and has had his works performed by ensembles internationally. To date, he has published ten albums with Tonus Music Records, Dumpf Edition and Prefermusic, and his 2020 Yarn/Wire album is forthcoming with Editions Verde in New York.",https://marcelzaes.com,2,djCXnlx9Emg,BV13K411N7Wd,https://www.dropbox.com/s/dwu77q3a5ld0clh/ISMIR-388.mp4?dl=0,,https://drive.google.com/open?id=1GDzdppZA42PiDb3Py-kRV7TRt02CRppc,0
406,Moon via Spirit,"This piece was commissioned for the Huddersfield Contemporary Music Festival (hcmf//) as part of the Fluid Corpus Manipulation (FluCoMa) project from the University of Huddersfield. The project studies how creative coders and technologists work with and incorporate new digital tools for deconstructing audio in novel ways. This commission was part of the concert, ’Reactive Flows’, which ""celebrated the first critical use of the Fluid Decomposition toolbox by five of the world-leading laptop artists"". In this piece, I explore these tools through an embodied approach to segmentation, slicing, and layering of sound in real time. Extensive use of the micro-sound technique pulsar synthesis is employed which is explored through the use of tangible controllers. I incorporate novel machine learning techniques in MaxMSP which deal with exploring large corpora of sound files.  www.laurensarahhayes.com www.flucoma.org Video: Angela Guyton",Lauren,Hayes,"Arizona State University, School of Arts, Media & Engineering","Lauren Hayes is a musician, improviser, and sound artist who builds and performs with hybrid analogue/digital instruments. She is Assistant Professor of Sound Studies within the School of Arts, Media and Engineering at Arizona State University where she founded the research group Practice and Research in Enactive Sonic Arts (PARIESA). Her research centers around embodied and enactive music cognition, enactive approaches to digital instrument design, interdisciplinary improvisation, and haptic technologies.

She has been commissioned by major festivals including the London Jazz Festival, the Huddersfield Contemporary Music Festival with a live BBC Radio 3 broadcast as part of its 2017 International Showcase, and Sonica, for which she gave four sold-out performances inside Hamilton Mausoleum, Scotland, famous for once holding the longest echo of any man-made structure. She has performed extensively across Europe and the US and The Wire described her most recent album MANIPULATION (pan y rosas discos) as “skittering melodies and clip-clopping rhythms suggesting a mischievous intelligence emerging from this web of wires”.

She is Director-At-Large of the International Computer Music Association, and a member of the New BBC Radiophonic Workshop, with whom she has been involved in the Oram Awards, responsible for promoting forward-thinking work from women and nonbinary artists.",http://pariesa.com,2,9FoEzcgDhTs,BV18i4y1E7hV,https://www.dropbox.com/s/0lb3hj10rr5vx4r/ISMIR-406.mp4?dl=0,,,0
385,Transcognition,"1.	This piece is created by 32 Multimodal Multiple Cognitive Agents (following the MAS paradigm [4]) that have the capacity of creating music in competitive or collaborative modes (that´s why the work is called transcognition, at the same time, the environment changes in a constant evolving relation). The work begins with all the agents in competitive mode. Each Agent creates its own music trying to generate the most interesting music to get the audience focused in its activity. Periodically, they check if what they do is really the most significant, if they decide that not, then, they become collaborative (they have the same intelligence to measure it). Slowly, all the agents become collaborative.   Two minutes after the last agent becomes collaborative, the work finishes automatically. Once they become collaborative they cannot return to competitive mode.",Fernando,Egido,Universidad Complutense de Madrid,"
He studied composition with José Luis de Delás at  the School of Music of the University of Alcalá de Henares and received musical training with Lachenmann, Spahlinger, Muraill, Sciarrino, Ferneyhough, Kagel, Haas, Dodge, Hubert, etc.... He studied Computer  Music with Emiliano del Cerro. 
His works have been performed at: II International Conference Sound Spaces and Audiovisual Spaces. The SlEMF 2019 in Seoul,  the ACMC 2019 conference in Melbourne, SID 2015 Conference, Venice Vending Machine III, New York City Electroacoustic Music Festival (2016 – 2017) , SMASH Festival, Encontres Festival, ACA, the Fundaçió Pilar i Joan Miró and, Nomad Roots.
",https://busevin.art,3,7tkXsqs1k_s,BV1E5411j7Qg,https://www.dropbox.com/s/pu3idqjybckhxns/ISMIR-385.mp4?dl=0,,,1
386,Attempts at Stillness,"Happy humans in troubled times. A privilege, and a recurring source of angst and guilt, as most of my program notes betray. Moreover, I am saturated, like most, by the incessant stream of stimuli: ads, social media, (curated) news of the unfair world, fascinating discoveries, and whatever else. It all resonates a little too much with the natural thunderstorm in my hyperactive head.  // To cope, I attempt mindfulness. Actually, it is more to really enjoy my privileged life as is: an incredibly difficult task. I therefore share here a sonic musing on the pleasure of contemplating the complex ebb and flow of a million things that fight for attention, when I manage to lower the grip of its frenzy on my angst, guilt and powerlessness, and just observe it with an open curiosity.  // Attempts at Stillness was written in the composer’s studio between April and November 2017. Thanks to Pavel Klusak, Édouard Levasseur, Maxime Levasseur, to have loaned their voices blindly, and to Michal Rataj for the absolute confidence and the stimulating discussions. Part of this research/project was supported by the FluCoMa project (European Union’s Horizon 2020 research and innovation programme, grant #725899). Links to practice-research in general, and to ISMIR in particular, available upon request.",Pierre Alexandre,Tremblay,CeReNeM / University of Huddersfield,"Pierre Alexandre Tremblay (Montréal, 1975) is a composer and an improviser on bass guitar and sound processing devices, in solo and within various ensembles. He is a member of the London-based collective Loop, and his music is also released on Empreintes DIGITALes and Ora. 

He formally studied composition with Michel Tétreault, Marcelle Deschênes, and Jonty Harrison, bass guitar with Jean-Guy Larin, Sylvain Bolduc, and Michel Donato, analysis with Michel Longtin and Stéphane Roy, studio technique with Francis Dhomont, Robert Normandeau, and Jean Piché. 

Pierre Alexandre is Professor in Composition and Improvisation at the University of Huddersfield (UK), where he anchored the Fluid Corpus Manipulation project. He previously worked in popular music as producer and bassist, and has a keen interest for creative coding. He enjoys spending time with his family, drinking oolong tea, gazing at dictionaries, reading prose, and taking long walks.",https://www.pierrealexandretremblay.com/,3,lQ77g_JsNhc,BV1kT4y1c7Do,https://www.dropbox.com/s/9f30i672631egry/ISMIR-386.mp4?dl=0,,,1
400,Sound | Figuration,"Sound | Figuration is a live interactive piece for piano and live multimedia. In this composition, the composer wants to grasp the projection of four-dimensional objects: sound, in the three-dimensional world through a combination of various media. The sound part of piano material is generated by Magenta (Tensorflow) using a pre-trained model and reconstructed by the composer. All of the electronics and video parts are real-time transformed and generated by the live pianist's performance. Therefore, the pianist's performance, as well as live processing audio and video, creating lively intermedia experience to reveal the figuration of sound in the process of emergence, growth and distillation. Besides, this is a brand new attempt in mixed electroacoustic music creation using machine learning.",Hongshuo,Fan,University of Manchester,"Hongshuo Fan 范弘硕 is a Chinese composer and new media artist. His work has involved a variety of real-time interactive multimedia contents, such as acoustic instruments, live electronics, generative visuals, light and body movements. His research and creative interest focus on the fusion of traditional culture and cutting-edge technology in the form of contemporary art. His output spans chamber music, live interactive electronics, installations, and audio-visual works.",https://www.research.manchester.ac.uk/portal/hongshuo.fan.html,3,XMjnoTDMCs0,BV1Wt4y1v7no,https://www.dropbox.com/s/4pyr9z959g3h40k/ISMIR-400.mp4?dl=0,,,0
409,Trees,"What do programs that manipulate trees sound like? This composition tries to find out. The running of a program is like a narrative: Once upon a time there was a poem about trees. The first word was “I”. It was stored at the root of a binary tree. The second word was “think”. It was stored in a node pointed to by the right pointer of the “I” node. But this was a splay tree, and so the two nodes were rearranged so that “think” was at the root. And so the story continues. The inspiration behind the composition was wondering whether following the construction of a search tree would result in a fractal-like composition, with related patterns occurring, to provide repetition for the listener — a method of providing compositional structure without resorting to existing forms and traditional techniques. A second inspiration behind the work was the idea of sonifying code, to provide feedback to a programmer in a new way, for example, about where possible inefficiencies lie. In the case of the chosen algorithm and data structure, each time two words are compared, the words are repeated for each character that needs to be compared, leading to the sequence “that, think, that, think, that, think” being sung, due to the first three characters needing to be compared. In terms of the concert theme of “music information”, the work communicates information about a computer program via musical output. It also renders musically a method of storing and retrieving information.",Alexandra,Uitdenbogerd,RMIT University,"Alexandra L. Uitdenbogerd (https://sites.rmit.edu.au/alu/) is a Senior Lecturer in the School of Science at RMIT University. Uitdenbogerd completed a B.Sc. degree from the University of Western Australia and a Computer Science Ph.D. from RMIT University in 2002. She has held government (OLT) and privately funded grants in her other research interest of supporting language acquisition, as well as in the automatic optical inspection of circuit boards. 
A pioneer in Music Information Retrieval and part of the winning team of the recently completed inaugural Eurovision AI Song Contest, Uitdenbogerd’s research has spanned symbolic melody retrieval, symbolic to audio matching, music genre classification, recommender systems, lyric analysis and matching, algorithmic composition, and sonification. In her slightly less academic life, she composes for, conducts and sings in choirs; writes, performs and records her compositions; self-publishes a comic book in French for beginners with an English-speaking background called Gnomeville; and follows in her father’s footsteps as family historian.
",https://sites.rmit.edu.au/alu/,3,It_ImuB1Ip8,BV18k4y1C7CA,https://www.dropbox.com/s/nf31b9rjifvag88/ISMIR-409.mp4?dl=0,,https://drive.google.com/open?id=11JlFlcVkjSHe82szfxJmdAOH4QNCUkub,0
415,The Pulse of The Sunrise,"The pulse of the sunrise, is a result of fusing the music played on the santur, the Persian hammered dulcimer and the daf, the Kurdish frame drum, accompanied by altered live samples of the same music, through audio participation, using Grainfield system. The piece was partly composed and partly improvised in Mahur, Homayun and Chahargah, three of the seven main modes in Iranian music. Innovatory composition and performance techniques were coupled with the cutting edge audio technologies.   The members of the audience participated in the live performance, by tilting their mobiles horizontally, and vertically, which played parts of the live performance in different ways through the speakers of their mobile phones. The performance won the prize for the best performance at Audio Mostly conference 2017.",Peyman,Heydarian,University of Waikato,"Born in Shiraz, Iran, Peyman Heydarian is an award-winning music scientist and santur virtuoso. He developed a unique performance style and adopted innovative tuning systems and performance techniques to play a multi-ethnic repertoire on the santur. Peyman started music at the age of four, and studied with Mojtaba Mirzadeh, and Pashang Kamkar. Since 2001, he has taught music, electronic engineering and computer science courses in Iran, Britain and New Zealand. Peyman established musical societies and bands, including The Voice of Santur. Since 1982, he has performed in Iran, USA, Syria, Jordan, Turkey, Greece, Italy, Hong Kong, Poland, New Zealand, and the UK.

During his BSc, MSc, MPhil and PhD, at Shiraz and Tarbiat Modarres Universities, Queen Mary, University of London, and London Metropolitan University he developed algorithms for music transcription and maqàm/mode recognition. He was highly commended at SSMS 2007 for Analysis and Classification of Persian Musical Modes, and his performance with audience participation won the first prize in AudioMostly 2017. 

He also makes music for films and Radio and TV channels. Peyman has been researching the possibilities of pushing the boundaries of the Persian music and santur performance.  He is currently developing a content-based personal music recommender system. 
",http://www.thesantur.com,3,-KpxaNhor-4,BV1ei4y1E7Xk,https://www.dropbox.com/s/bprduzj6wduc6g6/ISMIR-415.mp4?dl=0,,,0
