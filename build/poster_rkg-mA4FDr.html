


<!DOCTYPE html>
<html lang="en">
  <head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <link rel="icon" type="image/png" sizes="32x32" href="static/images/ismir_tab_icon.png">

    <link rel="stylesheet" href="static/css/main.css" />
    <link rel="stylesheet" href="static/css/custom.css" />
    <link rel="stylesheet" href="static/css/lazy_load.css" />
    <link rel="stylesheet" href="static/css/typeahead.css" />

    <!-- External Javascript libs  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"
    ></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>


    <!-- Library libs -->
    <script src="static/js/typeahead.bundle.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
      href="static/css/Lato.css"
      rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet" />
    <link
      href="static/css/Cuprum.css"
      rel="stylesheet"
    />
    <link
      href="static/css/Ambiant.css"
      rel="stylesheet"
    />

    <title>MiniConf 2020: Pre-training Tasks for Embedding-based Large-scale Retrieval</title>
    
<meta name="citation_title" content="Pre-training Tasks for Embedding-based Large-scale Retrieval" />

<meta name="citation_author" content="Wei-Cheng Chang" />

<meta name="citation_author" content="Felix X. Yu" />

<meta name="citation_author" content="Yin-Wen Chang" />

<meta name="citation_author" content="Yiming Yang" />

<meta name="citation_author" content="Sanjiv Kumar" />

<meta name="citation_publication_date" content="11-15 October 2020" />
<meta name="citation_conference_title" content="Ismir 2020 Virtual Conference" />
<meta name="citation_inbook_title" content="Proceedings of the First MiniCon Conference" />
<meta name="citation_abstract" content="We consider the large-scale query-document retrieval problem: given a query (e.g., a question), return the set of relevant documents (e.g., paragraphs containing the answer) from a large document corpus. This problem is often solved in two steps. The retrieval phase first reduces the solution space, returning a subset of candidate documents. The scoring phase then re-ranks the documents. Critically, the retrieval algorithm not only desires high recall but also requires to be highly efficient, returning candidates in time sublinear to the number of documents. Unlike the scoring phase witnessing significant advances recently due to the BERT-style pre-training tasks on cross-attention models, the retrieval phase remains less well studied. Most previous works rely on classic Information Retrieval (IR) methods such as BM-25 (token matching + TF-IDF weights). These models only accept sparse handcrafted features and can not be optimized for different downstream tasks of interest. In this paper, we conduct a comprehensive study on the embedding-based retrieval models. We show that the key ingredient of learning a strong embedding-based Transformer model is the set of pre-training tasks. With adequately designed paragraph-level pre-training tasks, the Transformer models can remarkably improve over the widely-used BM-25 as well as embedding models without Transformers. The paragraph-level pre-training tasks we studied are Inverse Cloze Task (ICT), Body First Selection (BFS), Wiki Link Prediction (WLP), and the combination of all three." />

<meta name="citation_keywords" content="attention" />

<meta name="citation_keywords" content="nlp" />

<meta name="citation_keywords" content="representation learning" />

<meta name="citation_keywords" content="transformer" />

<meta name="citation_keywords" content="unsupervised" />

<meta name="citation_pdf_url" content="" />


  </head>

  <body>
    <!-- NAV -->
    
    <nav
      class="navbar sticky-top navbar-expand-lg navbar-light mr-auto"
      id="main-nav"
    >
      <div class="container">
        <a class="navbar-brand" href="index.html">
          <img
             class="logo" src="static/images/ISMIR-logo_Horizontal - Acronym only_RGB.jpg"
             height="auto"
             width="200px"
          />
        </a>
        
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div
          class="collapse navbar-collapse text-right flex-grow-1"
          id="navbarNav"
        >
          <ul class="navbar-nav ml-auto">
            
            <li class="nav-item ">
              <a class="nav-link" href="index.html">Home</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="calendar.html">Schedule</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="papers.html">Papers</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="workshops.html">Workshops</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="chat.html">Chat</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="about.html">Help</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
    

    
    <!-- User Overrides -->
     

    <div class="container">
      <!-- Tabs -->
      <div class="tabs">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="">
      Pre-training Tasks for Embedding-based Large-scale Retrieval
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      
      <a href="papers.html?filter=authors&search=Wei-Cheng Chang" class="text-muted"
        >Wei-Cheng Chang</a
      >,
      
      <a href="papers.html?filter=authors&search=Felix X. Yu" class="text-muted"
        >Felix X. Yu</a
      >,
      
      <a href="papers.html?filter=authors&search=Yin-Wen Chang" class="text-muted"
        >Yin-Wen Chang</a
      >,
      
      <a href="papers.html?filter=authors&search=Yiming Yang" class="text-muted"
        >Yiming Yang</a
      >,
      
      <a href="papers.html?filter=authors&search=Sanjiv Kumar" class="text-muted"
        >Sanjiv Kumar</a
      >
      
    </h3>
    <p class="card-text text-center">
      <span class="">Keywords:</span>
      
      <a
        href="papers.html?filter=keywords&search=attention"
        class="text-secondary text-decoration-none"
        >attention</a
      >,
      
      <a
        href="papers.html?filter=keywords&search=nlp"
        class="text-secondary text-decoration-none"
        >nlp</a
      >,
      
      <a
        href="papers.html?filter=keywords&search=representation learning"
        class="text-secondary text-decoration-none"
        >representation learning</a
      >,
      
      <a
        href="papers.html?filter=keywords&search=transformer"
        class="text-secondary text-decoration-none"
        >transformer</a
      >,
      
      <a
        href="papers.html?filter=keywords&search=unsupervised"
        class="text-secondary text-decoration-none"
        >unsupervised</a
      >
      
    </p>
    <div class="text-center p-3">
      <a class="card-link" data-toggle="collapse" role="button" href="#details">
        Abstract
      </a>
      <a class="card-link" target="_blank" href="">
        Paper
      </a>
      
    </div>
  </div>
</div>
<div id="details" class="pp-card m-3 collapse">
  <div class="card-body">
    <div class="card-text">
      <div id="abstractExample">
        <span class="font-weight-bold">Abstract:</span>
        We consider the large-scale query-document retrieval problem: given a query (e.g., a question), return the set of relevant documents (e.g., paragraphs containing the answer) from a large document corpus. This problem is often solved in two steps. The retrieval phase first reduces the solution space, returning a subset of candidate documents. The scoring phase then re-ranks the documents. Critically, the retrieval algorithm not only desires high recall but also requires to be highly efficient, returning candidates in time sublinear to the number of documents. Unlike the scoring phase witnessing significant advances recently due to the BERT-style pre-training tasks on cross-attention models, the retrieval phase remains less well studied. Most previous works rely on classic Information Retrieval (IR) methods such as BM-25 (token matching + TF-IDF weights). These models only accept sparse handcrafted features and can not be optimized for different downstream tasks of interest. In this paper, we conduct a comprehensive study on the embedding-based retrieval models. We show that the key ingredient of learning a strong embedding-based Transformer model is the set of pre-training tasks. With adequately designed paragraph-level pre-training tasks, the Transformer models can remarkably improve over the widely-used BM-25 as well as embedding models without Transformers. The paragraph-level pre-training tasks we studied are Inverse Cloze Task (ICT), Body First Selection (BFS), Wiki Link Prediction (WLP), and the combination of all three.
      </div>
    </div>
    <p></p>
  </div>
</div>

<h5 style="color: red;">
  Add content for posters. This could be a video, embedded pdf, chat room ....
</h5>

<!-- Chat -->
<div class="border-top my-3"></div>
<div class="row p-4" id="faq">
  <div class="col-12 bd-content">
    <h2 class="text-center">Example Chat</h2>
  </div>
</div>

<div class="col-md-12 col-xs-12 p-2">
  <div id="gitter" class="slp">
    <center>
      <iframe frameborder="0" src="https://miniconf.sloppy.zone/channel/paper_rkg-mA4FDr?layout=embedded" height="700px" width="100%" ></iframe>
    </center>
  </div>
</div>

<!-- Slides Live-->

<div class="border-top my-3"></div>
<div class="row p-4" id="faq">
  <div class="col-12 bd-content">
    <h2 class="text-center">Example SlidesLive</h2>
  </div>
</div>

<div class="col-md-12 col-xs-12 my-auto p-2" >
  <div id="presentation-embed" class="slp my-auto"></div>
  <script src='https://slideslive.com/embed_presentation.js'></script>
  <script>
    embed = new SlidesLiveEmbed('presentation-embed', {
    presentationId: '38922815',
    autoPlay: false, // change to true to autoplay the embedded presentation
    verticalEnabled: true,
    verticalWhenWidthLte: 500,
    allowHiddenControlsWhenPaused: true,
    hideTitle: true
    });
  </script>
</div>


<!-- Chat -->

<div class="border-top my-3"></div>
<div class="row p-4" id="faq">
  <div class="col-12 bd-content">
    <h2 class="text-center">Example Poster</h2>
  </div>
</div>

<div role="main" id="pdf_view"></div>


<script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.3.200/build/pdf.min.js"></script>
<script src="static/js/pdf_render.js"></script>
<script>
    $(document).ready(() => {
        // render first page of PDF to div
        // PDF name can be bound to variable -- e.g. paper.content.poster_link
        initPDFViewer("/static/images/GLTR_poster.pdf",'#pdf_view');
    })
</script>







      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "UA-");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2020 MiniConf Organization Committee</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
  </body>
</html>