[{"UID":"68","category":"All Meeting","channel_url":"https://ismir2020.slack.com/archives/C01BWNNTL7M","day":"4","description":"Annual business meeting of the International Society for Music Information Retrieval, A.","organiser":"Audrey Laplante","slack_channel":"#ismir-board","start_date":"2020-10-15","start_date_b":"2020-10-16","start_time":"14:30","start_time_b":"3:00","title":"Awards & ISMIR Business Meeting A (Live)","web_link":""},{"UID":"11","category":"All Meeting","channel_url":"","day":"1","description":"Welcome to ISMIR 2020! Meet your hosts and hear about what is happening at this year's very special conference.","organiser":"Audrey Laplante","slack_channel":"","start_date":"2020-10-12","start_date_b":"2020-10-13","start_time":"12:00","start_time_b":"0:30","title":"Opening Plenary A","web_link":""},{"UID":"1","category":"Tutorials","channel_url":"https://ismir2020.slack.com/archives/C01C2MY697W","day":"-1","description":"\"This tutorial will walk attendees through the use of a Python framework called klio that makes use of the Apache Beam Python SDK to parallelize the execution of audio processing algorithms over a large dataset. Apache Beam is a portable and extensible programming model that unifies distributed batch and streaming processing. It manages the I/O and parallelized execution needed for large-scale data processing. Any audio processing algorithm that can be executed by a Python process and has dependencies that can be installed on machines supported by Apache Beam runners can be run with klio. Audio processing algorithms that have been added to a klio data processing job can be run locally on the practitioner\u2019s machine, before being run on large-scale data processing systems like Google Cloud Dataflow. This enables the practitioner to make quick local changes to their algorithm and test it on a few files before deploying a longer running job on more files.\nThe intended audience of this tutorial are audio processing practitioners who have wrestled with the complexity of iterating upon and coordinating the execution of algorithms that both consume and produce large audio datasets. klio provides best-practice standards and abstractions, encoded in its Python-based command line interface and API, that help audio practitioners prototype, organize and scale their work.\nDuring the tutorial, attendees will receive: An overview of klio that establishes core concepts and features; Guidance through building a klio audio processing graph and running it on an audio dataset\"","organiser":"Fallon Chen, Lynn Root, Dan Simon, and Shireen Kheradpey","slack_channel":"#tutorial-1-klio-tutorial","start_date":"2020-10-11","start_date_b":"2020-10-11","start_time":"11:00","start_time_b":"19:00","title":"T1:  Prototyping and Scaling Audio Research with Klio","web_link":"https://ismir.github.io/ISMIR2020/tutorials/"},{"UID":"2","category":"Tutorials","channel_url":"https://ismir2020.slack.com/archives/C01C95DPELR","day":"-1","description":"This tutorial will briefly summarize research on expressive timing in music, present an original research project (as an example) on rubato in four performances of Bach\u2019s Invention No. 9, explain and demonstrate how to use the freeware Sonic Visualiser as well as Excel for the analysis of expressive timing in music, and participants will, with the help of the tutorial leader, pursue their own analysis of other performances of Bach\u2019s Invention No. 9. (Recordings will be provided.) We will combine the data collected (in Excel files) to look for similarities and differences in the various performances and how expressive timing correlates to certain musical features. (An analytical score of the piece will be provided.) We will collectively formulate research findings.\nThis tutorial is suitable for anyone who is curious about the topic. Beyond curiosity, participants do not need to have a music or computer science background. Those interested in participating in the analyses should bring a laptop (Windows computer or Mac) to the tutorial, with Sonic Visualiser (http://sonicvisualiser.org) and the VAMP plugin \u201cNote Onset Detector\u201d (http://www.vamp-plugins.org) installed.","organiser":"Nico Sch\u00fcler","slack_channel":"#tutorial-2-expressive-timing","start_date":"2020-10-11","start_date_b":"2020-10-11","start_time":"14:00","start_time_b":"22:00","title":"T2:  Analysis of Expressive Timing in Recorded Music Performances","web_link":"https://ismir.github.io/ISMIR2020/tutorials/"},{"UID":"3","category":"Tutorials","channel_url":"https://ismir2020.slack.com/archives/C01BFPZHHSB","day":"-1","description":"Metric learning is a paradigm of representation learning, in which proximity between the representations of items is optimized to correspond to a notion of similarity. Compared to the classification, metric learning can leverage more flexible forms of supervision, for example, two audio clips belong to the same artist or not, or have the same tempo or not. This enables the learning model to take a large or indefinite number of classes. Moreover, metric learning handles the embedding space directly by measuring the distance between the transformations of different examples. This facilitates usage of different domains or modalities of inputs in the same framework (e.g., audio embedding in one input and word embedding in another input). Such flexible and adaptable characteristics of metric learning have been enjoyed in many of machine learning tasks, particularly, similarity-based content retrieval. In recent years, interest in metric learning from the MIR community has also increased. Considering the multi-faceted and hierarchical-level of notions in similarity (e.g., semantic-level, score-level or audio-level) and diverse forms of data (e.g., audio, MIDI, text labels, lyrics, album covers, and user data), we see a great potential of metric learning in music. Therefore, introducing the method in an educational manner and surveying recent progress will be timely and helpful to relevant researchers. In this tutorial, we plan to present three lectures as follows:\nMetric learning foundations: This lecture introduces mathematical foundations of metric learning.\nDeep metric learning and applications to MIR (1): core tasks - This lecture introduces recent deep metric learning methods and their applications to music classification and similarity-based retrieval tasks.\nDeep metric learning and applications to MIR (2): variations - This lecture introduces various applications of deep metric learning in MIR, showing how researchers have bridged diverse domains and modalities of input in metric learning.","organiser":"Brian McFee, Jongpil Lee, Juhan Nam","slack_channel":"#tutorial-3-metric-learning-mir","start_date":"2020-10-11","start_date_b":"2020-10-12","start_time":"14:00","start_time_b":"6:00","title":"T3:  Metric Learning for Music Information Retrieval","web_link":"https://ismir.github.io/ISMIR2020/tutorials/"},{"UID":"4","category":"Tutorials","channel_url":"https://ismir2020.slack.com/archives/C01C2MU93U4","day":"-1","description":"Musical source separation has become increasingly effective in recent years. As such, applications of music source separation have the potential to touch many aspects of MIR research. However, from a user\u2019s perspective, either in doing source separation research from scratch or in applying source separation to other tasks (e.g. polyphonic music transcription), there are still significant roadblocks. Code and data are released on a paper-by-paper basis, making it difficult to compare, use and extend existing techniques. This limits the usefulness of source separation for researchers not actively steeped in its many nuances, and hinders its applicability to broader MIR research.\nIn this tutorial, we present a set of complementary, easy-to-use, open-source tools and datasets for source separation research, evaluation, and deployment. We show how they interlock with one another, and how they can be used in concert to structure source separation within a project for research or deployment. Finally, we propose a generic and well-tested project structure for efficiently doing modern source separation research, from sweeping over hyperparameters, to setting up competitive baselines, to augmenting your datasets.\nParticipants of this tutorial will leave with:\nA practical overview of source separation including history and current research trends.\nThe ability to make educated decisions about how to best include source separation in their workflow;The ability to select the proper separation algorithm or a pre-trained model for their research;The ability to effectively train a custom model for their research using open-source tools.\nOur tutorial is aimed at researchers and practitioners that are familiar with audio and machine learning but have little or no experience with source separation. Our primary resources will be the following open-source/data projects: nussl, scaper, Slakh2100, and MUSDB18. References to additional tools and datasets (including for non-music audio) will be provided.","organiser":"Ethan Manilow, Prem Seetharaman, Justin Salamon","slack_channel":"#tutorial-4-open-source-separation","start_date":"2020-10-11","start_date_b":"2020-10-12","start_time":"14:00","start_time_b":"2:00","title":"T4:  Open-Source Tools & Data for Music Source Separation","web_link":"https://ismir.github.io/ISMIR2020/tutorials/"},{"UID":"5","category":"Tutorials","channel_url":"https://ismir2020.slack.com/archives/C01BWNF4YCT","day":"-1","description":"The version identification (VI) task concerns detecting and retrieving a set of songs that originate from the same underlying musical composition. Versions (or cover songs) convey the same musical entity while incorporating differences in several musical characteristics, including the differences in timbre, tempo, key, lyrics, and even added/deleted sections. The main applications include digital rights management and music catalog organization.\nFor more than a decade, VI systems suffered from the accuracy-scalability trade-off, with attempts to increase accuracy resulting in cumbersome, non-scalable systems. Recent years however have witnessed an increase in deep learning-based VI approaches that take a step toward bridging the accuracy-scalability gap, and we start seeing the possibility to deploy such systems in real-world applications. Although this trend positively influences the number of researchers and institutions working on VI, it may also result in obscuring the literature before the deep learning era. To appreciate the 20 years of novel ideas in VI and to facilitate building better systems in the next decade, we believe that now may be the right time to review some of the successful ideas and applications proposed in VI literature and connect them to current systems and ideas.\nWe will start the tutorial by explaining common input representations and feature post-processing steps. We will continue with comparing the pros and cons of alignment-based and embedding-based approaches, which constitute the two main perspectives for similarity estimation in VI. Lastly, after discussing a number of ideas that can be incorporated into any VI system, we will conclude by presenting the current challenges and future directions in VI research. Our goal is for the audience to leave with a thorough appreciation of both the history of the task and current directions, and that this context will allow them to jump into conducting novel research in the area.","organiser":"Furkan Yesiler, Christopher Tralie, Joan Serr\u00e0","slack_channel":"#tutorial-5-version-identification","start_date":"2020-10-11","start_date_b":"2020-10-12","start_time":"16:00","start_time_b":"8:00","title":"T5:  Version Identification in the 20s","web_link":"https://ismir.github.io/ISMIR2020/tutorials/"}]
